
to print all data in present weather column (column 9) matching fog 40,41, ...49
from data csv file

awk -F ,  '{print $9}' YAMB.txt| egrep "4[0-5]"

http://www.theunixschool.com/2012/05/awk-match-pattern-in-file-in-linux.html


to grab entire lines matching fog 40,41,42,....49 in the present weather column (column 9)

awk -F , '$9 ~ /4[0-9]/' YAMB.txt > YAMB_foggy.txt


The -F option in awk is used to specify the delimiter. 


42 Present weather in code

awk -F , '$42 ~ /4[0-9]/' HM01X_Data_040004_YAMB.txt > fog1.txt


45, 48, 51, type of 1st 2nd 3rd present wx code

53, 55, 57  type of 1st 2nd 3rd recent wx code

58, 59, 60 AWS present weather in code, last 15 minutes, last 60 minutes in code

awk -F , '$45 ~ /FG/ || $48 ~ /FG/  || $51 ~ /FG/  || $53 ~ /FG/  || $55 ~ /FG/|| $57 ~ /FG/ || $58 ~ /FG/ || $59 ~ /FG/ || $60 ~ /FG/ ' HM01X_Data_040004_YAMB.txt > fog2.txt

awk -F , '$42 ~ /4[0-9]/ || $45 ~ /FG/ || $48 ~ /FG/  || $51 ~ /FG/  || $53 ~ /FG/  || $55 ~ /FG/|| $57 ~ /FG/ || $58 ~ /FG/ || $59 ~ /FG/ || $60 ~ /FG/ ' HM01X_Data_040004_YAMB.txt > fog3.txt

last 2 cmds give same result!!

[vinorda@hp450-fed20 HM01X_99999999_8720828]$ diff fog2.txt fog3.txt
1c1
<  awk -F , '$45 ~ /FG/ || $48 ~ /FG/  || $51 ~ /FG/  || $53 ~ /FG/  || $55 ~ /FG/|| $57 ~ /FG/ || $58 ~ /FG/ || $59 ~ /FG/ || $60 ~ /FG/ ' HM01X_Data_040004_YAMB.txt > fog2.txt
---
> awk -F , '$42 ~ /4[0-9]/ || $45 ~ /FG/ || $48 ~ /FG/  || $51 ~ /FG/  || $53 ~ /FG/  || $55 ~ /FG/|| $57 ~ /FG/ || $58 ~ /FG/ || $59 ~ /FG/ || $60 ~ /FG/ ' HM01X_Data_040004_YAMB.txt > fog3.txt

------------------------------------------------------------------

grab number of columns if there was fog
awk -F , '$42 ~ /4[0-9]/ || $45 ~ /FG/ || $48 ~ /FG/  || $51 ~ /FG/  || $53 ~ /FG/  || $55 ~ /FG/|| $57 ~ /FG/ || $58 ~ /FG/ _
|| $59 ~ /FG/ || $60 ~ /FG/ _
 {print $8 "," $9 ","  $19  "," $18 "," $20 "," $39 "," $40 "," $41 "," $42 "," $43 "," $44 ","  $45 "," $46 "," $47 "," $48 "," _
 $49 "," $50 "," $51 "," $58 "," $59 "," $60 "," $12 "," $13 "," $14 "," $15 "," $61 "," $62 "," $63 ","$10 "," $11 "," $68 } ' _
HM01X_Data_040004_YAMB.txt > amb-test.txt
------------------------------------------------------------------
more limited fields grab 

awk -F , '$42 ~ /4[0-9]/ || $45 ~ /FG/ || $48 ~ /FG/  || $51 ~ /FG/  || $53 ~ /FG/  || $55 ~ /FG/|| $57 ~ /FG/ || $58 ~ /FG/ || $59 ~ /FG/ || $60 ~ /FG/ {print $8 "," $9 ","   $64 "," $19  "," $18 "," $20 "," $39 "," $40 "," $41 "," $42 "," $44$45 "," $47$48 "," $50$51  $12 "," $14 "," $15 "," $63 ","$10 "," $11 "," $72 } ' HM01X_Data_040004_YAMB.txt > amb-test.txt2
------------------------------------------------------------------
grab all specified fields regardless fog or not
drop Original message

awk -F , '{print $8 "," $9 ","   $64 "," $19  "," $18 "," $20 "," $39 "," $40 "," $41 "," $42 "," $44$45 "," $47$48 "," $50$51 ","  $21$22$23 ","  $24$25$26 ","  $27$28$29 ","  $33$34 "," $35$36 "," $37$38 "," $12 "," $14 "," $15 "," $63 ","$10 "," $11 } ' HM01X_Data_040004_YAMB.txt > amb-test3.txt
------------------------------------------------------------------

drop cloud type - just grab amount and ht, also drop msg type metar/speci

awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '  HM01X_Data_040004_YAMB.txt > YAMB1.txt


use this data set to process further info using aws_format.pl


grab first 45 columns from file created by above program  - sometimes prints extra erroneous columns!!!

[vinorda@hp450-fed20 HM01X_99999999_8720828]$ 

cut -d ',' -f1-45 YAMB1-all-synop.txt > YAMB1-all-synop.txt


clean further - remove dupicates - these lines usually have time in the 1st column
so remove lines with time in 1st colum

[vinorda@hp450-fed20 HM01X_99999999_8720828]$ 

awk -F , '$1 !~ /(05:00|08:00|11:00|14:00|17:00|20:00)/ {print $0}' YAMB1-all1.txt > YAMB1-all2.txt

above  cmd grabs all lines that dont have time in 1st col '$1! !~  (list of times)

this does same actually!!!   - It only grabs lines that have word/pattern YAMB in 1st col 
awk -F , '$1 ~ /YAMB/ {print $0}' YAMB1b.txt > YAMB1c.txt;

---------------------------------------------------
Step 0 - Run perl
================

grab all specified fields regardless fog or not
drop Original message
drop cloud type - just grab amount and ht, also drop msg type metar/speci



Step1 - Run perl
================
./aws_format_v5.9.pl YAMB1.txt YAMB1-all-new.txt


Step2 - Remove/Cut out extra columns - artifacts of bad coding !!!
======
cut -d ',' -f1-52 YAMB1-all-new.txt > YAMB1-all-new1.txt


Update: Step2 NO LONGER NEEDED - CODING FIXED


Step3 - Remove extra rows - again artifacts of coding
======
awk -F , '$1 !~ /(05:00|08:00|11:00|14:00|17:00|20:00)/ {print $0}' YAMB1-all-new1.txt > YAMB1-all2.txt

Update: Step3 NO LONGER NEEDED - CODING FIXED



Step4 - Remove data rows that appear erroneous - here we are removing 1st 2997 lines
======

R IMPORT TABLE CODING FIXED - 
amby = read.csv("YAMB5a.txt", header=T,sep="," )	#delimeter is ","
amby=na.omit(amby)		- this drops all lines/rows with missing data

includes all observations upto Dec 1999

Use sed to copy lines 2997 to 14172 from 1st file to new new file
sed -n '2997,14172p;14172q' YAMB1-all2.txt > YAMB1-all3.txt 
Add row for column headers back to data text file if it has gone missing
Now fire up stats package........

R

one-liners...
./aws_format_v6.0.pl YAMB1.txt  YAMB4a.txt;
cut -d ',' -f1-59 YAMB4a.txt >  YAMB4b.txt; 
awk -F , '$1 ~ /YAMB/ {print $0}' YAMB4b.txt > YAMB4c.txt;
sed -n '3406,14172p;14172q' YAMB1c.txt > YAMB1d.txt;


September 8th 2015
extract relevant columns from climatezone file

awk -F , '{print $6 "," $8 "," $9 "," $64  "," $19  "," $18 "," $20 "," $39 "," $40 "," $41 "," $42 "," $44$45 "," $47$48 "
," $50$51 ","  $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $12 "," 
$14 "," $15 "," $63 ","$10 "," $11 } '  HM01X_Data_040004_YAMB.txt > YAMB1.txt

awk -F , '{print $6 "," $8 "," $9 "," $64  "," $19  "," $18 "," $20 "," $39 "," $40 "," $41 "," $42 "," $44$45 "," $47$48 "
," $50$51 ","  $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $12 "," 
$14 "," $15 "," $63 ","$10 "," $11 } '  HM01X_Data_040842_YBBN-since1992.txt > HM01X_Data_040842_YBBNa.txt


run perl script to process file to get fog onset, finish, duration, rain24hr etc

./aws_format_v6.4.pl HM01X_Data_040842_YBBNa.txt HM01X_Data_040842_YBBNb.txt > HM01X_Data_040842_YBBNb_out.txt

./aws_format_v6.4.pl HM01X_Data_040004_YAMBa.txt HM01X_Data_040004_YAMBb.txt> HM01X_Data_040004_YAMBb_out.txt

plot

 R CMD BATCH amb_plot.R


extract only days with fog for further explatory plots

 awk -F , '$7 ~ /YES/ {print $0}' HM01X_Data_040842_YBBNb.txt > HM01X_Data_040842_YBBNc.txt




[vinorda@asus-h87centos7 HM01X_99999999_8720828]$ ls -ltr *R
-rwxr--r--. 1 vinorda users 3685 Aug 23 15:14 oakey_plot.R
-rwxr--r--. 1 vinorda users 3712 Aug 23 15:14 brisy_plot.R
-rwxr--r--. 1 vinorda users 3714 Aug 23 15:15 toomba_plot.R
-rwxr--r--. 1 vinorda users 3689 Aug 23 15:15 sunny_plot.R
-rwxr--r--. 1 vinorda users 3713 Aug 23 15:15 amb_plot.R

[vinorda@asus-h87centos7 HM01X_99999999_8720828]$ R CMD BATCH amb_plot.R
[vinorda@asus-h87centos7 HM01X_99999999_8720828]$ R CMD BATCH oakey_plot.R
[vinorda@asus-h87centos7 HM01X_99999999_8720828]$ R CMD BATCH toomba_plot.R
[vinorda@asus-h87centos7 HM01X_99999999_8720828]$ R CMD BATCH sunny_plot.R
[vinorda@asus-h87centos7 HM01X_99999999_8720828]$ R CMD BATCH brisy_plot.R





=======================================================
process daily climate data files

11,41 Fog patches
12    Shallow fog
28    Recent fog
40    Distant fog
42-49 Fog


Many days where daily column says "N" to FOG, but either the 6am or 9am synop reported FOG!!!
This looks at all fog, shallow fog, fog patches, recent fog etc

$awk -F , '$4 ~ /4[0-9]/ || $5 ~ /4[0-9]/ || $6 ~ /4[0-9]/ ||$7 ~ /4[0-9]/ || \
> $4 ~ /28|1[1-2]/ || $5 ~ /28|1[1-2]/ || $6 ~ /28|1[1-2]/ || $7 ~ /28|1[1-2]/ \
> {print $0 } ' DC02D_Data_040004_999999998734737.txt > amb-daily-fog1.txt

[vinorda@hp450-fed20 HM01X_99999999_8720828]$ wc amb-daily-fog1.txt
  4127  18358 156826 amb-daily-fog1.txt


And looking at just daily data
-------------------------------
$ awk -F , '$8 ~ /Y/ {print $0 } ' DC02D_Data_040004_999999998734737.txt > amb-daily-fog2.txt
[vinorda@hp450-fed20 HM01X_99999999_8720828]$ wc amb-daily-fog2.txt
 1783  7160 68067 amb-daily-fog2.txt

Looking at just daily data (midnight to midnight), we get only 1783 fog events.
This is much less, over 2300 less events.
I am guessing the 6am and 9am capture lot more shallow fog etc whereas 

Lets drop shallow fog MIFG  12

$awk -F , '$4 ~ /4[0-9]/ || $5 ~ /4[0-9]/ || $6 ~ /4[0-9]/ ||$7 ~ /4[0-9]/ || \
> $4 ~ /28|11/ || $5 ~ /28|11/ || $6 ~ /28|11/ || $7 ~ /28|11/ \
> {print $0 } ' DC02D_Data_040004_999999998734737.txt > amb-daily-fog3.txt

[vinorda@hp450-fed20 HM01X_99999999_8720828]$ wc  amb-daily-fog3.txt
  4044  17941 153672 amb-daily-fog3.txt


still over 2200 more fogs!!

Decide to use this list (minus MIFG) - count of 4044 fog days 
and then look through AWS data to verify RH>97 and vis<1km to tick off a fog.



awk -F , '$9 < 2 || $20 ~ /OVC/ || $21 <= 200 {print $0 } ' YAMB1-2015.txt > yam-fog.txt


=======================RSYNC=================================
rsync -avz source destination 
Transfer copy files from fog folder on hp fedora to usb
-a preserves option preserves the permissions, ownership, timestamp of files and folders that are to be rsynced. 

--max-size=SIZE         don't transfer any file larger than SIZE


# rsync -rv --max-size=1.5m root@tss01:/tmp/dm
Will send only files less than 1.5m.

The suffixes are as follows: lowercase works as well
"K" (or "KiB") is a kilobyte (1024), 
"M" (or "MiB") is a megabyte (1024*1024), and 
"G" (or "GiB") is a gigabyte (1024*1024*1024). 
If you want the multiplier to be 1000 instead of 1024, use "KB", "MB", or "GB" 

[vinorda@asus ~]$ rsync -avz  --max-size=100m   
/home/vinorda/stats-R/fog-project/Amb-climate /run/media/vinorda/met-bom/fog_project/


[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YBBN1.txt YBBN2manual.csv > YBBN3manual.txt;
Can't locate Math/Round.pm in @INC (you may need to install the Math::Round module) (@INC contains: /usr/local/lib64/perl5/5.30 /usr/local/share/perl5/5.30 /usr/lib64/perl5/vendor_perl /usr/share/perl5/vendor_perl /usr/lib64/perl5 /usr/share/perl5) at ./aws_format_v6.5_2020.pl line 30.
BEGIN failed--compilation aborted at ./aws_format_v6.5_2020.pl line 30.
[bou@bous-fed31 fog_climatology_2019]$ sudo cpan
[sudo] password for bou: 
Loading internal logger. Log::Log4perl recommended for better logging

CPAN.pm requires configuration, but most of it can be done automatically.
If you answer 'no' below, you will enter an interactive dialog for each
configuration option instead.

Would you like to configure as much as possible automatically? [yes] 


Autoconfiguration complete.

commit: wrote '/root/.local/share/.cpan/CPAN/MyConfig.pm'

You can re-run configuration any time with 'o conf init' in the CPAN shell
Terminal does not support AddHistory.

To fix that, maybe try>  install Term::ReadLine::Perl


cpan shell -- CPAN exploration and modules installation (v2.27)
Enter 'h' for help.

cpan[1]> install Math::Round
Fetching with LWP:
http://www.cpan.org/authors/01mailrc.txt.gz
Reading '/root/.local/share/.cpan/sources/authors/01mailrc.txt.gz'
............................................................................DONE
Fetching with LWP:
http://www.cpan.org/modules/02packages.details.txt.gz
Reading '/root/.local/share/.cpan/sources/modules/02packages.details.txt.gz'
  Database was generated on Mon, 27 Apr 2020 07:29:03 GMT
............................................................................DONE
Fetching with LWP:
http://www.cpan.org/modules/03modlist.data.gz
Reading '/root/.local/share/.cpan/sources/modules/03modlist.data.gz'
DONE
Writing /root/.local/share/.cpan/Metadata
Running install for module 'Math::Round'
Fetching with LWP:
http://www.cpan.org/authors/id/G/GR/GROMMEL/Math-Round-0.07.tar.gz
Fetching with LWP:
http://www.cpan.org/authors/id/G/GR/GROMMEL/CHECKSUMS
Checksum for /root/.local/share/.cpan/sources/authors/id/G/GR/GROMMEL/Math-Round-0.07.tar.gz ok
Scanning cache /root/.local/share/.cpan/build for sizes
DONE
Configuring G/GR/GROMMEL/Math-Round-0.07.tar.gz with Makefile.PL
Checking if your kit is complete...
Looks good
Generating a Unix-style Makefile
Writing Makefile for Math::Round
Writing MYMETA.yml and MYMETA.json
  GROMMEL/Math-Round-0.07.tar.gz
  /usr/bin/perl Makefile.PL -- OK
Running make for G/GR/GROMMEL/Math-Round-0.07.tar.gz
cp Round.pm blib/lib/Math/Round.pm
AutoSplitting blib/lib/Math/Round.pm (blib/lib/auto/Math/Round)
Manifying 1 pod document
  GROMMEL/Math-Round-0.07.tar.gz
  /usr/bin/make -- OK
Running make test for GROMMEL/Math-Round-0.07.tar.gz
PERL_DL_NONLAZY=1 "/usr/bin/perl" "-Iblib/lib" "-Iblib/arch" test.pl
1..11
ok 1
round............ok 2
round_even.......ok 3
round_odd........ok 4
round_rand.......ok 5
nearest..........ok 6
nearest_ceil.....ok 7
nearest_floor....ok 8
nearest_rand.....ok 9
nlowmult.........ok 10
nhimult..........ok 11
All tests successful.
Terminal does not support GetHistory.
Lockfile removed.
  GROMMEL/Math-Round-0.07.tar.gz
  /usr/bin/make test -- OK
Running make install for GROMMEL/Math-Round-0.07.tar.gz
Manifying 1 pod document
Installing /usr/local/share/perl5/5.30/auto/Math/Round/autosplit.ix
Installing /usr/local/share/perl5/5.30/Math/Round.pm
Installing /usr/local/share/man/man3/Math::Round.3pm
Appending installation info to /usr/lib64/perl5/perllocal.pod
  GROMMEL/Math-Round-0.07.tar.gz
  /usr/bin/make install  -- OK

cpan[2]> exit
[bou@bous-fed31 fog_climatology_2019]$ 




2020
==============================================================================================================
==============================================================================================================
==============================================================================================================
==============================================================================================================
[bou@bous-fed31 fog_climatology_2019]$ ls -ltr tcz_se_qld/
total 5119076
-rwxrwxrwx. 1 bou bou     30777 Mar 29 18:38 HM01X_Notes_999999999780442.txt
-rwxrwxrwx. 1 bou bou 601162787 Mar 29 18:40 HM01X_Data_040004.txt
-rwxrwxrwx. 1 bou bou 684818939 Mar 29 18:42 HM01X_Data_040211.txt
-rwxrwxrwx. 1 bou bou 677087489 Mar 29 18:45 HM01X_Data_040842.txt
-rwxrwxrwx. 1 bou bou 699195710 Mar 29 18:47 HM01X_Data_040717.txt
-rwxrwxrwx. 1 bou bou 709337882 Mar 29 18:50 HM01X_Data_040861.txt
-rwxrwxrwx. 1 bou bou 590709494 Mar 29 18:52 HM01X_Data_041359.txt
-rwxrwxrwx. 1 bou bou 588509291 Mar 29 18:54 HM01X_Data_041529.txt
-rwxrwxrwx. 1 bou bou 691017140 Mar 29 18:56 HM01X_Data_039083.txt
-rwxrwxrwx. 1 bou bou      1591 Mar 29 18:56 HM01X_StnDet_999999999780442.txt


columns we read from tcz

[bou@bous-fed31 fog_climatology_2019]$ sed -n 20,93p tcz_se_qld/HM01X_Notes_999999999780442.txt

1-2           ,2           , Record identifier - hm
4-9           ,6           , Bureau of Meteorology Station Number.
11-17         ,7           , Month/Year site opened. (MM/YYYY)
19-25         ,7           , Month/Year site closed. (MM/YYYY)
27-31         ,5           , WMO (World Meteorological Organisation) Index Number.
33-38         ,6           , Aviation ID.
40-45         ,6           , Height of station above mean sea level, in m.
47-62         ,16          , Day/Month/Year Hour24:Minutes in DD/MM/YYYY HH24:MI format in Universal coordinated time
64            ,1           , Message type
66-71         ,6           , Precipitation in last 10 minutes in mm
73-78         ,6           , Precipitation since 9am local time in mm
80-84         ,5           , Air Temperature in degrees C
86-90         ,5           , Wet bulb temperature in degrees C
92-96         ,5           , Dew point temperature in degrees C
98-100        ,3           , Relative humidity in percentage %
102-107       ,6           , Vapour pressure in hPa
109-114       ,6           , Saturated vapour pressure in hPa
116-120       ,5           , Wind speed in knots
122-124       ,3           , Wind direction in degrees
126-130       ,5           , Speed of maximum wind gust in last 10 minutes in knots
132           ,1           , Cloud amount (of first group) in eighths
134-135       ,2           , Cloud type (of first group) in code
137-141       ,5           , Cloud height (of first group) in feet
143           ,1           , Cloud amount (of second group) in eighths
145-146       ,2           , Cloud type (of second group) in code
148-152       ,5           , Cloud height (of second group) in feet
154           ,1           , Cloud amount (of third group) in eighths
156-157       ,2           , Cloud type (of third group) in code
159-163       ,5           , Cloud height (of third group) in feet
165           ,1           , Cloud amount (of fourth group) in eighths
167-168       ,2           , Cloud type (of fourth group) in code
170-174       ,5           , Cloud height (of fourth group) in feet
176           ,1           , Ceilometer cloud amount (of first group) in eighths
178-182       ,5           , Ceilometer cloud height (of first group) in feet
184           ,1           , Ceilometer cloud amount (of second group) in eighths
186-190       ,5           , Ceilometer cloud height (of second group) in feet
192           ,1           , Ceilometer cloud amount (of third group) in eighths
194-198       ,5           , Ceilometer cloud height (of third group) in feet
200           ,1           , Ceilometer sky clear flag
202-207       ,6           , Horizontal visibility in km
209-211       ,3           , Direction of minimum visibility in degrees
213-218       ,6           , Automatic Weather Station visibility in km
220-221       ,2           , Present weather in code
223           ,1           , Intensity of first present weather in code
225-226       ,2           , Descriptor of first present weather in code
228-233       ,6           , Type of first present weather in code
235           ,1           , Intensity of second present weather in code
237-238       ,2           , Descriptor of second present weather in code
240-245       ,6           , Type of second present weather in code
247           ,1           , Intensity of third present weather in code
249-250       ,2           , Descriptor of third present weather in code
252-257       ,6           , Type of third  present in code
259-260       ,2           , Descriptor of first recent weather in code
262-267       ,6           , Type of first recent weather in code
269-270       ,2           , Descriptor of second recent weather in code
272-277       ,6           , Type of second recent weather in code
279-280       ,2           , Descriptor of third recent weather in code
282-287       ,6           , Type of third recent weather in code
289-290       ,2           , AWS present weather in code
292-293       ,2           , AWS weather for last 15 minutes in code
295-296       ,2           , AWS weather for last 60 minutes in code
298-303       ,6           , Mean sea level pressure in hPa
305-310       ,6           , Station level pressure in hPa
312-317       ,6           , QNH pressure in hPa
319-320       ,2           , Automatic Weather Station Flag
322           ,1           , Error Flag
324-339       ,16          , Date of last modification
341-342       ,2           , Data source
344-1367      ,1024        , Original Message
1369-1608     ,240         , Comment text
1610-1849     ,240         , Remarks text
1851          ,1           , CAVOK/SKY clear flag
1853          ,1           , Runway wind shear flag
1855          ,1           , # symbol, end of record indicator.

Grab columm names (3rd column) into a file than read intp dataframe to extract col names
read only 1st 73 lines that describe col names - lines 20 to 93  => sed -n 20, 93p
extract 3rd col awk -F , '{print $3}'

bou@bous-fed31 fog_climatology_2019]$ sed -n 20,93p tcz_se_qld/HM01X_Notes_999999999780442.txt |
 awk -F , '{print $3}'  > tcz_columns.txt

>>> tcz_cols = pd.read_fwf('tcz_columns.txt',header=None)
>>> tcz_cols
                                                    0    1    2    3    4
0                              Record identifier - hm  NaN  NaN  NaN  NaN
1               Bureau of Meteorology Station Number.  NaN  NaN  NaN  NaN
2                   Month/Year site opened. (MM/YYYY)  NaN  NaN  NaN  NaN
3                   Month/Year site closed. (MM/YYYY)  NaN  NaN  NaN  NaN
4   WMO (World Meteorological Organisation) Index ...  NaN  NaN  NaN  NaN
..                                                ...  ...  ...  ...  ...
69                                       Comment text  NaN  NaN  NaN  NaN
70                                       Remarks text  NaN  NaN  NaN  NaN
71                               CAVOK/SKY clear flag  NaN  NaN  NaN  NaN
72                             Runway wind shear flag  NaN  NaN  NaN  NaN
73                                           # symbol  NaN  NaN  NaN  NaN

>>> tcz_cols.iloc[:,0]    (same output as tcz_cols.drop([1,2,3,4], axis=1))
0                                Record identifier - hm
1                 Bureau of Meteorology Station Number.
2                     Month/Year site opened. (MM/YYYY)
3                     Month/Year site closed. (MM/YYYY)
4     WMO (World Meteorological Organisation) Index ...
                            ...                        
69                                         Comment text
70                                         Remarks text
71                                 CAVOK/SKY clear flag
72                               Runway wind shear flag
73                                             # symbol

awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '  tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt


We only reading in those col names from tcz data  (see field)

>>> orig.columns
Index(['Aviation ID', 'Message type', 'AWS Flag',
       'Wind direction in degrees true', 'Wind speed in knots',
       'Speed of maximum windgust in last 10 minutes in knots',
       'Horizontal visibility in km',
       'Direction of minimum visibility in degrees', 'AWS visibility in km',
       'Present weather in code',
       'Descriptor of first present weather in codeType of first present weather in code',  #note col concats
       'Descriptor of second present weather in codeType of second present weather in code',
       'Descriptor of third present weather in codeType of third present weather in code',
       'AWS present weather in code',
       'AWS weather for last 15 minutes in code',
       'AWS weather for last 60 minutes in code',
       'Cloud amount (of first group) in eighths',
       'Cloud height (of first group) in feet',
       'Cloud amount(of second group) in eighths',
       'Cloud height (of second group) in feet',
       'Cloud amount (of third group) in eighths',
       'Cloud height (of third group) in feet',
       'Ceilometer cloud amount (of first group)',
       'Ceilometer cloud height (of first group) in feet',
       'Ceilometer cloud amount (of second group)',
       'Ceilometer cloud height (of second group) in feet',
       'Ceilometer cloud amount (of third group)',
       'Ceilometer cloud height (of third group) in feet',
       'Ceilometer sky clear flag', 'Air Temperature in degrees C',
       'Dew point temperature in degrees C',
       'Relative humidity in percentage %', 'QNH pressure in hPa',
       'Precipitation in last 10 minutes in mm',
       'Precipitation since 9am local time in mm'],
      dtype='object')


Replace with these short column names
Note the 'Day/Month/Year Hour24:Minutes in DD/MM/YYYY HH24:MI format in Universal coordinated time' goes in as index

new_cols = [ "avID","mtype","awsflag","wind_DIR","wind_SPD","wind_GST","vis_obs","vis_dir","vis_aws","PW","PW1","PW2","PW3","PW_auto","PW_15","PW_60","obs_CL1A","obs_CL1H","obs_CL2A","obs_CL2H","obs_CL3A","obs_CL3H","aws_CL1A","aws_CL1H","aws_CL2A","aws_CL2H","aws_CL3A","aws_CL3H","skc_flag","T","Td","RH","QNH","pptn10min","pptnSince9"]

check they match !!

see https://www.geeksforgeeks.org/create-pandas-dataframe-from-lists-using-zip/

>>> pd.DataFrame(
... list(zip(new_cols,list(orig.columns))),
... columns=['new_col_names', 'tcz_col_names'])

   new_col_names                                      tcz_col_names
0           avID                                        Aviation ID
1          mtype                                       Message type
2        awsflag                                           AWS Flag
3       wind_DIR                     Wind direction in degrees true
4       wind_SPD                                Wind speed in knots
5       wind_GST  Speed of maximum windgust in last 10 minutes i...
6        vis_obs                        Horizontal visibility in km
7        vis_dir         Direction of minimum visibility in degrees
8        vis_aws                               AWS visibility in km
9             PW                            Present weather in code
10           PW1  Descriptor of first present weather in codeTyp...
11           PW2  Descriptor of second present weather in codeTy...
12           PW3  Descriptor of third present weather in codeTyp...
13       PW_auto                        AWS present weather in code
14         PW_15            AWS weather for last 15 minutes in code
15         PW_60            AWS weather for last 60 minutes in code
16      obs_CL1A           Cloud amount (of first group) in eighths
17      obs_CL1H              Cloud height (of first group) in feet
18      obs_CL2A           Cloud amount(of second group) in eighths
19      obs_CL2H             Cloud height (of second group) in feet
20      obs_CL3A           Cloud amount (of third group) in eighths
21      obs_CL3H              Cloud height (of third group) in feet
22      aws_CL1A           Ceilometer cloud amount (of first group)
23      aws_CL1H   Ceilometer cloud height (of first group) in feet
24      aws_CL2A          Ceilometer cloud amount (of second group)
25      aws_CL2H  Ceilometer cloud height (of second group) in feet
26      aws_CL3A           Ceilometer cloud amount (of third group)
27      aws_CL3H   Ceilometer cloud height (of third group) in feet
28      skc_flag                          Ceilometer sky clear flag
29             T                       Air Temperature in degrees C
30            Td                 Dew point temperature in degrees C
31            RH                  Relative humidity in percentage %
32           QNH                                QNH pressure in hPa
33     pptn10min             Precipitation in last 10 minutes in mm
34    pptnSince9           Precipitation since 9am local time in mm


They do - so great !!!


>>> orig['mtype'].value_counts()
m    323357
s     30142
M      9477
S       462
Name: mtype, dtype: int64




see https://thispointer.com/python-pandas-how-to-display-full-dataframe-i-e-print-all-rows-columns-without-truncation/
>>> pd.options.display.max_rows
60
>>> pd.set_option('display.max_rows',None)

>>> pd.options.display.max_rows
>>> cols_compare

'{print $6 "," $8 "," $9 "," $64 "," $19  "," $18 "," $20 "," $40 "," $41 "," $42 "," $43 "," $45$46 "," $48$49 "," $51$52 "," 
 $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $39 "," $12 "," $14 "," $15 "," $64 ","$10 "," $11 } ' 

awk -F , '{print $6 "," $8 "," $9 "," $65 "," $19  "," $18 "," $20 "," $40 "," $41 "," $42 "," $43 "," $45$46 "," $48$49 "," $51$52 ","
$59 ","$60 ","$61 ","  $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $39 "," $12 "," $14 "," $15 "," $64 ","$10 "," $11 } '

awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '
awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '  tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt


>>> cols_compare
        new_col_names                                      tcz_col_names  << Cols we keep 40 columns
0                  hm                             Record identifier - hm
1               StNum              Bureau of Meteorology Station Number.
2              Opened                  Month/Year site opened. (MM/YYYY)
3              Closed                  Month/Year site closed. (MM/YYYY)
4             WMO_Num  WMO (World Meteorological Organisation) Index ...
5                AvID                                       Aviation ID.   <<<<
6         Elevation_m             Height of station above mean sea level
7                 UTC  Day/Month/Year Hour24:Minutes in DD/MM/YYYY HH...   <<<<   becomes index/key
8              M_type                                       Message type   <<<<
9           pptn10min             Precipitation in last 10 minutes in mm   <<<<
10            pptn9am           Precipitation since 9am local time in mm   <<<<
11                  T                       Air Temperature in degrees C   <<<<
12                Twb                  Wet bulb temperature in degrees C   
13                 Td                 Dew point temperature in degrees C   <<<<
14                 RH                  Relative humidity in percentage %   <<<<
15                 VP                             Vapour pressure in hPa   
16                SVP                   Saturated vapour pressure in hPa   
17                 WS                                Wind speed in knots   <<<<
18               WDir                          Wind direction in degrees   <<<<
19       MaxGust10min  Speed of maximum wind gust in last 10 minutes ...   <<<<
20           CL1_amnt           Cloud amount (of first group) in eighths   <<<<
21           CL1_type                Cloud type (of first group) in code   
22             CL1_ht              Cloud height (of first group) in feet   <<<<
23           CL2_amnt          Cloud amount (of second group) in eighths   <<<<
24           CL2_type               Cloud type (of second group) in code
25             CL2_ht             Cloud height (of second group) in feet   <<<<
26           CL3_amnt           Cloud amount (of third group) in eighths   <<<<
27           CL3_type                Cloud type (of third group) in code
28             CL3_ht              Cloud height (of third group) in feet   <<<<
29           CL4_amnt          Cloud amount (of fourth group) in eighths
30           CL4_type               Cloud type (of fourth group) in code
31             CL4_ht             Cloud height (of fourth group) in feet
32         Ceil1_amnt  Ceilometer cloud amount (of first group) in ei...   <<<<
33           Ceil1_ht   Ceilometer cloud height (of first group) in feet   <<<<
34         Ceil2_amnt  Ceilometer cloud amount (of second group) in e...   <<<<
35           Ceil2_ht  Ceilometer cloud height (of second group) in feet   <<<<
36         Ceil3_amnt  Ceilometer cloud amount (of third group) in ei...   <<<<
37           Ceil3_ht   Ceilometer cloud height (of third group) in feet   <<<<
38        CeilSKCflag                          Ceilometer sky clear flag   <<<<
39                vis                        Horizontal visibility in km   <<<<
40        vis_min_dir         Direction of minimum visibility in degrees   <<<<
41            vis_aws         Automatic Weather Station visibility in km   <<<<
42                 PW                            Present weather in code   <<<<
43            PW1_int         Intensity of first present weather in code
44           PW1_desc        Descriptor of first present weather in code   <<<<
45           PW1_type              Type of first present weather in code   <<<<
46            PW2_int        Intensity of second present weather in code
47           PW2_desc       Descriptor of second present weather in code   <<<<
48           PW2_type             Type of second present weather in code   <<<<
49            PW3_int         Intensity of third present weather in code
50           PW3_desc        Descriptor of third present weather in code   <<<<
51           PW3_type                     Type of third  present in code   <<<<
52           RW1_desc         Descriptor of first recent weather in code
53           RW1_type               Type of first recent weather in code
54           RW2_desc        Descriptor of second recent weather in code
55           RW2_type              Type of second recent weather in code
56           RW3_desc         Descriptor of third recent weather in code
57           RW3_type               Type of third recent weather in code
58             AWS_PW                        AWS present weather in code   <<<<
59        AWS_PW15min            AWS weather for last 15 minutes in code   <<<<
60        AWS_PW60min            AWS weather for last 60 minutes in code   <<<<
61                MSL                     Mean sea level pressure in hPa   
62                SLP                      Station level pressure in hPa
63                QNH                                QNH pressure in hPa   <<<<
64           AWS_Flag                     Automatic Weather Station Flag   <<<<
65           Err_Flag                                         Error Flag
66           Date_mod                          Date of last modification
67           Data_src                                        Data source
68                MSG                                   Original Message
69             Coment                                       Comment text
70                RMK                                       Remarks text
71     CAVOK_SKC_flag                               CAVOK/SKY clear flag   <<<<
72  RWY_ws_shear_flag                             Runway wind shear flag
73                END                                           # symbol


>>> cols = ['avID', 'mtype', 'awsflag', 'wind_DIR', 'wind_SPD', 'wind_GST',
... 'vis_obs', 'vis_dir', 'vis_aws', 'PW', 'PW1', 'PW2', 'PW3', 'PW_auto',
... 'PW_15', 'PW_60', 'obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H',
... 'obs_CL3A', 'obs_CL3H', 'aws_CL1A', 'aws_CL1H', 'aws_CL2A', 'aws_CL2H',
... 'aws_CL3A', 'aws_CL3H', 'skc_flag', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']
>>> len(cols)
35
>>> Note there is also "UTC"- date col and the fields Pw1 PW2 and PW3 is 2 fields each so 6 cols
so altogether about 42 columns

awk grabs these 42 columns out of original tcz file 

awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52",
  "$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15",
  "$64","$10","$11} '


new_cols = ['avID', 'mtype', 'awsflag', 'wind_DIR', 'wind_SPD', 'wind_GST','vis_obs', 'vis_dir', 'vis_aws', 'PW', 'PW1', 'PW2', 'PW3', 'PW_auto','PW_15', 'PW_60', 'obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1A', 'aws_CL1H', 'aws_CL2A', 'aws_CL2H','aws_CL3A', 'aws_CL3H', 'skc_flag', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']


dtype =  {'avID':str, 'mtype':str, 'awsflag':np.int32, 'wind_DIR':str, 'wind_SPD':np.float64, 'wind_GST':np.float64,
'vis_obs':np.float64, 'vis_dir':str, 'vis_aws':np.float64, 'PW':np.int32, 'PW1':str, 'PW2':str, 'PW3':str, 'PW_auto':np.int32,
'PW_15':np.int32, 'PW_60':np.int32, 'obs_CL1A':np.int32, 'obs_CL1H':np.int32, 'obs_CL2A':np.int32, 'obs_CL2H':np.int32,
'obs_CL3A':np.int32, 'obs_CL3H':np.int32, 'aws_CL1A':str, 'aws_CL1H':np.int32, 'aws_CL2A':str, 'aws_CL2H':np.int32,
'aws_CL3A':str, 'aws_CL3H':np.int32, 'skc_flag':str, 'T':np.float64, 'Td':np.float64, 'RH':np.float64, 'QNH':np.float64, 'pptn10min':np.float64,'pptnSince9':np.float64}

converters =  {'avID':str, 'mtype':str, 'awsflag':pd.Int64Dtype(), 'wind_DIR':str, 'wind_SPD':np.float64, 'wind_GST':np.float64,
'vis_obs':np.float64, 'vis_dir':str, 'vis_aws':np.float64, 'PW':pd.Int64Dtype(), 'PW1':str, 'PW2':str, 'PW3':str, 'PW_auto':pd.Int64Dtype(),
'PW_15':pd.Int64Dtype(), 'PW_60':pd.Int64Dtype(), 'obs_CL1A':pd.Int64Dtype(), 'obs_CL1H':pd.Int64Dtype(), 'obs_CL2A':pd.Int64Dtype(), 'obs_CL2H':pd.Int64Dtype(),
'obs_CL3A':pd.Int64Dtype(), 'obs_CL3H':pd.Int64Dtype(), 'aws_CL1A':str, 'aws_CL1H':pd.Int64Dtype(), 'aws_CL2A':str, 'aws_CL2H':pd.Int64Dtype(),
'aws_CL3A':str, 'aws_CL3H':pd.Int64Dtype(), 'skc_flag':str, 'T':np.float64, 'Td':np.float64, 'RH':np.float64, 'QNH':np.float64, 'pptn10min':np.float64,'pptnSince9':np.float64}

only use converters for int type conversion
converters =  {'awsflag':pd.Int64Dtype(),'PW':pd.Int64Dtype(),'PW_auto':pd.Int64Dtype(),'PW_15':pd.Int64Dtype(), 'PW_60':pd.Int64Dtype(), 'obs_CL1A':pd.Int64Dtype(),'obs_CL1H':pd.Int64Dtype(), 'obs_CL2A':pd.Int64Dtype(), 'obs_CL2H':pd.Int64Dtype(),'obs_CL3A':pd.Int64Dtype(), 'obs_CL3H':pd.Int64Dtype(),'aws_CL1H':pd.Int64Dtype(), 'aws_CL2H':pd.Int64Dtype(),'aws_CL3H':pd.Int64Dtype()}

new_cols = ['avID', 'mtype', 'awsflag', 'wind_DIR', 'wind_SPD', 'wind_GST','vis_obs', 'vis_dir', 'vis_aws', 'PW', 'PW1', 'PW2', 'PW3', 'PW_auto','PW_15', 'PW_60', 'obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1A', 'aws_CL1H', 'aws_CL2A', 'aws_CL2H','aws_CL3A', 'aws_CL3H', 'skc_flag', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']

col_types = {'avID':str, 'mtype':str,  'wind_DIR':str, 'wind_SPD':np.float64, 'wind_GST':np.float64,'vis_obs':np.float64, 'vis_dir':str, 'vis_aws':np.float64,  'PW1':str, 'PW2':str, 'PW3':str, 'aws_CL1A':str, 'aws_CL2A':str, 'aws_CL3A':str,  'skc_flag':str, 'T':np.float64, 'Td':np.float64, 'RH':np.float64, 'QNH':np.float64, 'pptn10min':np.float64,'pptnSince9':np.float64}

TypeError: 'Int64Dtype' object is not callable


>>> df = pd.read_csv('YBBN1.csv', parse_dates=[1], dayfirst=True, infer_datetime_format=True, index_col=[1], names=new_cols, skiprows=1, header=None)
sys:1: DtypeWarning: Columns (3,4,5,6,29,30,31,32,33) have mixed types. Specify dtype option on import or set low_memory=False.


integer data types - no fractional parts

cols_int = ['awsflag', 'PW', 'PW_auto','PW_15', 'PW_60', 'obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1H', 'aws_CL2H', 'aws_CL3H']

>>> df[cols_int].astype('int32').dtypes
>>> df.astype({'awsflag': 'int32'}).dtypes

floats 
['wind_SPD', 'wind_GST','vis_obs', 'vis_aws','T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']

String data types
['avID', 'mtype', 'wind_DIR', 'vis_dir', 'PW1', 'PW2', 'PW3' , 'aws_CL1A','aws_CL2A','aws_CL3A', 'skc_flag'

skc_flag : can't make this numeric as can't test for sky obscured
skc_flag = {0: sky is not clear below 12500 ft, 1: sky is clear below 12500 ft , 
NULL: and associated cloud fields are NULL -sky may be obscured, 
0: A flag of 0 when associated cloud fields are NULL indicates an instrument error}

>>> df['skc_flag'].astype('category').value_counts()
1    157575
0    121085
      32372
1     27447
0     24695
2       264
Name: skc_flag, dtype: int64


>>> df['mtype'].astype('category')
2000-01-01 00:00:00    M
2000-01-01 00:30:00    M
2000-01-01 01:00:00    M
2000-01-01 01:30:00    M
2000-01-01 02:00:00    M
                      ..
2020-03-29 16:00:00    m
2020-03-29 16:30:00    m
2020-03-29 17:00:00    m
2020-03-29 17:30:00    m
2020-03-29 18:00:00    m
Name: mtype, Length: 363438, dtype: category
Categories (4, object): [M, S, m, s]

>>> df['mtype'].value_counts()
m    323357
s     30142
M      9477
S       462
Name: mtype, dtype: int64

>>> df['mtype'].astype('category').value_counts()
m    323357
s     30142
M      9477
S       462
Name: mtype, dtype: int64

>>> df['mtype'].isna().sum()
0  (not missing)


>>> df['awsflag'].astype('category').value_counts()
2     345036
        9890
 2      6456
1       1972
0         46
 1        35
 0         3
Name: awsflag, dtype: int64



>>> df['awsflag'].value_counts()
2     345036
        9890
 2      6456
1       1972
0         46
 1        35
 0         3
Name: awsflag, dtype: int64


>>> df['awsflag'].str.strip().value_counts()
     9890
2    6456
1      35
0       3
Name: awsflag, dtype: int64


We got some real problems with data that is read !!!!!



Column names have whitespace at start or end or in middle or braces '(' or ')'  - how TO fix
whitespace at start or end 		==>> df.columns.str.strip()
whitespace in middle			==>> df.columns.str.replace(' ', '_')  (replaces with undescore
 braces '(' or ')'			    ==>> df.columns.str.replace('(', '').str.replace(')', '')

pandas has a convenient .str method that you can use on text data. 
Since the column names are an ‘index’ type, you can use .str on them too. 
You can fix all these lapses of judgement by chaining together a bunch of these .str functions. Like so:

df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')




===================================================================
>>> df[df['awsflag'] == 2]['awsflag'].count()   # integer value 2
345036

>>> df[df['awsflag'] == '2']['awsflag'].count() # string 2
0

>>> df[df['awsflag'] == ' 2']['awsflag'].count() # string with space b4 value 2
6456

>>> df[df['awsflag'] == '']['awsflag'].count()  #empty string
0

>>> df[df['awsflag'] == '  ']['awsflag'].count() # string two whitepspace wide
9890


>>> #regular expression that removes surrounding whitespace
>>> df['awsflag'].value_counts()
2     345036
        9890
 2      6456
1       1972
0         46
 1        35
 0         3
Name: awsflag, dtype: int64

replace on eof more whitespace with empty string

>>> df['awsflag'] = df['awsflag'].replace(r'\s*', '', regex=True)
>>> df['awsflag'].value_counts()
2    345036
       9890
2      6456  <-- fixed
1      1972
0        46
1        35  <-- fixed
0         3  <-- fixed

replace any value that is empty string '' with a numeric 0

>>> df['awsflag'] = df['awsflag'].replace(r'', 0, regex=True)
>>> df['awsflag'].value_counts()
2    345036
0      9936 <-- fixed  9890 + 46 = 9936
2      6456
1      1972
1        35
0         3



why we getting repeats same value 2 and 1s

This tells us why
>>> df['awsflag'].unique()
array([0, '2', '1', '0', 2, 1], dtype=object)

Fix string repersentation to real int numbers

>>> df['awsflag'] = df['awsflag'].replace(r'2', 2, regex=True)
>>> df['awsflag'] = df['awsflag'].replace(r'1', 1, regex=True)
>>> df['awsflag'] = df['awsflag'].replace(r'0', 0, regex=True)

>>> df['awsflag'].unique()
array([0, 2, 1])

>>> df['awsflag'].value_counts()
2    351492
0      9939
1      2007
Name: awsflag, dtype: int64

Now that all values are int - we can convert all to int
but look at info() and its already an int

>>> df.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 363438 entries, 2000-01-01 00:00:00 to 2020-03-29 18:00:00
Data columns (total 35 columns):
awsflag       363438 non-null int64

Don't force to int like this - then you can't get categorical value counts

>>> df['awsflag']= df['awsflag'].astype('int32').dtypes
>>> df['awsflag'].value_counts()
int32    363438
Name: awsflag, dtype: int64

# side effect we can't count them as categories now
So leave it as string 

same issue with skc_flag
>>> df['skc_flag'].unique()
array([' ', '0', '2', '1', 1, 0], dtype=object)


https://pandas.pydata.org/pandas-docs/stable/user_guide/missing_data.html

>>> df['skc_flag'] = df['skc_flag'].replace(r'0', 0, regex=True)
>>> df['skc_flag'] = df['skc_flag'].replace(r'1', 1, regex=True)
>>> df['skc_flag'] = df['skc_flag'].replace(r'2', 2, regex=True)
>>> df['skc_flag'] = df['skc_flag'].replace(r' ', -99, regex=True)
>>> df['skc_flag'].unique()
array([-99,   0,   2,   1])

>>> df['skc_flag'].value_counts()
 1     185022
 0     145780
-99     32372
 2        264
Name: skc_flag, dtype: int64


detect if there are missing values pandas has the isna() function. 
This returns True if the value is missing and False if not. 
We can use this to determine the volume of missing values per column as shown below.

>>> df.apply(lambda x: sum(x.isna()) / len(df))
avID          0.0
mtype         0.0
awsflag       0.0
wind_DIR      0.0
wind_SPD      0.0
wind_GST      0.0
vis_obs       0.0
vis_dir       0.0
vis_aws       0.0
PW            0.0
PW1           0.0
PW2           0.0
PW3           0.0
PW_auto       0.0
PW_15         0.0
PW_60         0.0
obs_CL1A      0.0
obs_CL1H      0.0
obs_CL2A      0.0
obs_CL2H      0.0
obs_CL3A      0.0
obs_CL3H      0.0
aws_CL1A      0.0
aws_CL1H      0.0
aws_CL2A      0.0
aws_CL2H      0.0
aws_CL3A      0.0
aws_CL3H      0.0
skc_flag      0.0
T             0.0
Td            0.0
RH            0.0
QNH           0.0
pptn10min     0.0
pptnSince9    0.0
dtype: float64


Thats not right as there are many missing values
Lets read data in again and this time we force conversion of all data that look like number (int/float) to numeric

floats 
++++++

cols_float = ['wind_SPD', 'wind_GST','vis_obs', 'vis_aws','T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']

String data types
+++++++++++++++++
['avID', 'mtype', 'PW1', 'PW2', 'PW3' , 'aws_CL1A','aws_CL2A','aws_CL3A']

Integer data types - no fractional parts
+++++++++++++++++++++++++++++++++++++++++

cols_int = ['awsflag','skc_flag', 'wind_DIR', 'vis_dir','PW', 'PW_auto','PW_15', 'PW_60', 'obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1H', 'aws_CL2H', 'aws_CL3H']


>>> df[cols_int].astype('int32').dtypes
>>> df.astype({'awsflag': 'int32'}).dtypes

df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='signed')   # would keep integers as numbers without decimal point FAILS
df[int_cols] = df[int_cols].apply(pd.to_numeric, errors='ignore')     # ignoer any errors   - also FAILS


>>> df[cols_num] = df[cols_num].apply(pd.to_numeric, downcast='signed')
Traceback (most recent call last):
  File "pandas/_libs/lib.pyx", line 1897, in pandas._libs.lib.maybe_convert_numeric
ValueError: ('Unable to parse string "  " at position 0', 'occurred at index awsflag')

>>> df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='float')
Traceback (most recent call last):
  File "pandas/_libs/lib.pyx", line 1897, in pandas._libs.lib.maybe_convert_numeric
ValueError: Unable to parse string "  "


no choice but to lump all int like data with floats and convert all to float
this way at least missing values get np.NAN which is much better


>>> cols_num = ['awsflag',  'skc_flag', 'wind_DIR', 'wind_SPD', 'wind_GST','vis_obs', 'vis_dir', 'vis_aws', 'PW', 'PW_auto','PW_15', 'PW_60','obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1H', 'aws_CL2H', 'aws_CL3H', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']
>>> cols_float = ['wind_SPD', 'wind_GST','vis_obs', 'vis_aws','T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']
>>> cols_int = ['awsflag','skc_flag', 'wind_DIR', 'vis_dir','PW', 'PW_auto','PW_15', 'PW_60', 'obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1H', 'aws_CL2H', 'aws_CL3H']
>>> len(cols_num) 27
>>> len(cols_int + cols_float) 27
>>> set(cols_int + cols_float) == set(cols_num) True
>>> sorted(cols_int + cols_float) == sorted(cols_num) True

>>> cols_num = ['awsflag',  'skc_flag', 'wind_DIR', 'wind_SPD', 'wind_GST','vis_obs', 'vis_dir', 'vis_aws', 'PW', 'PW_auto','PW_15', 'PW_60','obs_CL1A', 'obs_CL1H', 'obs_CL2A', 'obs_CL2H','obs_CL3A', 'obs_CL3H', 'aws_CL1H', 'aws_CL2H', 'aws_CL3H', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']

>>> df = pd.read_csv('YBBN1.csv', parse_dates=[1], dayfirst=True, infer_datetime_format=True, index_col=[1], names=new_cols, skiprows=1, header=None)


>>> df.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 363438 entries, 2000-01-01 00:00:00 to 2020-03-29 18:00:00
Data columns (total 35 columns):
avID          363438 non-null object
mtype         363438 non-null object
awsflag       363438 non-null object
wind_DIR      363438 non-null object
wind_SPD      363438 non-null object
wind_GST      363438 non-null object
vis_obs       363438 non-null object
vis_dir       363438 non-null object
vis_aws       363438 non-null object
PW            363438 non-null object
PW1           363438 non-null object
PW2           363438 non-null object
PW3           363438 non-null object
PW_auto       363438 non-null object
PW_15         363438 non-null object
PW_60         363438 non-null object
obs_CL1A      363438 non-null object
obs_CL1H      363438 non-null object
obs_CL2A      363438 non-null object
obs_CL2H      363438 non-null object
obs_CL3A      363438 non-null object
obs_CL3H      363438 non-null object
aws_CL1A      363438 non-null object
aws_CL1H      363438 non-null object
aws_CL2A      363438 non-null object
aws_CL2H      363438 non-null object
aws_CL3A      363438 non-null object
aws_CL3H      363438 non-null object
skc_flag      363438 non-null object
T             363438 non-null object
Td            363438 non-null object
RH            363438 non-null object
QNH           363438 non-null object
pptn10min     363438 non-null object
pptnSince9    363438 non-null object
dtypes: object(35)
memory usage: 99.8+ MB

>>> df.apply(lambda x: sum(x.isna()) / len(df))
avID          0.0
mtype         0.0
awsflag       0.0
wind_DIR      0.0
wind_SPD      0.0
wind_GST      0.0
vis_obs       0.0
vis_dir       0.0
vis_aws       0.0
PW            0.0
PW1           0.0
PW2           0.0
PW3           0.0
PW_auto       0.0
PW_15         0.0
PW_60         0.0
obs_CL1A      0.0
obs_CL1H      0.0
obs_CL2A      0.0
obs_CL2H      0.0
obs_CL3A      0.0
obs_CL3H      0.0
aws_CL1A      0.0
aws_CL1H      0.0
aws_CL2A      0.0
aws_CL2H      0.0
aws_CL3A      0.0
aws_CL3H      0.0
skc_flag      0.0
T             0.0
Td            0.0
RH            0.0
QNH           0.0
pptn10min     0.0
pptnSince9    0.0
dtype: float64

>>> df[cols_num] = df[cols_num].apply(pd.to_numeric, errors='coerce')

>>> df.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 363438 entries, 2000-01-01 00:00:00 to 2020-03-29 18:00:00
Data columns (total 35 columns):
avID          363438 non-null object
mtype         363438 non-null object
awsflag       353548 non-null float64
wind_DIR      363392 non-null float64
wind_SPD      363405 non-null float64
wind_GST      360925 non-null float64
vis_obs       361237 non-null float64
vis_dir       1890 non-null float64
vis_aws       351625 non-null float64
PW            34884 non-null float64
PW1           363438 non-null object
PW2           363438 non-null object
PW3           363438 non-null object
PW_auto       0 non-null float64
PW_15         0 non-null float64
PW_60         0 non-null float64
obs_CL1A      258605 non-null float64
obs_CL1H      258545 non-null float64
obs_CL2A      147406 non-null float64
obs_CL2H      147405 non-null float64
obs_CL3A      55139 non-null float64
obs_CL3H      55139 non-null float64
aws_CL1A      363438 non-null object
aws_CL1H      158253 non-null float64
aws_CL2A      363438 non-null object
aws_CL2H      86838 non-null float64
aws_CL3A      363438 non-null object
aws_CL3H      49272 non-null float64
skc_flag      331066 non-null float64
T             363374 non-null float64
Td            363362 non-null float64
RH            363362 non-null float64
QNH           361602 non-null float64
pptn10min     363091 non-null float64
pptnSince9    360323 non-null float64
dtypes: float64(27), object(8)
memory usage: 99.8+ MB


This is better - reports many missing values correctly e.g 90% of obs has no PW , 99% has no vis_dir and thats perfectly erasonable

>>> df.apply(lambda x: sum(x.isna()) / len(df))
avID          0.000000
mtype         0.000000
awsflag       0.027212
wind_DIR      0.000127
wind_SPD      0.000091
wind_GST      0.006915
vis_obs       0.006056
vis_dir       0.994800
vis_aws       0.032503
PW            0.904017
PW1           0.000000
PW2           0.000000
PW3           0.000000
PW_auto       1.000000
PW_15         1.000000
PW_60         1.000000
obs_CL1A      0.288448
obs_CL1H      0.288613
obs_CL2A      0.594412
obs_CL2H      0.594415
obs_CL3A      0.848285
obs_CL3H      0.848285
aws_CL1A      0.000000
aws_CL1H      0.564567
aws_CL2A      0.000000
aws_CL2H      0.761065
aws_CL3A      0.000000
aws_CL3H      0.864428
skc_flag      0.089072
T             0.000176
Td            0.000209
RH            0.000209
QNH           0.005052
pptn10min     0.000955
pptnSince9    0.008571
dtype: float64


>>> df['skc_flag'].value_counts()
1.0    185022
0.0    145780
2.0       264
Name: skc_flag, dtype: int64
>>> df['awsflag'].value_counts()
2.0    351492
1.0      2007
0.0        49
Name: awsflag, dtype: int64
>>> df['mtype'].value_counts()
m    323357
s     30142
M      9477
S       462
Name: mtype, dtype: int64


$avID,$datetime, $mtype, $awsflag,$wind_DIR,$wind_SPD,$wind_GST,$vis_obs,$vis_dir,$vis_aws,$PW,$PW1,$PW2,$PW3,$PW_auto,$PW_15,$PW_60,
$obs_CL1A,$obs_CL1H,$obs_CL2A,$obs_CL2H,$obs_CL3A,$obs_CL3H,$aws_CL1A,$aws_CL1H,$aws_CL2A,$aws_CL2H,$aws_CL3A,$aws_CL3H,$skc_flag,$T,$Td,$RH,$QNH,$pptn10min,$pptnSince9


PRESENT WEATHER  ==>> $PW,$PW1,$PW2,$PW3

>>> df['PW'].unique()
array(['  ', '60', '50', '80', '52', '81', '62', '64', '95', '17', '11',
       '41', '54', '42', '10', ' 4', '82', '96', '97', '19', ' 5', '30',
       '58', ' 8', '33', '59', '99'], dtype=object)
>>> df['PW1'].unique()
array(['        ', '      RA', 'SH      ', '      DZ', 'SH    RA',
       'TS    RA', 'TS      ', 'MI    FG', 'BC    FG', 'SH  RADZ',
       '      FG', 'PR    FG', '      BR', '      FU', 'TS    GR',
       '      FC', '      HZ', '      DS', 'BL    DU', '    DZRA',
       '    RADZ', '      PO', '      DU', 'BL    SA', 'DR    DU'],
      dtype=object)
>>> df['PW2'].unique()
array(['        ', '      DZ', 'SH      ', '      BR', '      SQ',
       '      FU', 'BC    FG', 'MI    FG', '      RA', 'SH    RA',
       '      FG', 'PR    FG', 'TS      ', '      HZ', 'TS    RA',
       'TS    GR'], dtype=object)
>>> df['PW3'].unique()
array(['        ', 'SH      ', '      RA', '      FU', '      HZ',
       '      FG', '      BR', 'SH    RA', '      DZ', '      SQ',
       'TS      ', 'MI    FG', 'PR    FG'], dtype=object)

The code meanings below apply to the manual Present Weather observations: 

10    Mist
11,41 Fog patches
12    Shallow fog
28    Recent fog
40    Distant fog
42-49 Fog


WEATHER DESCRIPTORS
MI: shallow
BC: patches
DR: drifting (less than 2 metres above the ground)
BL: blowing (raised 2 or more metres above the ground)
PR: partial


WEATHER TYPE (PHENOMENA)
BR:  mist
FG:  fog


AWS WEATHER codes ==>>  $PW_auto,$PW_15,$PW_60
The code meanings below apply to the AWS present weather, AWS weather for the last 15 minutes and AWS weather for the last 60 minutes fields:
10 Mist
20 Fog at the station during the preceding hour but not at the time of observation
30 Fog
31 Fog or ice fog, in patches
32 Fog or ice fog, has become thinner during the past hour
33 Fog or ice fog, no applicable change during the past hour
34 Fog or ice fog, has begun or become thicker during the past hour


Brisbane does not do AWS PW groups 
>>> df['PW_auto'].unique()
array(['  '], dtype=object)
>>> df['PW_15'].unique()
array(['  '], dtype=object)
>>> df['PW_60'].unique()
array(['  '], dtype=object)



Only two liner to get all fog stats
[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6 "," $8 "," $9 "," $65 "," $19  "," $18 "," $20 "," $40 "," $41 "," $42 "," $43 "," $45$46 "," $48$49 "," $51$52 ","$59 ","$60 ","$61 ","  $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $39 "," $12 "," $14 "," $15 "," $64 ","$10 "," $11 } ' tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt
[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YBBN1.txt YBBN2.csv > YBBN3.txt

To get fog only days
[bou@bous-fed31 fog_climatology_2019]$ head -n 1 YBBN2.csv > YBBN_fog_days.csv; awk -F , '$8 ~ /YES/ {print $0}' YBBN2.csv  >> vinorda_YBBN_fog_days.csv;


Can further modify blank/missing (empty string) rain24hr TO ZERO ==>> if($7 ~ /^ *$/) $7 = "0.0" }
Or even sets all blank fileds to zero!!!! awk 'BEGIN { FS = OFS = "," } { for(i=1; i<=NF; i++) if($i ~ /^ *$/) $i = 0 }; 1' 

Now use this for fog analog model - Roberts stuff misses lot of marginal fog. We want those days too as well
Any fog Partial, Vicinity, Fog Bank - not only thse with vis < 1km
Include MIFG events for now just to see if we can somehow discriminate between shallow and deep fog.

But we definitely not counting MIFG for our stats as that happens a lot.


Download climate zone harlf hourly fewquency metar/speci aws 
Grab Select columns from tcz file
[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '  tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt


Run through perl script to figure fog days
[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YBBN1.txt YBBN2.csv > YBBN3.txt

grab only fog days
[bou@bous-fed31 fog_climatology_2019]$ head -n 1 YBBN2.csv > YBBN_fog_days.csv; awk -F , '$8 ~ /YES/ {print $0}' YBBN2.csv  >> vinorda_YBBN_fog_days.csv;
  

analyse roberts fog file
--------------------------

[bou@bous-fed31 fog_climatology_2019]$ head -n 2 robert_fog/FogSummaryUnfiltered_QLD.csv 
Station.Number	Station	Event.Date	Month	Bimonth	FG.type			Low.DPD.onset		FG.onset			FG.clearance	Speci.clearance	Low.DPD.onset.hour	FG.onset.hour	FG.clearance.hour	Speci.clearance.hour	FG.duration	Thick.FG.duration	Precip_9am.to.onset	Precip.24h.to.9am	Precip.12h.to.9am	Dry.day	FG.detected	Time.of.first.tip	Hour.of.first.tip	Tip.possible.from.dew
27045,"YBWP",2008-01-01,"Jan","JanFeb","Nil fog",NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,7.2,7.2,FALSE,FALSE,2007-12-31 17:30:00,17.5,FALSE


grab Brisbane fog days - column 2 is Station ==>> awk -F',' 'BEGIN {IGNORECASE = 1} $2 ~ /ybbn/'
and col 6 is "post rain" or "no rain fogs"  - so to get all fog events just look for rain pattern in column 6 ==>> awk -F',' '$6 ~ /rain/' 


[bou@bous-fed31 fog_climatology_2019]$ head -n 1 robert_fog/FogSummaryUnfiltered_QLD.csv > roberts_YBBN_fog_days.csv; # grab col headers 1st into a new file
[bou@bous-fed31 fog_climatology_2019]$ awk -F',' 'BEGIN {IGNORECASE = 1} $2 ~ /ybbn/' robert_fog/FogSummaryUnfiltered_QLD.csv | awk -F',' 'BEGIN {IGNORECASE = 1} $6 ~ /rain/' >> roberts_YBBN_fog_days.csv



vinordas fog anal steps to get same data but some different columns
-------------------------------------------------------------------
Extract only certain relevant fields/columns from climate zone data file for Brisbane
awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt

Process using my fog prel script to get fog stats 
./aws_format_v6.5_2020.pl YBBN1.txt YBBN2.csv > YBBN3.txt


Get only fog days ==>> awk -F , '$8 ~ /YES/ {print $0}'
set missing rain24 to 0.0 as well ==>> if($7 ~ /^ *$/) $7 = "0.0" 

head -n 1 YBBN2.csv > vinorda_YBBN_fog_days.csv;  # grab col headers 1st into a new file
awk -F , '$8 ~ /YES/ {print $0}' YBBN2.csv  >> vinorda_YBBN_fog_days.csv;





check out data from original climate zone file for any given day

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt


-------------------------------------------------------------

check_fog_V0() finds ~ 180 fog days
------------------------------------
[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YBBN1.txt YBBN_bb0.csv > YBBN_cc0.txt
[bou@bous-fed31 fog_climatology_2019]$ head -n 1  YBBN_bb0.csv > YBBN_bb0_fog_days.csv 
[bou@bous-fed31 fog_climatology_2019]$ awk -F , '$8 ~ /YES/ {print $0}' YBBN_bb0.csv >> YBBN_bb0_fog_days.csv



check_fog_V1() finds ~ 160 fog days
------------------------------------
[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YBBN1.txt YBBN_bb1.csv > YBBN_cc1.txt
[bou@bous-fed31 fog_climatology_2019]$ head -n 1  YBBN_bb1.csv > YBBN_bb1_fog_days.csv 
[bou@bous-fed31 fog_climatology_2019]$ awk -F , '$8 ~ /YES/ {print $0}' YBBN_bb1.csv >> YBBN_bb1_fog_days.csv


check_fog_V2() finds ~ 325 fog days 
------------------------------------
Observers reporting fog even with rain 0.04mm  11/10/2019 and RH 80% 13/06/2014 00:30

Introduce check pptn < 0.2 and TH > 98 
&&     ($pptn10min < 0.2) && ($RH > 98))  reduces to 180 days 




However still picking many only MIFG days 

} elsif ( ( ($vis_aws <= 2.5) && ($RH >  98.0) && ($pptn10min < 0.2) )  180 fog days



----------------------------------------------------------------------------------
Lowered VIS <= 2 and RH > 99 - quite strict !!!!   get 150 fog days 

} elsif ( ( ($vis_aws <= 2.0) && ($RH >  99.0) && ($pptn10min < 0.2) )  150 fog days - that looks better

[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YBBN1.txt YBBN_bb2.csv > YBBN_cc2.txt
[bou@bous-fed31 fog_climatology_2019]$ head -n 1  YBBN_bb2.csv > YBBN_bb2_fog_days.csv 
[bou@bous-fed31 fog_climatology_2019]$ awk -F , '$8 ~ /YES/ {print $0}' YBBN_bb2.csv >> YBBN_bb2_fog_days.csv
------------------------------------------------------------------------------------------


Introduced T-TD check like ROberts fog checker


    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || ($PW1 =~ /FG/) || ($PW2 =~ /FG/) )
    &&   ( ($PW !~ /\b12\b/) && ($PW1 !~ /MI/) && ($PW2 !~ /MI/) )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( ($RH > 98) || (($T-$Td) < 1)) )   

 1003  ./aws_format_v6.5_2020.pl YBAF1.txt YBAF2.csv > YBAF3.txt
 1004  ./aws_format_v6.5_2020.pl YBBN1.txt YBBN2.csv > YBBN3.txt
 1005  head -n 1  YBBN2.csv > YBBN2_fog_days.csv 
 1006  awk -F , '$8 ~ /YES/ {print $0}' YBBN2.csv >> YBBN2_fog_days.csv


281 fog days - thats too mnay  - this is step back!!! 


-----------------------------------------------------------

    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || ($PW1 =~ /FG/) || ($PW2 =~ /FG/) )
    &&   ( ($PW !~ /\b12\b/) && ($PW1 !~ /MI/) && ($PW2 !~ /MI/) )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( ($RH > 98) && (($T-$Td) < 1)) )   T-TD check AND with RH>98

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59",
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040004.txt > YAMB1.txt

[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl YAMB1.txt YAMB_bb2.csv > YAMB_cc2.txt
[bou@bous-fed31 fog_climatology_2019]$ head -n 1  YAMB_bb2.csv > YAMB_bb2_fog_days.csv
[bou@bous-fed31 fog_climatology_2019]$ awk -F , '$8 ~ /YES/ {print $0}' YAMB_bb2.csv >> YAMB_bb2_fog_days.csv


140 days  - lowest number of fog days with this pattern matchuing
---------------------------------------------------------------
  &&     ($pptn10min < 0.2) && ( ($T-$Td) < 1) )   gives 281 fog days

-----------------------------------------------------------------
 &&     ($pptn10min < 0.2) && ($RH > 98) )     144 days but miss 2016-05-02!!



---------------------------------------------------------------

144 days miss  2016-05-02!!

? o or more times 

    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || ($PW1 =~ /(PR|BC|VC)?FG/i) || ($PW2 =~ /(PR|BC|VC)?FG/i) )
    &&   ( ($PW !~ /\b12\b/) && ($PW1 !~ /MI/i) && ($PW2 !~ /MI/i) )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( ($RH > 98) && (($T-$Td) < 1)) )


---------------------------------------------------------------
144 days miss  2016-05-02!!  what da hell


    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || ($PW1 =~ /(PR|BC|VC)?FG/i) || ($PW2 =~ /(PR|BC|VC)?FG/i) )
    &&   ( ($PW !~ /\b12\b/) && ($PW1 !~ /MI/i) && ($PW2 !~ /MI/i) )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( $RH > 98 ) )  #&& (($T-$Td) < 1)



---------------------------------------------------------------
137  - this appears most stringest - lowets fog day count

allow for zero or more spaces '\s*' in PW groups  like MIFG, MI  FG, PR FG, BC FG, VC FG etc

.
    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || (trim($PW1) =~ /(PR|BC|VC)\s*FG/i) || (trim($PW2) =~ /(PR|BC|VC)\s*FG/i) )
    &&   ( ($PW !~ /\b12\b/) && (trim($PW1) !~ /MI\s*FG/i) && (trim($PW2) !~ /MI\s*FG/i) )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( $RH > 98 ) )

---------------------------------------------------------------
178  - this appears worse but is capturing some real fog 
Only change  $RH > 98  to $RH >= 98   equal to 98% RH captuers so many more days 
appears these includels days where we have PRFG, BCFG even VCFG - thats okay as we are casting wider net
for fog-like conditions (Roberts approach is very restrictive hence leaves out number of possible fog days)   

    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || (trim($PW1) =~ /(PR|BC|VC)\s*FG/i) || (trim($PW2) =~ /(PR|BC|VC)\s*FG/i) )
    &&   ( ($PW !~ /\b12\b/) && (trim($PW1) !~ /MI\s*FG/i) && (trim($PW2) !~ /MI\s*FG/i) )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( $RH >= 98 ) )


./aws_format_v6.5_2020.pl YBBN1.txt YBBN2.csv > YBBN3.txt; head -n 1  YBBN2.csv > YBBN2_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBBN2.csv >> YBBN2_fog_days.csv


194  - this appears worse but is capturing some real fog  
Only change  drop PW2 check && (trim($PW2) !~ /MI\s*FG/i)  chck for shallow fog

    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/) || (trim($PW1) =~ /(PR|BC|VC)\s*FG/i) || (trim($PW2) =~ /(PR|BC|VC)\s*FG/i) )
    &&   ( ($PW !~ /\b12\b/) && (trim($PW1) !~ /MI\s*FG/i)  )  # MIFG  drop working well
    &&     ($pptn10min < 0.2) && ( $RH >= 98 ) )


However we miss a day with RH 96 and PRFG that robs code picks up 2012-08-22

242 days now !!!

  if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/)
    ||     (trim($PW1) =~ /(PR|BC|VC)\s*FG/i) || (trim($PW2) =~ /(PR|BC|VC)\s*FG/i) )
    &&   ( ($PW !~ /\b12\b/) && (trim($PW1) !~ /MI\s*FG/i) && (trim($PW2) !~ /MI\s*FG/i) )  # MIFG  drop working well
    &&   ($pptn10min < 0.2)
    &&   ($RH >= 96) )  #2012-08-22 rh 96 with PRFG - increases no fog days by 64!!


265 days now !!! if also drop PW2 check && (trim($PW2) !~ /MI\s*FG/i)  chck for shallow fog
    if ( ( ($PW =~ /\b4[0-9]\b/) || ($PW =~ /\b1[0-1]\b/)
    ||     (trim($PW1) =~ /(PR|BC|VC)\s*FG/i) || (trim($PW2) =~ /(PR|BC|VC)\s*FG/i) )
    &&   ( ($PW !~ /\b12\b/) && (trim($PW1) !~ /MI\s*FG/i) )  # MIFG  drop working well
    &&   ($pptn10min < 0.2)
    &&   ($RH >= 96) )  #2012-08-22 rh 96 with PRFG - increases no fog days by 64!!


-rwxrwxrwx. 1 bou bou 588509291 Mar 29 18:54 HM01X_Data_041529.txt YTWB
-rwxrwxrwx. 1 bou bou 590709494 Mar 29 18:52 HM01X_Data_041359.txt YBOK
-rwxrwxrwx. 1 bou bou 709337882 Mar 29 18:50 HM01X_Data_040861.txt YBSU 
-rwxrwxrwx. 1 bou bou 677087489 Mar 29 18:45 HM01X_Data_040842.txt YBBN
-rwxrwxrwx. 1 bou bou 699195710 Mar 29 18:47 HM01X_Data_040717.txt YBCG
-rwxrwxrwx. 1 bou bou 684818939 Mar 29 18:42 HM01X_Data_040211.txt YBAF
-rwxrwxrwx. 1 bou bou 601162787 Mar 29 18:40 HM01X_Data_040004.txt YAMB
-rwxrwxrwx. 1 bou bou 691017140 Mar 29 18:56 HM01X_Data_039083.txt YBRK


st,040842,40  ,BRISBANE AERO            ,12/1992,       ,-27.3917, 153.1292,GPS            ,QLD,   4.5,   9.5,94578,2000,2020,102,  0,  0,  0,  0,  0,#
st,040004,40  ,AMBERLEY AMO             ,01/1941,       ,-27.6297, 152.7111,GPS            ,QLD,  24.2,  24.9,94568,2000,2020, 91,  0,  0,  0,  0,  0,#
st,041359,41  ,OAKEY AERO               ,01/1970,       ,-27.4034, 151.7413,GPS            ,QLD, 405.7, 407.1,94552,2000,2020, 89,  0,  0,  0,  0,  0,#
st,040211,40  ,ARCHERFIELD AIRPORT      ,04/1929,       ,-27.5716, 153.0071,GPS            ,QLD,  12.5,  13.0,94575,2000,2020,104,  0,  0,  0,  0,  0,#
st,041529,41  ,TOOWOOMBA AIRPORT        ,06/1996,       ,-27.5425, 151.9134,GPS            ,QLD, 640.9, 641.5,95551,2000,2020, 89,  0,  0,  0,  0,  0,#
st,040717,40  ,COOLANGATTA              ,01/1982,       ,-28.1681, 153.5053,GPS            ,QLD,   4.0,   4.7,94592,2000,2020,106,  0,  0,  0,  0,  0,#
st,040861,40  ,SUNSHINE COAST AIRPORT   ,06/1994,       ,-26.5990, 153.0912,SURVEY         ,QLD,   3.4,   3.7,94569,2000,2020,107,  0,  0,  0,  0,  0,#
st,039083,39  ,ROCKHAMPTON AERO         ,01/1939,       ,-23.3753, 150.4775,GPS            ,QLD,  10.4,  15.1,94374,2000,2020,105,  0,  0,  0,  0,  0,#


Final manual and auto obs checking code May 2020
Manual

    # 40 distant FG, 41 FG patches, 42 to 49 FG so 40 to 49 is FG, NB 10 is BR, we treat observer BR as fog-like conditions!!
    # No shallow fogs MI or code 12 - working well but still a few get caught in the 2nd block below!
    if ( ( ($PW =~ /4[0-9]/) || (($PW =~ /10/) && (trim($aws_CL1A.$aws_CL1H) =~ /^(BKN|OVC)\s*[1-2]00$/))
    ||     (trim($PW1) =~ /(PR|BC|VC|BL|DR)\s*FG/i) || (trim($PW2) =~ /(PR|BC|VC|BL|DR)\s*FG/i) )
    &&   ( ($PW !~ /12/) && (trim($PW1) !~ /MI\s*FG/i) )  # make sure no MIST(10) or MIFG(12)
    &&   ($pptn10min < 0.1)  # just sanity check - silly observer fog report rain 0.4mm 11/10/2019 20:00
    &&   (($T-$Td) <= 1) && (($vis_obs < 6)||($vis_aws < 6)) )  #sanity check - observer saying fog 13/06/2014 00:30 vis>10km
    {

Auto

    } elsif ( ( ($wind_SPD < 6) && ($vis_aws < 6) && (($T-$Td) < 1) &&
                    ($pptn10min < 0.1) && ($pptn_last < 0.1)) # ROB uses Vis<8km bigger NET!
    &&        ( (trim($aws_CL1A.$aws_CL1H) =~ /^(BKN|OVC)\s*[1-2]00$/) ) )# fogs missed if no cld near gnd - usually MIFG so OK
    #||          ($PW_auto =~ /[2-3]0/) || ($PW_auto =~ /3[1-4]/)    # PW sensors not yet turned on for many AWS sites
    #||          ($PW_15 =~ /[2-3]0/  ) || ($PW_15 =~ /3[1-4]/  )
    #||          ($PW_60 =~ /[2-3]0/  ) || ($PW_60 =~ /3[1-4]/  )  ) )
    {


[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040842.txt > YBBN1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040004.txt > YAMB1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_041359.txt > YBOK1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040211.txt > YBAF1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_041529.txt > YTWB1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040717.txt > YBCG1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_040861.txt > YBSU1.txt

[bou@bous-fed31 fog_climatology_2019]$ awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59", \
"$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' tcz_se_qld/HM01X_Data_039083.txt > YBRK1.txt








++++++++++++++++++++++++++++
check_fog_manual_obs_first()
++++++++++++++++++++++++++++
./aws_format_v6.5_2020.pl YBBN1.txt YBBN2manual.csv > YBBN3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YAMB1.txt YAMB2manual.csv > YAMB3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBOK1.txt YBOK2manual.csv > YBOK3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBAF1.txt YBAF2manual.csv > YBAF3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YTWB1.txt YTWB2manual.csv > YTWB3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBCG1.txt YBCG2manual.csv > YBCG3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBSU1.txt YBSU2manual.csv > YBSU3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBRK1.txt YBRK2manual.csv > YBRK3manual.txt;


head -n 1  YBBN2manual.csv > YBBN2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBBN2manual.csv >> YBBN2manual_fog_days.csv;sleep 2;head -n 1  YAMB2manual.csv > YAMB2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YAMB2manual.csv >> YAMB2manual_fog_days.csv;sleep 2;head -n 1  YBOK2manual.csv > YBOK2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBOK2manual.csv >> YBOK2manual_fog_days.csv;sleep 2;head -n 1  YBAF2manual.csv > YBAF2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBAF2manual.csv >> YBAF2manual_fog_days.csv;sleep 2;head -n 1  YTWB2manual.csv > YTWB2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YTWB2manual.csv >> YTWB2manual_fog_days.csv;sleep 2;head -n 1  YBCG2manual.csv > YBCG2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBCG2manual.csv >> YBCG2manual_fog_days.csv;sleep 2;head -n 1  YBSU2manual.csv > YBSU2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBSU2manual.csv >> YBSU2manual_fog_days.csv;sleep 2;head -n 1  YBRK2manual.csv > YBRK2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBRK2manual.csv >> YBRK2manual_fog_days.csv;









++++++++++++++++++++++++++++
check_fog_auto_obs_first()
++++++++++++++++++++++++++
./aws_format_v6.5_2020.pl YBBN1.txt YBBN2auto.csv > YBBN3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YAMB1.txt YAMB2auto.csv > YAMB3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YBOK1.txt YBOK2auto.csv > YBOK3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YBAF1.txt YBAF2auto.csv > YBAF3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YTWB1.txt YTWB2auto.csv > YTWB3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YBCG1.txt YBCG2auto.csv > YBCG3auto.txt;sleep 10;./aws_format_v6.5_2020.pl YBSU1.txt YBSU2auto.csv > YBSU3auto.txt;sleep 10;./aws_format_v6.5_2020.pl YBRK1.txt YBRK2auto.csv > YBRK3auto.txt;





head -n 1  YBBN2auto.csv > YBBN2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBBN2auto.csv >> YBBN2auto_fog_days.csv;sleep 2;head -n 1  YAMB2auto.csv > YAMB2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YAMB2auto.csv >> YAMB2auto_fog_days.csv;sleep 2;head -n 1  YBOK2auto.csv > YBOK2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBOK2auto.csv >> YBOK2auto_fog_days.csv;sleep 2;head -n 1  YBAF2auto.csv > YBAF2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBAF2auto.csv >> YBAF2auto_fog_days.csv;sleep 2;head -n 1  YTWB2auto.csv > YTWB2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YTWB2auto.csv >> YTWB2auto_fog_days.csv;sleep 2;head -n 1  YBCG2auto.csv > YBCG2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBCG2auto.csv >> YBCG2auto_fog_days.csv;sleep 2;head -n 1  YBSU2auto.csv > YBSU2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBSU2auto.csv >> YBSU2auto_fog_days.csv;sleep 2;head -n 1  YBRK2auto.csv > YBRK2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBRK2auto.csv >> YBRK2auto_fog_days.csv;








check_fog_modified_2015();
++++++++++++++++++++++++++
./aws_format_v6.5_2020.pl YBBN1.txt YBBN2mod2015.csv > YBBN3mod2015.txt;sleep 10;./aws_format_v6.5_2020.pl YAMB1.txt YAMB2mod2015.csv > YAMB3mod2015.txt;sleep 10;./aws_format_v6.5_2020.pl YBOK1.txt YBOK2mod2015.csv > YBOK3mod2015.txt;sleep 10;/aws_format_v6.5_2020.pl YBAF1.txt YBAF2mod2015.csv > YBAF3mod2015.txt;sleep 10;./aws_format_v6.5_2020.pl YTWB1.txt YTWB2mod2015.csv > YTWB3mod2015.txt;sleep 10;./aws_format_v6.5_2020.pl YBCG1.txt YBCG2mod2015.csv > YBCG3mod2015.txt;sleep 10;./aws_format_v6.5_2020.pl YBSU1.txt YBSU2mod2015.csv > YBSU3mod2015.txt;sleep 10;./aws_format_v6.5_2020.pl YBRK1.txt YBRK2mod2015.csv > YBRK3mod2015.txt;


head -n 1  YBBN2mod2015.csv > YBBN2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBBN2mod2015.csv >> YBBN2mod2015_fog_days.csv;sleep 2;head -n 1  YAMB2mod2015.csv > YAMB2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YAMB2mod2015.csv >> YAMB2mod2015_fog_days.csv;sleep 2;head -n 1  YBOK2mod2015.csv > YBOK2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBOK2mod2015.csv >> YBOK2mod2015_fog_days.csv;sleep 2;head -n 1  YBAF2mod2015.csv > YBAF2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBAF2mod2015.csv >> YBAF2mod2015_fog_days.csv;sleep 2;head -n 1  YTWB2mod2015.csv > YTWB2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YTWB2mod2015.csv >> YTWB2mod2015_fog_days.csv;sleep 2;head -n 1  YBCG2mod2015.csv > YBCG2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBCG2mod2015.csv >> YBCG2mod2015_fog_days.csv;sleep 2;head -n 1  YBSU2mod2015.csv > YBSU2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBSU2mod2015.csv >> YBSU2mod2015_fog_days.csv;sleep 2;head -n 1  YBRK2mod2015.csv > YBRK2mod2015_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBRK2mod2015.csv >> YBRK2mod2015_fog_days.csv;



No difference in fog_checker functions for sites that are totally automated.


[bou@bous-fed31 fog_climatology_2019]$ diff YTWB2manual.csv YTWB2auto.csv
[bou@bous-fed31 fog_climatology_2019]$ diff YBCG2manual.csv YBCG2auto.csv
[bou@bous-fed31 fog_climatology_2019]$ diff YBSU2manual.csv YBSU2auto.csv
[bou@bous-fed31 fog_climatology_2019]$ diff YBAF2manual.csv YBAF2auto.csv

For others that have some manual obs 
number of fog days detected is almost same 

[bou@bous-fed31 fog_climatology_2019]$ wc YBBN2auto.csv; wc YBBN2manual.csv
   7308  289677 2319833 YBBN2auto.csv
   7308  289677 2319840 YBBN2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YBAF2auto.csv; wc YBAF2manual.csv
   7348  295048 2336388 YBAF2auto.csv
   7348  295048 2336388 YBAF2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YBCG2auto.csv; wc YBCG2manual.csv
   7337  285303 2326006 YBCG2auto.csv
   7337  285303 2326006 YBCG2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YBSU2auto.csv; wc YBSU2manual.csv
   7320  284617 2319052 YBSU2auto.csv
   7320  284617 2319052 YBSU2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YBRK2auto.csv; wc YBRK2manual.csv
   7344  294551 2338760 YBRK2auto.csv
   7344  294551 2338745 YBRK2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YBOK2auto.csv; wc YBOK2manual.csv
   5175  213131 1652737 YBOK2auto.csv
   5175  213131 1652729 YBOK2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YTWB2auto.csv; wc YTWB2manual.csv
   4678  191844 1505428 YTWB2auto.csv
   4678  191844 1505428 YTWB2manual.csv

[bou@bous-fed31 fog_climatology_2019]$ wc YAMB2auto.csv; wc YAMB2manual.csv
   5175  212287 1656273 YAMB2auto.csv
   5175  212287 1656246 YAMB2manual.csv




The only differences are due to columns that count number of manual obs derives fog obs
and number of auto obs derived fog hours

see the 3 cols far left - all else same!!!

[bou@bous-fed31 fog_climatology_2019]$ diff YAMB2manual.csv YAMB2auto.csv |tail
< YAMB  , 2, 1, 2019, 10, 17,    6.8, YES,   0.20, 19.12, 21.00, 01.48, 05:00, 90, 15.0, 31.3, 19.0,12.30, 48,1006.3,08:00, 70, 12.1, 22.2, 19.6,2.60, 85,1009.9,11:00,220,  4.1, 20.6, 18.5,2.10, 88,1013.3,14:00,330,  5.1, 19.6, 18.6,1.00, 94,1013.9,17:00,330,  4.1, 17.8, 17.2,0.60, 96,1013.0,20:00,320,  6.0, 16.7, 16.2,0.50, 97,1015.0
---
> YAMB  , 0, 3, 2019, 10, 17,    6.8, YES,   0.20, 19.12, 21.00, 01.48, 05:00, 90, 15.0, 31.3, 19.0,12.30, 48,1006.3,08:00, 70, 12.1, 22.2, 19.6,2.60, 85,1009.9,11:00,220,  4.1, 20.6, 18.5,2.10, 88,1013.3,14:00,330,  5.1, 19.6, 18.6,1.00, 94,1013.9,17:00,330,  4.1, 17.8, 17.2,0.60, 96,1013.0,20:00,320,  6.0, 16.7, 16.2,0.50, 97,1015.0
5145c5145
< YAMB  , 4, 2, 2020, 02, 27,    0.4, YES,   0.45, 18.30, 21.30, 03.00, 05:00, 90, 18.1, 29.0, 21.7,7.30, 65,1005.9,08:00, 60,  8.0, 26.3, 21.5,4.80, 75,1006.4,11:00, 50,  1.9, 23.1, 20.4,2.70, 85,1008.3,14:00, 50,  4.1, 21.8, 19.7,2.10, 88,1008.0,17:00, 50,  4.1, 21.0, 19.7,1.30, 92,1007.5,20:00, 50,  2.9, 20.5, 19.8,0.70, 96,1007.9
---
> YAMB  , 1, 5, 2020, 02, 27,    0.4, YES,   0.45, 18.30, 21.30, 03.00, 05:00, 90, 18.1, 29.0, 21.7,7.30, 65,1005.9,08:00, 60,  8.0, 26.3, 21.5,4.80, 75,1006.4,11:00, 50,  1.9, 23.1, 20.4,2.70, 85,1008.3,14:00, 50,  4.1, 21.8, 19.7,2.10, 88,1008.0,17:00, 50,  4.1, 21.0, 19.7,1.30, 92,1007.5,20:00, 50,  2.9, 20.5, 19.8,0.70, 96,1007.9





---------------------------------------------------------------

Robert has event date 23 AUg 2012 - its actually 22 Aug 2012
ALso 3rd JUne 2014      - this would be 2nd JUne 2014

Station.Number	Station	Event.Date	Month	Bimonth	FG.type			Low.DPD.onset		FG.onset			FG.clearance	Speci.clearance	Low.DPD.onset.hour	FG.onset.hour	FG.clearance.hour	Speci.clearance.hour	FG.duration	Thick.FG.duration	Precip_9am.to.onset	Precip.24h.to.9am	Precip.12h.to.9am	Dry.day	FG.detected	Time.of.first.tip	Hour.of.first.tip	Tip.possible.from.dew
40842	YBBN	2012-08-23	Aug	JulAug	No rain fog	2012-08-22 12:00:00	2012-08-22 17:00:00	2012-08-22 17:25:00	2012-08-22 19:23:00	12	17	17.42	19.38	0.42	0	0	0	0	TRUE	TRUE	NA	NA	NA

[bou@bous-fed31 fog_climatology_2019]$ grep -i '22/08/2012' tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '|grep 'FG'

YBBN  ,22/08/2012 13:44,s, 2,  0,  0.0,  0.0, 10.00,   , 10.00,11,MI    FG,        ,        ,1, 1800, ,     , ,     ,   ,     ,   ,     ,   ,     , 16.0, 15.3, 96,1018.7,   0.0,   0.0
YBBN  ,22/08/2012 14:42,s, 2,  0,  0.0,  0.0, 10.00,   ,  6.00,11,MI    FG,        ,        ,1, 7000, ,     , ,     ,   ,     ,   ,     ,   ,     , 14.6, 13.9, 96,1018.5,   0.0,   0.0
YBBN  ,22/08/2012 15:00,s, 2,  0,  0.0,  0.0, 10.00,   ,  0.35,11,MI    FG,        ,        ,1, 7000, ,     , ,     ,   ,     ,   ,     ,   ,     , 14.6, 13.9, 96,1018.3,   0.0,   0.0
YBBN  ,22/08/2012 15:10,s, 2,  0,  0.0,  0.0,  5.00,   ,  0.90,10,      BR,MI    FG,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 14.5, 14.0, 97,1018.3,   0.0,   0.0
YBBN  ,22/08/2012 15:24,s, 2,  0,  0.0,  0.0,  2.00,   ,  2.40,10,      BR,MI    FG,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 14.2, 13.3, 94,1018.2,   0.0,   0.0
YBBN  ,22/08/2012 15:30,s, 2,  0,  0.0,  0.0,  1.50,360,  6.00,10,      BR,MI    FG,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 14.6, 13.9, 96,1018.1,   0.0,   0.0
YBBN  ,22/08/2012 15:44,s, 2,  0,  0.0,  0.0,  0.80,   ,  1.20,42,      FG,        ,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 14.7, 14.3, 97,1018.1,   0.0,   0.0
YBBN  ,22/08/2012 16:00,s, 2,  0,  0.0,  0.0,  4.00,   ,  1.00,10,      BR,MI    FG,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 14.9, 14.5, 97,1018.1,   0.0,   0.0
YBBN  ,22/08/2012 16:21,s, 2,  0,  0.0,  0.0,  4.00,   ,  1.30,42,PR    FG,      BR,MI    FG, ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 13.9, 13.4, 97,1018.3,   0.0,   0.0
YBBN  ,22/08/2012 16:30,s, 2,  0,  0.0,  0.0,  3.00,   ,  0.70,42,PR    FG,      BR,MI    FG,1,  100, ,     , ,     ,   ,     ,   ,     ,   ,     , 13.3, 12.6, 96,1018.3,   0.0,   0.0
YBBN  ,22/08/2012 16:44,s, 2,260,  1.9,  9.4,  0.50,180,  7.00,42,PR    FG,        ,        ,2,  100, ,     , ,     ,   ,     ,   ,     ,   ,     , 12.8, 12.1, 96,1018.4,   0.0,   0.0
YBBN  ,22/08/2012 17:00,s, 2,230,  2.9,  7.6,  1.00, 45,  5.00,10,      BR,MI    FG,        ,1,  100, ,     , ,     ,SCT,  100,   ,     ,   ,     , 13.3, 12.7, 96,1018.4,   0.0,   0.0
YBBN  ,22/08/2012 17:30,s, 2,200,  1.0,  5.4,  0.80, 45,  0.60,42,PR    FG,      BR,MI    FG, ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 13.4, 12.8, 96,1018.4,   0.0,   0.0
YBBN  ,22/08/2012 18:30,s, 2,210,  5.1, 11.2,  9.00,   ,  5.00,11,MI    FG,        ,        ,1, 1800, ,     , ,     ,   ,     ,   ,     ,   ,     , 13.4, 12.8, 96,1018.5,   0.0,   0.0
YBBN  ,22/08/2012 19:00,s, 2,210,  5.1, 11.2,  9.00,   ,  7.00,11,MI    FG,        ,        ,1, 1200,7, 2300, ,     ,SCT, 2300,   ,     ,   ,     , 13.3, 12.6, 96,1018.7,   0.0,   0.0


Another day I missed 2014-06-03
Station.Number	Station	Event.Date	Month	Bimonth	FG.type			Low.DPD.onset		FG.onset			FG.clearance	Speci.clearance	Low.DPD.onset.hour	FG.onset.hour	FG.clearance.hour	Speci.clearance.hour	FG.duration	Thick.FG.duration	Precip_9am.to.onset	Precip.24h.to.9am	Precip.12h.to.9am	Dry.day	FG.detected	Time.of.first.tip	Hour.of.first.tip	Tip.possible.from.dew
40842	YBBN	2014-06-03	Jun	MayJun	No rain fog	2014-06-02 17:00:00	2014-06-02 20:08:00	2014-06-02 22:30:00	2014-06-02 23:30:00	17	20.13	22.5	23.5	2.37	0	0	0	0	TRUE	TRUE	2014-06-03 03:30:00	27.5	FALSE

[bou@bous-fed31 fog_climatology_2019]$ grep -i '02/06/2014' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'

YBBN  ,02/06/2014 17:30,s, 2,240,  1.9,  9.4, 10.00,   , 10.00,11,MI    FG,        ,        ,1, 4500,5,30000, ,     ,   ,     ,   ,     ,   ,     , 16.5, 15.6, 94,1017.4,   0.0,   0.0
YBBN  ,02/06/2014 18:00,s, 2,270,  2.9,  9.4, 10.00,   , 10.00,11,MI    FG,        ,        ,1, 4500,5,30000, ,     ,   ,     ,   ,     ,   ,     , 16.5, 15.8, 96,1017.4,   0.0,   0.0
YBBN  ,02/06/2014 18:30,s, 2,260,  4.1,  9.4, 10.00,   , 10.00,11,MI    FG,        ,        ,1, 4500,3,24000, ,     ,   ,     ,   ,     ,   ,     , 16.3, 15.8, 97,1017.7,   0.0,   0.0
YBBN  ,02/06/2014 19:00,s, 2,180,  2.9,  9.4, 10.00,   , 10.00,11,MI    FG,        ,        ,1, 4500, ,     , ,     ,   ,     ,   ,     ,   ,     , 16.1, 15.8, 98,1017.9,   0.0,   0.0
YBBN  ,02/06/2014 19:30,s, 2,170,  1.0,  9.4, 10.00,   , 10.00,11,MI    FG,        ,        ,1, 4500, ,     , ,     ,   ,     ,   ,     ,   ,     , 16.0, 15.5, 97,1018.2,   0.0,   0.0
YBBN  ,02/06/2014 20:00,s, 2,190,  5.1, 11.2,  0.90,180,  8.00,42,      FG,        ,        ,3, 4500, ,     , ,     ,SCT,  100,SCT, 4300,   ,     , 16.6, 16.3, 98,1018.3,   0.0,   0.0
YBBN  ,02/06/2014 20:08,s, 2,190,  5.1, 11.2,  0.50,   ,  2.00,42,      FG,        ,        ,5, 4500, ,     , ,     ,SCT,  100,BKN, 4300,   ,     , 16.6, 16.3, 98,1018.4,   0.0,   0.0
YBBN  ,02/06/2014 20:30,s, 2,200,  6.0, 16.6,  0.70,   ,  0.70,42,      FG,        ,        , ,     , ,     , ,     ,OVC,  100,   ,     ,   ,     , 16.5, 16.2, 98,1018.5,   0.0,   0.0
YBBN  ,02/06/2014 20:38,s, 2,200,  6.0, 14.8,  1.00,   ,  1.00,42,      FG,        ,        , ,     , ,     , ,     ,OVC,  100,   ,     ,   ,     , 16.7, 16.4, 98,1018.7,   0.0,   0.0
YBBN  ,02/06/2014 20:44,s, 2,200,  7.0, 16.6,  2.00,   ,  1.80,42,      FG,        ,        , ,     , ,     , ,     ,OVC,  100,   ,     ,   ,     , 16.8, 16.5, 98,1018.8,   0.0,   0.0
YBBN  ,02/06/2014 21:14,s, 2,190,  7.0, 16.6,  1.00,   ,  1.50,42,      FG,        ,        , ,     , ,     , ,     ,OVC,  100,   ,     ,   ,     , 16.9, 16.6, 98,1018.8,   0.0,   0.0
YBBN  ,02/06/2014 21:24,s, 2,190,  5.1, 13.0,  2.50,   ,  1.70,10,      BR,BC    FG,        , ,     , ,     , ,     ,OVC,  100,   ,     ,   ,     , 16.9, 16.7, 99,1018.9,   0.0,   0.0
YBBN  ,02/06/2014 21:30,s, 2,190,  4.1,  9.4,  2.50,   ,  3.20,10,      BR,BC    FG,        ,7,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     , 17.0, 16.8, 99,1018.9,   0.0,   0.0
YBBN  ,02/06/2014 21:46,s, 2,190,  5.1, 11.2,  3.50,   ,  3.50,10,      BR,BC    FG,        ,5,  100,3, 4500, ,     ,OVC,  100,   ,     ,   ,     , 17.2, 16.9, 98,1019.0,   0.0,   0.0


[bou@bous-fed31 fog_climatology_2019]$ grep -i '03/06/2014' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '|grep 'FG'
YBBN  ,03/06/2014 12:30,s, 2,  0,  0.0,  5.4, 10.00,   ,  7.00,11,MI    FG,        ,        ,2,11000, ,     , ,     ,SCT,11300,   ,     ,   ,     , 17.1, 16.1, 94,1019.9,   0.0,   2.0
YBBN  ,03/06/2014 13:00,s, 2,290,  2.9,  9.4, 10.00,   ,  5.00,11,MI    FG,        ,        ,1,11000, ,     , ,     ,   ,     ,   ,     ,   ,     , 16.9, 16.1, 95,1019.9,   0.0,   2.0
YBBN  ,03/06/2014 13:30,s, 2,250,  2.9, 11.2, 10.00,   , 10.00,11,MI    FG,        ,        ,1,11000, ,     , ,     ,   ,     ,   ,     ,   ,     , 17.1, 15.9, 93,1020.0,   0.0,   2.0
YBBN  ,03/06/2014 14:00,s, 2,  0,  0.0,  0.0, 10.00,   ,  8.00,11,MI    FG,        ,        ,2,11000, ,     , ,     ,   ,     ,   ,     ,   ,     , 16.3, 15.6, 96,1019.9,   0.0,   2.0



Check out also where Roberts fog date and mine are out by 2 days ( by 1 is fine since Rob uses day+1)
Lets see 3 cases 

Roberts date	my dates
-------------------------
2008-06-13   2008-06-12 we both correct 
2012-07-13   2012-07-11 Rob definitely got this date wrong!
2012-08-23   I missed !! 2012-08-22 but new script got it
2016-05-03   2016-05-02 we both correct (there was ONLY one fog speci that day!!!)


-------------------------
2008-06-13   2008-06-12
Roberts date 12th JUne 2008 is correct - mines on 12th - thats okay

[bou@bous-fed31 fog_climatology_2019]$ grep -i '12/06/2008' tcz_se_qld/HM01X_Data_040842.txt |
 awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' >> 2008_June.txt


[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl 2008_June.txt 2008_Junebb.csv > 2008_Junecc.txt 

my stats
YBBN  	17	1	2008-6-12	0	 YES	0.05	14.3	23.45	9.15


Station.Number	Station	Event.Date	Month	Bimonth	FG.type			Low.DPD.onset		FG.onset			FG.clearance	Speci.clearance	Low.DPD.onset.hour	FG.onset.hour	FG.clearance.hour	Speci.clearance.hour	FG.duration	Thick.FG.duration	Precip_9am.to.onset	Precip.24h.to.9am	Precip.12h.to.9am	Dry.day	FG.detected	Time.of.first.tip	Hour.of.first.tip	Tip.possible.from.dew
40842	YBBN	2008-06-13	Jun	MayJun	No rain fog	2008-06-12 13:00:00	2008-06-12 21:00:00	2008-06-12 23:30:00	2008-06-12 23:30:00	13	21	23.5	23.5	2.5	1.5	0	0	0	TRUE	TRUE	2008-06-13 02:00:00	26	FALSE


[bou@bous-fed31 fog_climatology_2019]$ grep -i '11/06/2008' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
No fog traces on 11th JUne 2008

[bou@bous-fed31 fog_climatology_2019]$ grep -i '12/06/2008' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
YBBN  ,12/06/2008 15:00,s, 2,  0,  0.0,  5.4, 10.00,   ,  2.90,11,MI    FG,        ,        ,1, 5000, ,     , ,     ,   ,     ,   ,     ,   ,     , 14.2, 13.5, 96,1020.0,   0.0,   0.0
YBBN  ,12/06/2008 15:30,m, 2,  0,  0.0,  0.0, 10.00,   ,  0.90,11,MI    FG,        ,        ,1, 5000, ,     , ,     ,   ,     ,   ,     ,   ,     , 13.3, 13.1, 99,1019.8,   0.0,   0.0
YBBN  ,12/06/2008 16:00,s, 2,260,  2.9,  7.6, 10.00,   ,  0.60,11,MI    FG,        ,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 13.5, 13.0, 97,1019.5,   0.0,   0.0
YBBN  ,12/06/2008 17:30,m, 2,  0,  0.0,  0.0, 10.00,   ,  4.70,11,MI    FG,        ,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 13.9, 13.6, 98,1019.0,   0.0,   0.0
YBBN  ,12/06/2008 18:00,s, 2,280,  1.0,  9.4, 10.00,   ,  0.45,11,MI    FG,        ,        , ,     , ,     , ,     ,   ,     ,   ,     ,   ,     , 12.8, 12.7, 99,1018.9,   0.0,   0.0
YBBN  ,12/06/2008 19:52,s, 2,220,  4.1, 11.2,  5.00,   ,  1.80,11,MI    FG,      BR,      FG,1,15000, ,     , ,     ,   ,     ,   ,     ,   ,     , 12.1, 11.7, 97,1019.0,   0.0,   0.0
YBBN  ,12/06/2008 20:00,s, 2,230,  6.0, 13.0,  5.00,   ,  3.40,11,MI    FG,      BR,      FG,1,15000, ,     , ,     ,   ,     ,   ,     ,   ,     , 11.9, 11.5, 97,1019.2,   0.0,   0.0
YBBN  ,12/06/2008 20:26,s, 2,210,  4.1, 11.2,  3.00,   ,  3.10,42,      FG,      BR,        ,1,15000, ,     , ,     ,   ,     ,   ,     ,   ,     , 12.2, 12.0, 99,1019.4,   0.0,   0.0
YBBN  ,12/06/2008 20:30,s, 2,210,  5.1, 13.0,  0.80,   ,  3.60,42,      FG,        ,        ,1,15000, ,     , ,     ,   ,     ,   ,     ,   ,     , 12.2, 12.0, 99,1019.4,   0.0,   0.0
YBBN  ,12/06/2008 21:00,s, 2,210,  5.1, 14.8,  0.05,   ,  0.25,42,      FG,        ,        ,7,  100, ,     , ,     ,BKN,  100,   ,     ,   ,     , 12.8, 12.6, 99,1019.5,   0.0,   0.0
YBBN  ,12/06/2008 21:30,s, 2,210,  4.1, 11.2,  0.05,   ,  0.20,42,      FG,        ,        ,7,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     , 13.0, 12.6, 97,1019.4,   0.0,   0.0
YBBN  ,12/06/2008 22:00,s, 2,220,  5.1, 13.0,  0.10,   ,  0.30,42,      FG,        ,        ,7,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     , 13.7, 13.3, 97,1019.7,   0.0,   0.0
YBBN  ,12/06/2008 22:30,s, 2,220,  7.0, 14.8,  0.30,   ,  0.70,42,      FG,        ,        ,7,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     , 14.6, 14.3, 98,1019.8,   0.0,   0.0
YBBN  ,12/06/2008 22:40,s, 2,210,  5.1, 14.8,  1.50,   ,  0.80,42,      FG,        ,        ,5,  100,3,25000, ,     ,OVC,  100,   ,     ,   ,     , 14.7, 14.5, 99,1019.6,   0.0,   0.0
YBBN  ,12/06/2008 22:48,s, 2,230,  2.9,  9.4,  3.00,   ,  1.90,41,BC    FG,        ,        ,3,  100,3,25000, ,     ,BKN,  100,   ,     ,   ,     , 15.3, 15.0, 98,1019.8,   0.0,   0.0
YBBN  ,12/06/2008 23:00,s, 2,230,  5.1, 11.2,  4.00,   ,  6.00,41,BC    FG,        ,        ,2,  100, ,     , ,     ,SCT,  100,   ,     ,   ,     , 15.8, 15.5, 98,1019.9,   0.0,   0.0
YBBN  ,12/06/2008 23:15,s, 2,250,  5.1, 13.0,  7.00,   ,  6.00,41,BC    FG,        ,        ,1,  100, ,     , ,     ,SCT,  100,   ,     ,   ,     , 16.8, 16.1, 96,1019.8,   0.0,   0.0


Roberts date	my dates
-------------------------
2012-07-13   2012-07-11

Roberts wrong here  FOG was on 11th - not on 12th July
Station.Number	Station	Event.Date	Month	Bimonth	FG.type			Low.DPD.onset		FG.onset			FG.clearance	Speci.clearance	Low.DPD.onset.hour	FG.onset.hour	FG.clearance.hour	Speci.clearance.hour	FG.duration	Thick.FG.duration	Precip_9am.to.onset	Precip.24h.to.9am	Precip.12h.to.9am	Dry.day	FG.detected	Time.of.first.tip	Hour.of.first.tip	Tip.possible.from.dew
40842	YBBN	2012-07-13	Jul	JulAug	Post rain fog	2012-07-13 03:06:00	2012-07-13 04:13:00	2012-07-13 05:00:00	2012-07-13 05:00:00	27.1	28.22	29	29	0.78	0	18.6	18.6	5.8	FALSE	TRUE	2012-07-12 05:00:00	5	FALSE




[bou@bous-fed31 fog_climatology_2019]$ grep -i '11/07/2012' tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' >> 2012_July.txt
[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl 2012_July.txt 2012_Julybb.csv > 2012_Julycc.txt

my stats
YBBN  	12	7	2012-7-11	0	 YES	0.5	11.51	23	11.09


[bou@bous-fed31 fog_climatology_2019]$ grep -i '11/07/2012' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
YBBN  ,11/07/2012 12:21,s, 2,  0,  0.0,  0.0,  5.00,   ,  2.60,41,BC    FG,        ,        ,2, 4500,5,19000, ,     ,SCT, 4900,   ,     ,   ,     , 16.7, 16.2, 97,1022.4,   0.0,   1.6
YBBN  ,11/07/2012 12:30,s, 2,  0,  0.0,  0.0,  9.00,   ,  0.50,11,MI    FG,        ,        ,1, 4500,5,19000, ,     ,   ,     ,   ,     ,   ,     , 16.4, 15.9, 97,1022.3,   0.0,   1.6
YBBN  ,11/07/2012 13:00,s, 2,  0,  0.0,  0.0,  6.00,   ,  0.50,41,BC    FG,MI    FG,        ,1,  500,3, 5000,5,19000,SCT, 4700,BKN, 5300,   ,     , 16.4, 16.1, 98,1022.1,   0.0,   1.6
YBBN  ,11/07/2012 13:30,s, 2,  0,  0.0,  0.0, 10.00,   ,  5.00,11,MI    FG,        ,        ,1, 2000,6, 5000, ,     ,BKN, 4900,OVC, 5300,   ,     , 16.4, 16.1, 98,1022.1,   0.0,   1.6
YBBN  ,11/07/2012 14:00,s, 2,  0,  0.0,  0.0, 10.00,   ,  4.20,11,MI    FG,        ,        ,1, 1000,5, 6000, ,     ,SCT, 3700,BKN, 6000,OVC, 7000, 16.2, 15.9, 98,1021.9,   0.0,   1.6
YBBN  ,11/07/2012 14:30,s, 2,  0,  0.0,  0.0, 10.00,   ,  3.10,41,BC    FG,MI    FG,        ,1, 1000,3, 4000,5,17500,SCT, 4300,SCT, 6200,   ,     , 16.3, 16.1, 99,1021.6,   0.0,   1.6
YBBN  ,11/07/2012 15:00,s, 2,  0,  0.0,  0.0, 10.00,   ,  2.40,41,BC    FG,MI    FG,        ,1,  800,5,18000, ,     ,   ,     ,   ,     ,   ,     , 16.3, 16.1, 99,1021.5,   0.0,   1.6
YBBN  ,11/07/2012 15:30,s, 2,  0,  0.0,  0.0,  5.00,   ,  0.80,10,      BR,MI    FG,        ,1,  800,6,18000, ,     ,   ,     ,   ,     ,   ,     , 16.0, 16.0,100,1021.1,   0.0,   1.6
YBBN  ,11/07/2012 16:00,s, 2,  0,  0.0,  0.0,  5.00,   ,  0.60,10,      BR,MI    FG,        ,1,  800,7,18000, ,     ,   ,     ,   ,     ,   ,     , 15.8, 15.6, 99,1020.8,   0.0,   1.6
YBBN  ,11/07/2012 17:30,s, 2,180,  2.9,  9.4,  3.00,   ,  0.90,10,      BR,MI    FG,        ,1,  500,3,12500, ,     ,   ,     ,   ,     ,   ,     , 15.3, 15.1, 99,1020.3,   0.0,   1.6
YBBN  ,11/07/2012 18:00,s, 2,220,  1.0,  5.4,  4.00,   ,  2.60,10,      BR,MI    FG,        ,1,  500,4,12500, ,     ,SCT, 2100,SCT,12000,   ,     , 15.5, 15.2, 98,1020.1,   0.0,   1.6
YBBN  ,11/07/2012 19:30,s, 2,  0,  0.0,  0.0,  5.00,   ,  0.60,10,      BR,MI    FG,        ,1,  800,4,12500, ,     ,SCT, 5100,SCT,12000,   ,     , 15.4, 15.2, 99,1019.9,   0.0,   1.6
YBBN  ,11/07/2012 20:00,s, 2,  0,  0.0,  0.0,  5.00,   ,  1.60,10,      BR,MI    FG,        ,1,  800,3, 5500, ,     ,SCT, 1900,SCT, 2600,BKN, 5100, 15.4, 15.2, 99,1020.2,   0.0,   1.6
YBBN  ,11/07/2012 20:30,s, 2,250,  1.0,  7.6,  6.00,   ,  6.00,11,MI    FG,        ,        ,1, 1000,5,14000, ,     ,SCT, 1900,   ,     ,   ,     , 15.9, 15.7, 99,1020.3,   0.0,   1.6
YBBN  ,11/07/2012 21:00,s, 2,  0,  0.0,  0.0,  6.00,   ,  6.00,11,MI    FG,        ,        ,1, 1000,5,12000, ,     ,BKN,12000,   ,     ,   ,     , 15.7, 15.5, 99,1020.4,   0.0,   1.6
YBBN  ,11/07/2012 21:30,s, 2,210,  4.1, 11.2,  6.00,   ,  6.00,  ,      FG,        ,        ,1, 1000,5,13000, ,     ,SCT,12000,   ,     ,   ,     , 16.4, 16.2, 99,1020.8,   0.0,   1.6
YBBN  ,11/07/2012 22:00,s, 2,200,  5.1, 11.2,  6.00,   ,  6.00,  ,      FG,        ,        ,1, 1000,7, 5400, ,     ,SCT, 1900,SCT, 4900,BKN, 5400, 17.3, 17.0, 98,1021.1,   0.0,   1.6
YBBN  ,11/07/2012 22:30,s, 2,190,  4.1,  9.4,  7.00,   ,  7.00,  ,      FG,        ,        ,1, 1200,7, 5200, ,     ,SCT, 2400,OVC, 5200,   ,     , 18.0, 17.7, 98,1021.3,   0.0,   1.6

[bou@bous-fed31 fog_climatology_2019]$ grep -i '12/07/2012' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
[bou@bous-fed31 fog_climatology_2019]$ grep -i '13/07/2012' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'

no trace on 12th or the 13th 


Roberts date	my dates
-------------------------
2016-05-03   2016-05-02


one and only one fog obs on 2nd May 2016 
Station.Number	Station	Event.Date	Month	Bimonth	FG.type			Low.DPD.onset		FG.onset			FG.clearance	Speci.clearance	Low.DPD.onset.hour	FG.onset.hour	FG.clearance.hour	Speci.clearance.hour	FG.duration	Thick.FG.duration	Precip_9am.to.onset	Precip.24h.to.9am	Precip.12h.to.9am	Dry.day	FG.detected	Time.of.first.tip	Hour.of.first.tip	Tip.possible.from.dew
40842	YBBN	2016-05-03			May		MayJun	Post rain fog	2016-05-02 14:30:00	2016-05-02 21:42:00	2016-05-02 22:30:00	2016-05-02 23:30:00	14.5	21.7	22.5	23.5	0.8	0	19.2	19.2	19.2	FALSE	TRUE	2016-05-02 14:00:00	14	FALSE


[bou@bous-fed31 fog_climatology_2019]$ grep -i '01/05/2016' tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' > 2016_May.txt
[bou@bous-fed31 fog_climatology_2019]$ grep -i '02/05/2016' tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' >> 2016_May.txt
[bou@bous-fed31 fog_climatology_2019]$ grep -i '03/05/2016' tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' >> 2016_May.txt
[bou@bous-fed31 fog_climatology_2019]$ grep -i '04/05/2016' tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' >> 2016_May.txt

[bou@bous-fed31 fog_climatology_2019]$ ./aws_format_v6.5_2020.pl 2016_May.txt 2016_Maybb.csv > 2016_Maycc.txt 

my stats
YBBN  	0	0	2016-5-1	0	 NO	10000	 	 	 
YBBN  	4	0	2016-5-1	0	 YES	1.8	12	16.3	4.3
YBBN  	1	0	2016-5-2	0	 YES	2.5	21.12	22.12	1
YBBN  	0	0	2016-5-3	0	 NO	10000	 	 	 





[bou@bous-fed31 fog_climatology_2019]$ grep -i  FG 2016_May.txt
YBBN  ,01/05/2016 08:30,s, 2, 90,  2.9,  5.1, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  800,3,11000,5,27000,   ,     ,   ,     ,   ,     ,1, 21.3, 20.5, 95,1015.9,   0.0,   9.8
YBBN  ,01/05/2016 09:00,s, 2,  0,  0.0,  0.0, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  800,5,27000, ,     ,   ,     ,   ,     ,   ,     ,1, 20.6, 20.0, 96,1016.2,   0.0,   9.8
YBBN  ,01/05/2016 09:30,s, 2,260,  2.9,  6.0, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  800,5,25000, ,     ,   ,     ,   ,     ,   ,     ,1, 20.4, 19.8, 96,1016.5,   0.0,   9.8
YBBN  ,01/05/2016 10:00,s, 2,210,  6.0,  8.0, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  800,5,25000, ,     ,   ,     ,   ,     ,   ,     ,1, 20.5, 20.0, 97,1017.2,   0.0,   9.8
YBBN  ,01/05/2016 11:00,s, 2,200,  1.0,  4.1, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  700,5,25000, ,     ,   ,     ,   ,     ,   ,     ,1, 20.2, 19.9, 98,1017.0,   0.0,   9.8
YBBN  ,01/05/2016 11:30,s, 2,  0,  0.0,  0.0, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  700,5,25000, ,     ,   ,     ,   ,     ,   ,     ,1, 20.3, 19.8, 97,1017.0,   0.0,   9.8
YBBN  ,01/05/2016 12:00,s, 2,190,  4.1,  6.0, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1,  700,6,25000, ,     ,SCT, 5600,   ,     ,   ,     ,0, 20.0, 19.7, 98,1017.1,   0.0,   9.8
YBBN  ,01/05/2016 12:30,s, 2,150,  2.9,  4.1, 10.00,   ,  3.10,11,MI    FG,        ,        ,  ,  ,  ,1,  600,3, 5300,5,25000,   ,     ,   ,     ,   ,     ,1, 19.9, 19.6, 98,1016.9,   0.0,   9.8
YBBN  ,01/05/2016 13:00,s, 2,  0,  0.0,  0.0, 10.00,   ,  7.00,11,MI    FG,        ,        ,  ,  ,  ,1,  600,4, 9800,5,24000,SCT, 7900,   ,     ,   ,     ,0, 19.7, 19.4, 98,1016.8,   0.0,   9.8
YBBN  ,01/05/2016 13:30,s, 2,  0,  0.0,  0.0, 10.00,   ,  5.00,11,MI    FG,        ,        ,  ,  ,  ,1,  700,3, 7800,6,23000,SCT,10000,   ,     ,   ,     ,0, 19.9, 19.6, 98,1016.3,   0.0,   9.8
YBBN  ,01/05/2016 14:00,s, 2,  0,  0.0,  0.0, 10.00,   ,  5.00,11,MI    FG,        ,        ,  ,  ,  ,1,  700,3,12000,6,23000,   ,     ,   ,     ,   ,     ,1, 19.8, 19.5, 98,1016.1,   0.0,   9.8
YBBN  ,01/05/2016 15:30,s, 2,360,  2.9,  4.1, 10.00,   ,  6.00,11,MI    FG,        ,        ,  ,  ,  ,1,  600,3,20000,5,24000,   ,     ,   ,     ,   ,     ,1, 19.9, 19.6, 98,1014.9,   0.0,   9.8
YBBN  ,01/05/2016 16:00,s, 2,310,  1.9,  5.1,  9.00,   ,  1.80,11,MI    FG,        ,        ,  ,  ,  ,1,  800,5,24000, ,     ,   ,     ,   ,     ,   ,     ,1, 19.2, 19.0, 99,1014.7,   0.0,   9.8
YBBN  ,01/05/2016 20:07,s, 2,190,  1.0,  4.1, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1, 2300,3,22000,7,25000,   ,     ,   ,     ,   ,     ,1, 19.5, 19.0, 97,1016.4,   0.0,   9.8
YBBN  ,01/05/2016 20:30,s, 2,230,  2.9,  5.1, 10.00,   , 10.00,11,MI    FG,        ,        ,  ,  ,  ,1, 2000,3,22000,7,25000,   ,     ,   ,     ,   ,     ,1, 19.3, 19.0, 98,1016.6,   0.0,   9.8
YBBN  ,02/05/2016 21:42,s, 2,210,  7.0,  8.9,  2.50,   ,  4.40,42,PR    FG,        ,        ,  ,  ,  ,8,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     ,0, 19.3, 19.0, 98,1014.6,   0.0,  19.2




[bou@bous-fed31 fog_climatology_2019]$ grep -i '02/05/2016' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
YBBN  ,02/05/2016 21:42,s, 2,210,  7.0, 16.6,  2.50,   ,  4.40,42,PR    FG,        ,        ,8,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     , 19.3, 19.0, 98,1014.6,   0.0,  19.2


checking data for day we had storms previous day - nb mist reported previos obs


YBBN  ,02/05/2016 21:00,s, 2,200,  6.0, 14.8,  8.00,   , 10.00,  ,        ,        ,        ,2,  200,5,  900,6,30000,SCT,  200,BKN,  900,   ,     , 19.2, 18.6, 96,1014.3,   0.0,  19.2
YBBN  ,02/05/2016 21:30,s, 2,210,  7.0, 16.6,  5.00,   ,  7.00,10,      BR,        ,        ,4,  100,8,  300, ,     ,OVC,  100,   ,     ,   ,     , 19.4, 18.9, 97,1014.5,   0.0,  19.2
YBBN  ,02/05/2016 21:42,s, 2,210,  7.0, 16.6,  2.50,   ,  4.40,42,PR    FG,        ,        ,8,  100, ,     , ,     ,OVC,  100,   ,     ,   ,     , 19.3, 19.0, 98,1014.6,   0.0,  19.2
YBBN  ,02/05/2016 22:00,s, 2,210,  8.0, 18.4,  9.00,   ,  5.00,  ,        ,        ,        ,3,  200,8,  400, ,     ,OVC,  100,   ,     ,   ,     , 19.6, 19.1, 97,1014.7,   0.0,  19.2


[bou@bous-fed31 fog_climatology_2019]$ grep -i '03/05/2016' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
[bou@bous-fed31 fog_climatology_2019]$ grep -i '04/05/2016' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'
[bou@bous-fed31 fog_climatology_2019]$ grep -i '05/05/2016' HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |grep 'FG'


++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
APRIL 2020
++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
Creatd some new scripts Apr 2020 to check my fog dates against Robs FCT tool

The File amb_dates_rob_only.txt contains list of dates seperated by a commas

The Data file "./HM01X_Data_040842.txt"  is in same folder
[bou@bous-fed31 fog_climatology_2019]$ grep  '22/03/2008' ./HM01X_Data_040842.txt | head -n 1
hm, 40842,12/1992,       ,94578,YBBN  ,   4.5,22/03/2008 00:00,m,   0.0,   0.0, 25.9, 21.5, 19.1, 66,  22.1,  33.4, 11.1,100, 15.9,4, 8, 2500, ,  ,     , ,  ,     , ,  ,     ,   ,     ,   ,     ,   ,     ,1, 10.00,   , 10.00,  , ,  ,      , ,  ,      , ,  ,      ,  ,      ,  ,      ,  ,      ,  ,  ,  ,1017.4,1016.4,1017.5, 2, ,2008,03,22,00,06, 0,METARAWS YBBN 220000 10011/16KT 9999 4CU025 25.9/19.1 1017.5 RMK RF00.0/000.0 CLD:CLR BLW 125 VIS:9999 BV:13.5 IT:38.1 VER:2.1 TTF:NOSIG                                                                                                                                                                                                                                             

I write a bash loop to read the dates one at a time and then search by date and 
then grab some fields before sending to text file

for date in $(cat amb_dates_rob_only.txt);do 
echo $date;
grep  $date ./tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} '|grep 'FG' >> amb_fog_dates.txt

done

This only echos the dates - the CPU and memory usage goes up - 
so pc doing something but no output!! except echo of the dates


Data and Digital Group | Business Solution Services
Bureau of Meteorology
T: +61 7 3239 8613 | M: +61 4 6875 1943
===================================================================================
E: graham.buis@bom.gov.au | Teams: graham.buis@bom.gov.au
Hi Vinord,
You are missing a semicolon ';' after "echo $date" as highlighted in your code below.

You are processing the entire data file for each date and you are selecting for 'FG' only after you have split the lines. 
From an efficiency perspective it would be much better to process the big data file only once and 
do the expensive line processing only on the lines you need.


egrep allows you to grep for regex expressions. So if you can get your dates into a string separated by '|', then you can do this:


DATESTR="date1|date2|date3"
e.g. "29/10/2017|22/03/2008"    (remove single quotes and replace commas with vertical bars)

DATESTR=`sed -e "s/'//g" -e "s/,/|/g" -e "s/\\n//g"amb_dates_rob_only.txt `
egrep FG ./tcz_se_qld/HM01X_Data_040842.txt | egrep "$DATESTR" | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' >> output_file

 
This way you only read the data file once, filter by FG lines, then filter by dates, the process the remaining lines.
 
You can use awk to do it in a single command and save on multiple shell commands.

awk '/FG/ && /$DATESTR/ {print …}' in_file >> out_file
==========================================================================

The awk only version does not work!!

These are the steps

get dates list from file generated from set algebra in python script
fog_dates.py and fog_plots.py

which compares dates from my fog tool and Roberts

Then use sed to strip the list chars '[' and ']' (python list symbols)
and single qoutes around dates and replace delimiters/comma "," with pipe |
and remove all spaces - store these dates in variable $DATESTR

DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  YBBN_vin_only_dates.txt`

Note if you just want to look at dates on screen better replace commas with doble newlines 
and dont push on to bash variable

sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/\\n\\n/g" compare_rob_vin_new/YBBN_dates_vin_only.txt



[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
16/05/2008|02/06/2008|03/06/2008|12/06/2008|19/06/2008|14/07/2008|16/07/2008|04/06/2009|15/06/2009|23/06/2009|23/08/2009|11/10/2009|10/04/2010|24/04/2010|04/05/2010|11/05/2010|28/07/2010|29/07/2010|21/04/2011|22/04/2012|13/07/2012|22/08/2012|21/06/2013|05/07/2013|14/09/2013|23/09/2013|31/05/2014|02/06/2014|19/05/2015|20/05/2015|29/05/2015|17/06/2015|23/07/2015|05/10/2015|02/05/2016|18/07/2016|14/05/2017|20/05/2017|21/05/2017|23/05/2017|07/07/2017|11/08/2017|12/10/2017|28/06/2018|28/07/2018|08/09/2018

Reason for getting dates in this form seperated by pipes is that 
egrep expects list of patterns seperated by |

$ man egrep
grep, egrep, fgrep - print lines that match patterns
grep searches for PATTERNS in each FILE.  
PATTERNS is one or patterns separated by newline characters, 
and grep prints each line that matches a pattern.
the variant programs egrep and fgrep are the same as grep -E and grep -F


To grab all these dates from the data file
$ egrep "$DATESTR" ./HM01X_Data_040842.txt 

To 1st get all lines with FG and then grab these dates
$ egrep FG ./HM01X_Data_040842.txt | egrep "$DATESTR"  

To 1st get all dates then look for fog
$ egrep "$DATESTR"  | egrep FG ./HM01X_Data_040842.txt


To use awk to split data on comma delimiter and grabs only certain 'numbered' fields
egrep FG ./HM01X_Data_040842.txt | egrep "$DATESTR" | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' > output_file.txt




Robs definition for FG
Station	Station.Number	Time.Zone	HAM.base	HAM.vis	Fog.Def
YAMB	40004	Australia/Brisbane	1959	7	D10
YBBN	40842	Australia/Brisbane	1337	7	F6
YBOK	41359	Australia/Brisbane	1635	6	A0.8
YBRK	39083	Australia/Brisbane	1544	7	C10

YBBN F6  ---> Vis < 6 AND SCT/BKN/OVC001
YAMB D10 ---> (Vis < 1) OR ( (Vis <= 10 AND SCT/BKN/OVC001 )
YBRK C10 ---> (Vis <= 1) OR ( (Vis <= 10 AND SCT/BKN/OVC001) )
YBOK A0.8 --> Vis <= 0.8
For YBCG, YBAF and YBSU, YTWB, YGLA Fog.Def in NA ???



Compare this to my conditions
For manual sites YBBN/YAMB
if ( (($PW =~ /4[0-9]/) || 
    	  (trim($PW1) =~ /(PR|BC|VC|BL|DR)\s*FG/i) ||
    	  (trim($PW2) =~ /(PR|BC|VC|BL|DR)\s*FG/i) ||
    	  ($PW =~ /10/))
      # &&    (trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\s*[1-2]00$/)) # cld on gnd reqd condition 
      && (($PW !~ /12/) &&  (trim($PW1) !~ /MI\s*FG/i))  # make sure no MIST(10) or MIFG(12)
      && ($pptn10min < 0.1)  # just sanity check - silly observer fog report rain 0.4mm 11/10/2019 20:00
      && (($T-$Td) < 2) 
      && (($vis_obs < 10)||($vis_aws < 10)))  #sanity check - observer saying fog 13/06/2014 00:30 vis>10km

For AUTO OBS sites
if ( ($wind_SPD <= 10) &&   # to stop elevated spots like YTWB dropping fogs!!
    	 (($vis_obs <= 6)||($vis_aws <= 6)) && # ROB uses Vis<8km bigger NET!
    	 (($T-$Td) < 2) &&
    	 (($pptn10min < 0.1) && ($pptn_last < 0.1)) && 
         ((trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\s*[1-2]00$/)) )# NB fogs missed if no cld near gnd


STEPS TO LOOK UP DATES IN AWS DATA FILE
++++++++++++++++++++++++++++++++++++++++++
$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBBN_dates_vin_only.txt`

 echo $DATESTR
27/01/2008|31/03/2008|02/04/2008|20/05/2008|05/06/2008|27/08/2008|10/09/2008|04/10/2008|15/03/2009|05/04/2009|23/04/2009|29/04/2009|05/05/2009|16/06/2009|09/08/2009|10/08...

egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040842.txt | awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' > compare_rob_vin_new/YBBN_fogdays_vin_only.txt


# print all lines auto_vis < 1km
only vis < 1km
$ awk -F , '$10 < 1 {print ;}' compare_rob_vin/YAMB_fogdays_rob_only.txt |wc
    754   25348  147030

only cloud SCT|BKN|OVC/ 100ft  (Note no cld 100ft !! really
$ awk -F , '/$24~SCT|BKN|OVC/ && /$25~\s\s100/ {print ;}' compare_rob_vin/YAMB_fogdays_rob_only.txt |wc
      0       0       0

only cld cld SCT|BKN|OVC 100 or 200 ft --> /$25~\s\s100|\s\s200/
$ awk -F , '/$24~SCT|BKN|OVC/ && /$25~\s\s100|\s\s200/ {print ;}' compare_rob_vin/YAMB_fogdays_rob_only.txt |wc
    108    3278   21060

vis<1 ORRRRR cld SCT|BKN|OVC 100 OR SCT|BKN|OVC 200 ft
$ awk -F , '$10 < 1 || /$24~SCT|BKN|OVC/ && /$25~\s\s100|\s\s200/ {print ;}' compare_rob_vin/YAMB_fogdays_rob_only.txt |wc
    858   28497  167310

vis<1 ANDDDD cld SCT|BKN|OVC 100 or 200 ft (only 4 matches)
$ awk -F , '$10 < 1 && /$24~SCT|BKN|OVC/ && /$25~\s\s100|\s\s200/ {print ;}' compare_rob_vin/YAMB_fogdays_rob_only.txt |wc
      4     129     780


Change code May 31st
MANUAL CHECKER  	        
if ( (($PW =~ /4[0-9]/) || 
    	  (trim($PW1) =~ /(PR|BC|VC|BL|DR)\s*FG/i) ||
    	  (trim($PW2) =~ /(PR|BC|VC|BL|DR)\s*FG/i) )
      # ($PW =~ /10/)) don't test for BR unlike MIFG/12 we test explicitly check to remove  
      && ((trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\s*[1-5]00$/) ||
          (trim($obs_CL1A.$obs_CL1H) =~ /^[1-8]\s*[1-5]00$/))
      && (($PW !~ /12/) && ($PW !~ /[5-9][0-9]/) && (trim($PW1) !~ /MI\s*FG/i))  # make sure no MIFG(12) and no precip wx
      && ($pptn10min < 0.2)  # just sanity check - silly observer fog report rain 0.4mm 11/10/2019 20:00
      && (($T-$Td) < 2)     # just sanity check 
      && (($vis_obs < 10)||($vis_aws < 10)))  #YAMB observer has BCFG fkn every morning 6am vis>10km 
 
AUTO CHECKER

} elsif ( (($vis_obs <= 6)||($vis_aws <= 6)) && # ROB uses Vis<8km bigger NET!
    	      (($T-$Td) < 2) &&
    	      (($pptn10min < 0.2) && ($pptn_last < 0.2)) &&
    	      (($PW !~ /12/) && ($PW !~ /[5-9][0-9]/) && (trim($PW1) !~ /MI\s*FG/i)) &&
    	      ((trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\s*[1-5]00$/) ||
               (trim($obs_CL1A.$obs_CL1H) =~ /^[1-8]\s*[1-5]00$/)))
    #          ((trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\D{2}[1-2]00$/)) )# NB fogs missed if no cld near gnd - usually MIFG so OK
    #&& ($wind_SPD <= 10) &&   # to stop elevated spots like YTWB dropping fogs!!
    #||          ($PW_auto =~ /[2-3]0/) || ($PW_auto =~ /3[1-4]/)    # PW sensors not yet turned on for many AWS sites
    #||          ($PW_15 =~ /[2-3]0/  ) || ($PW_15 =~ /3[1-4]/  )
    #||          ($PW_60 =~ /[2-3]0/  ) || ($PW_60 =~ /3[1-4]/  )  ) )
    {

  

---------------------------------------------
Old code  VS New code vin auto vs man
(Old fog counts in ()
New one definitely gets more fog - If we allow BR/10 get 285 fog days!!!

YBBN   
Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 181
common_dates :	 181

YBAF
Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using VINS manual obs matching
all_dates :	 140   (124   )
common_dates :	 140

YAMB  (920 days with PW==BR/10
Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using VINS manual obs matching

Dates in man check only [Timestamp('2012-11-28 00:00:00')]
all_dates :	 658   (720   )
common_dates :	 658


YBCG
Getting fog data for YBCG derived using VINS auto obs matching
Getting fog data for YBCG derived using VINS manual obs matching
all_dates :	 17   (10   )
common_dates :	 17


YBSU
Getting fog data for YBSU derived using VINS auto obs matching
Getting fog data for YBSU derived using VINS manual obs matching
all_dates :	 88   (  56  )
common_dates :	 88


YTWB
Getting fog data for YTWB derived using VINS auto obs matching
Getting fog data for YTWB derived using VINS manual obs matching
all_dates :	 527 (121)
common_dates :	 527 (121)


YBOK
Getting fog data for YBOK derived using VINS auto obs matching
Getting fog data for YBOK derived using VINS manual obs matching
all_dates :	 315  (306)
common_dates :	 315 

YBRK
Getting fog data for YBRK derived using VINS auto o=bs matching
Getting fog data for YBRK derived using VINS manual obs matching
all_dates :	 663   (672   )
common_dates :	 663



===============================================================
Old VS New code Vin vs Rob


YBBN
Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using ROBS FCT
all_dates :	 98		old -->	98 
common_dates :	 46		old -->	46 
not_common :	 52		old -->	 52
vins_only :	 52		old -->	 52
robs_only :	 0		old -->	 0

YBAF
Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using ROBS FCT
all_dates :	 245		old -->	 244
common_dates :	 119		old -->	 103
not_common :	 126		old -->	 141
vins_only :	 5		old -->	 4
robs_only :	 121		old -->	 137

YAMB (all_dates 1004 when PW BR/10 match)
Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using ROBS FCT
all_dates :	 896		old -->	 913
common_dates :	 501		old -->	487 
not_common :	 395		old -->	 426
vins_only :	 88		old -->	 105
robs_only :	 307		old -->	 321

YBCG
Getting fog data for YBCG derived using VINS auto obs matching
Getting fog data for YBCG derived using ROBS FCT
all_dates :	 97		old -->	 96
common_dates :	 10		old -->	 7
not_common :	 87		old -->	 89
vins_only :	  1		old -->	 0
robs_only :	 86		old -->	 89

YBSU
Getting fog data for YBSU derived using VINS auto obs matching
Getting fog data for YBSU derived using ROBS FCT
all_dates :	 273		old -->	 269
common_dates :	 77		old -->	 48
not_common :	 196		old -->	 221
vins_only :	 4		old -->	 0
robs_only :	 192		old -->	 221

YTWB
Getting fog data for YTWB derived using VINS auto obs matching
Getting fog data for YTWB derived using ROBS FCT
all_dates :	 808		old -->	 762
common_dates :	 421		old -->	104 
not_common :	 387		old -->	 658
vins_only :	 48		old -->	 2
robs_only :	 339		old -->	 656

YBOK (all_dates 408 with BR/10 match)
Getting fog data for YBOK derived using VINS auto obs matching
Getting fog data for YBOK derived using ROBS FCT
all_dates :	 360		old -->	 342
common_dates :	 220		old -->	 216
not_common :	 140		old -->	 126
vins_only :	 50		old -->	 32
robs_only :	 90		old -->	 94

YBRK (all_dates 549 with BR/10 match)
Getting fog data for YBRK derived using VINS auto obs matching
Getting fog data for YBRK derived using ROBS FCT
all_dates :	 483		old -->	 479
common_dates :	 296		old -->	283 
not_common :	 187		old -->	 196
vins_only :	 137		old -->	 133
robs_only :	 50		old -->	 63



New code

cloud at/below 300ft
vis<10 manual <= 6 auto

if ( (($PW =~ /4[0-9]/) || 
    	  (trim($PW1) =~ /(PR|BC|VC|BL|DR)\s*FG/i) ||
    	  (trim($PW2) =~ /(PR|BC|VC|BL|DR)\s*FG/i) )
      # ($PW =~ /10/)) don't test for BR unlike MIFG/12 we test explicitly check to remove  
      # && ((trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\s*[1-5]00$/)
      && ((trim($aws_CL1H) =~ /[1-3]00$/) || (trim($obs_CL1H) =~ /[1-3]00$/))
      && (($PW !~ /12/) && ($PW !~ /[5-9][0-9]/) && (trim($PW1) !~ /MI\s*FG/i))  # make sure no MIFG(12) and no precip wx
      && ($pptn10min < 0.2)  # just sanity check - silly observer fog report rain 0.4mm 11/10/2019 20:00
      && (($T-$Td) < 2)     # just sanity check 
      && (($vis_obs < 10)||($vis_aws < 10))) 

} elsif ( (($vis_obs <= 6)||($vis_aws <= 6)) && # ROB uses Vis<8km bigger NET!
    	      (($T-$Td) < 2) &&
    	      (($pptn10min < 0.2) && ($pptn_last < 0.2)) && 
    	      ((trim($aws_CL1H) =~ /[1-3]00$/) || (trim($obs_CL1H) =~ /[1-3]00$/)))




YBBN
Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 186
common_dates :	 186
not_common :	 0
auto_only :	 0
man_only :	 0
YBAF
Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 198
common_dates :	 198
not_common :	 0
auto_only :	 0
man_only :	 0
YAMB
Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 742
common_dates :	 742
not_common :	 0
auto_only :	 0
man_only :	 0
YBCG
Getting fog data for YBCG derived using VINS auto obs matching
Getting fog data for YBCG derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 70
common_dates :	 70
not_common :	 0
auto_only :	 0
man_only :	 0
YBSU
Getting fog data for YBSU derived using VINS auto obs matching
Getting fog data for YBSU derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 162
common_dates :	 162
not_common :	 0
auto_only :	 0
man_only :	 0
YTWB
Getting fog data for YTWB derived using VINS auto obs matching
Getting fog data for YTWB derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 1259
common_dates :	 1259
not_common :	 0
auto_only :	 0
man_only :	 0
YBOK
Getting fog data for YBOK derived using VINS auto obs matching
Getting fog data for YBOK derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 360
common_dates :	 360
not_common :	 0
auto_only :	 0
man_only :	 0
YBRK
Getting fog data for YBRK derived using VINS auto obs matching
Getting fog data for YBRK derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 629
common_dates :	 629
not_common :	 0
auto_only :	 0
man_only :	 0




Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using ROBS FCT
all_dates :	 81
common_dates :	 46
not_common :	 35
vins_only :	 35
robs_only :	 0
YBAF
Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using ROBS FCT
all_dates :	 281
common_dates :	 135
not_common :	 146
vins_only :	 41
robs_only :	 105
YAMB
Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using ROBS FCT
all_dates :	 871
common_dates :	 545
not_common :	 326
vins_only :	 63
robs_only :	 263
YBCG
Getting fog data for YBCG derived using VINS auto obs matching
Getting fog data for YBCG derived using ROBS FCT
all_dates :	 126
common_dates :	 15
not_common :	 111
vins_only :	 30
robs_only :	 81
YBSU
Getting fog data for YBSU derived using VINS auto obs matching
Getting fog data for YBSU derived using ROBS FCT
all_dates :	 312
common_dates :	 103
not_common :	 209
vins_only :	 43
robs_only :	 166
YTWB
Getting fog data for YTWB derived using VINS auto obs matching
Getting fog data for YTWB derived using ROBS FCT
all_dates :	 1128
common_dates :	 740
not_common :	 388
vins_only :	 368
robs_only :	 20
YBOK
Getting fog data for YBOK derived using VINS auto obs matching
Getting fog data for YBOK derived using ROBS FCT
all_dates :	 387
common_dates :	 211
not_common :	 176
vins_only :	 77
robs_only :	 99
YBRK
Getting fog data for YBRK derived using VINS auto obs matching
Getting fog data for YBRK derived using ROBS FCT
all_dates :	 458
common_dates :	 285
not_common :	 173
vins_only :	 112
robs_only :	 61


New code check for BKN/OVC cloud cover >=1000ft 
for 2nd and 3rd cloud groups - 
previous fog detection picks gets many many fog days when there is BKN/OVC cloud cover
we correct for this


YBBN
Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 180
common_dates :	 180
not_common :	 0
auto_only :	 0
man_only :	 0
YBAF
Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 170
common_dates :	 170
not_common :	 0
auto_only :	 0
man_only :	 0
YAMB
Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 695
common_dates :	 695
not_common :	 0
auto_only :	 0
man_only :	 0
YBCG
Getting fog data for YBCG derived using VINS auto obs matching
Getting fog data for YBCG derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 37
common_dates :	 37
not_common :	 0
auto_only :	 0
man_only :	 0
YBSU
Getting fog data for YBSU derived using VINS auto obs matching
Getting fog data for YBSU derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 125
common_dates :	 125
not_common :	 0
auto_only :	 0
man_only :	 0
YTWB
Getting fog data for YTWB derived using VINS auto obs matching
Getting fog data for YTWB derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 1167
common_dates :	 1167
not_common :	 0
auto_only :	 0
man_only :	 0
YBOK
Getting fog data for YBOK derived using VINS auto obs matching
Getting fog data for YBOK derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 332
common_dates :	 332
not_common :	 0
auto_only :	 0
man_only :	 0
YBRK
Getting fog data for YBRK derived using VINS auto obs matching
Getting fog data for YBRK derived using VINS manual obs matching
Dates in auto check only []
Dates in man check only []
all_dates :	 587
common_dates :	 587
not_common :	 0
auto_only :	 0
man_only :	 0



-------------------------------------------------------------------------------
YBBN
Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using ROBS FCT
all_dates :	 	79
common_dates :	 45
not_common :	 34
vins_only :	 33
robs_only :	 1

Don't need any changes to detection for YBBN - appears to be capturing most fog days

[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBBN_dates_rob_only.txt`
[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
02/06/2008
[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBBN_dates_vin_only.txt`
[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
11/06/2008|18/06/2008|15/07/2008|28/09/2008|05/11/2008|18/11/2008|20/11/2008|24/06/2009|22/08/2009|28/08/2009|24/05/2010|25/06/2010|
10/08/2010|26/09/2010|09/05/2011|07/10/2011|23/04/2012|20/06/2012|10/07/2012|11/07/2012|23/09/2012|12/06/2013|29/03/2014|21/06/2014|
24/06/2015|17/09/2015|22/10/2015|21/07/2016|09/04/2017|24/05/2017|16/08/2017|24/02/2018|26/08/2018

-------------------------------------------------------------------------------

YBAF
Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using ROBS FCT
all_dates :	 264
common_dates :	 129
not_common :	 135
vins_only :	 24
robs_only :	 111   <-- too many misses by my algorithm

[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBAF_dates_vin_only.txt`
[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
02/06/2008|18/06/2008|15/04/2009|03/05/2010|28/07/2010|10/08/2010|06/01/2011|27/08/2011|06/10/2011|26/11/2011|20/09/2012|23/09/2012|01/11/2012|01/05/2013|03/05/2013|13/05/2013|11/12/2013|27/03/2014|16/01/2015|30/03/2016|30/03/2017|04/12/2017|25/03/2018|05/07/2018

[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBAF_dates_rob_only.txt`
[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
09/06/2008|17/06/2008|13/07/2008|16/07/2008|17/07/2008|28/09/2008|18/02/2009|02/06/2009|03/06/2009|01/07/2009|07/07/2009|03/08/2009|22/08/2009|14/09/2009|20/09/2009|29/11/2009|21/04/2010|22/04/2010|11/07/2010|30/07/2010|26/09/2010|04/10/2010|01/08/2011|14/08/2011|25/08/2011|01/09/2011|06/09/2011|03/10/2011|29/06/2012|26/07/2012|18/09/2012|01/04/2013|05/05/2013|10/05/2013|11/05/2013|28/05/2013|29/05/2013|30/05/2013|05/06/2013|07/06/2013|10/06/2013|12/06/2013|04/07/2013|14/07/2013|19/07/2013|28/07/2013|30/08/2013|11/09/2013|06/10/2013|20/10/2013|02/03/2014|16/03/2014|25/04/2014|16/05/2014|17/05/2014|22/05/2014|19/06/2014|20/06/2014|23/06/2014|04/07/2014|22/07/2014|09/08/2014|01/09/2014|15/09/2014|27/09/2014|29/12/2014|03/05/2015|05/05/2015|28/05/2015|30/05/2015|22/06/2015|23/06/2015|27/06/2015|30/06/2015|22/07/2015|30/07/2015|01/08/2015|19/08/2015|06/09/2015|29/09/2015|30/09/2015|13/03/2016|27/04/2016|19/07/2016|20/07/2016|07/08/2016|19/05/2017|22/05/2017|28/05/2017|04/06/2017|17/06/2017|27/06/2017|03/07/2017|04/07/2017|05/07/2017|15/07/2017|16/07/2017|28/07/2017|29/07/2017|06/08/2017|10/08/2017|14/08/2017|18/11/2017|14/04/2018|20/04/2018|06/07/2018|27/07/2018|11/08/2018|28/08/2018|15/09/2018|18/10/2018


we need to decrease days missed by my algorithm
robs detects over 100 more fog days - wee need to check how many of these are genuine
[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBAF_dates_rob_only.txt`

[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
09/06/2008|17/06/2008|13/07/2008|16/07/2008|17/07/2008|28/09/2008|18/02/2009|02/06/2009|03/06/2009|01/07/2009|07/07/2009|03/08/2009|
22/08/2009|14/09/2009|20/09/2009|29/11/2009|21/04/2010|22/04/2010|11/07/2010|30/07/2010|26/09/2010|04/10/2010|01/08/2011|14/08/2011|
25/08/2011|01/09/2011|06/09/2011|03/10/2011|29/06/2012|26/07/2012|18/09/2012|01/04/2013|05/05/2013|10/05/2013|11/05/2013|28/05/2013|
29/05/2013|30/05/2013|05/06/2013|07/06/2013|10/06/2013|12/06/2013|04/07/2013|14/07/2013|19/07/2013|28/07/2013|30/08/2013|11/09/2013|
06/10/2013|20/10/2013|02/03/2014|16/03/2014|25/04/2014|16/05/2014|17/05/2014|22/05/2014|19/06/2014|20/06/2014|23/06/2014|04/07/2014|
22/07/2014|09/08/2014|01/09/2014|15/09/2014|27/09/2014|29/12/2014|03/05/2015|05/05/2015|28/05/2015|30/05/2015|22/06/2015|23/06/2015|
27/06/2015|30/06/2015|22/07/2015|30/07/2015|01/08/2015|19/08/2015|06/09/2015|29/09/2015|30/09/2015|13/03/2016|27/04/2016|19/07/2016|
20/07/2016|07/08/2016|19/05/2017|22/05/2017|28/05/2017|04/06/2017|17/06/2017|27/06/2017|03/07/2017|04/07/2017|05/07/2017|15/07/2017|
16/07/2017|28/07/2017|29/07/2017|06/08/2017|10/08/2017|14/08/2017|18/11/2017|14/04/2018|20/04/2018|06/07/2018|27/07/2018|11/08/2018|
28/08/2018|15/09/2018|18/10/2018

[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040211.txt | 
awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' 
> compare_rob_vin_new/YBAF_days_rob_only.csv


only grab line with vis less than 1km as that seems to be the test for fog 
[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040211.txt | 
awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' 
| awk -F , '$10<1 {print ;}' > compare_rob_vin_new/YBAF_days_rob_only_visLT1.csv


[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YBAF_dates_vin_only.txt`
[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
02/06/2008|18/06/2008|15/04/2009|03/05/2010|28/07/2010|10/08/2010|06/01/2011|27/08/2011|06/10/2011|26/11/2011|20/09/2012|23/09/2012|
01/11/2012|01/05/2013|03/05/2013|13/05/2013|11/12/2013|27/03/2014|16/01/2015|30/03/2016|30/03/2017|04/12/2017|25/03/2018|05/07/2018

[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040211.txt | 
awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' | awk -F , '$10<1 {print ;}' > compare_rob_vin_new/YBAF_days_vin_only_visLT1.csv
[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040211.txt | 
awk -F , '{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24","$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' | awk -F , '$10<2 {print ;}' > compare_rob_vin_new/YBAF_days_vin_only_visLT2.csv



# for YAMB/YBAF turn off upper cld checks (2nd and 3rd Cloud layers) as these places can get fog
# with some cloud
#&& ((trim($aws_CL2A.$aws_CL2H) !~ /^(BKN|OVC)\s*[1-9][0-9]{3}$/) &&
#    (trim($aws_CL3A.$aws_CL3H) !~ /^(BKN|OVC)\s*[1-9][0-9]{3}$/))

The first condition below (check for fog like PW ) would always be FALSE for YBAF/YAMB on auto obs
and it falls on the elseif clause to catch any fog.

so basically needs to tick these 3 checks 
(($vis_obs <= )||($vis_aws <= 1))
(($T-$Td) <= 1)
(($pptn10min < 0.2)

These give the best results for YBAF 

if ( (($PW =~ /4[0-9]/) || 
     (trim($PW1) =~ /(PR|BC|VC|BL|DR)\s*FG/i) ||
     (trim($PW2) =~ /(PR|BC|VC|BL|DR)\s*FG/i) )
      && (($PW !~ /12/) && ($PW !~ /[5-9][0-9]/) && ($PW1 !~ /(SH|RA|DZ|TS)/) && (trim($PW1) !~ /MI\s*FG/i))  # make sure no MIFG(12) and no precip wx
      && ((trim($aws_CL1A.$aws_CL1H) =~ /^(SCT|BKN|OVC)\s*[1-5]00$/) ||
          (trim($obs_CL1A.$obs_CL1H) =~ /^[1-8]\s*[1-5]00$/))
      && ($pptn10min < 0.2)  # just sanity check - silly observer fog report rain 0.4mm 11/10/2019 20:00
      && (($T-$Td) < 2)     # just sanity check 
      && (($vis_obs <= 10)||($vis_aws <= 10)))  #Y

elsif ( (($vis_obs <= )||($vis_aws <= 1)) 
        (($T-$Td) <= 1) &&
    	(($pptn10min < 0.2) && ($pptn_last < 0.2)) &&
    	(($PW !~ /12/) && ($PW !~ /[5-9][0-9]/) && ($PW1 !~ /(SH|RA|DZ|TS)/) && (trim($PW1) !~ /MI\s*FG/i))) 


Note larger VIS threshold VIS<=10 and Td seperation T-Td<2 for manual fog reports
Allows for cases where fog not at airport but some dist like VCFG, PRFG ,BCFG etc

But for auto cases VIS<=1 and T-Td<=1 so lower thresholds 

Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using ROBS FCT
all_dates :	 685   (264)
common_dates :	 240	(129)
not_common :	 445	(135)
vins_only :	 445	(24)   - appears too high!!
robs_only :	 0	(111) 

If use vis threshold 1km in the else/other clause (similar to what Rob has) and also as used for YAMB below
results much better

Getting fog data for YBAF derived using VINS auto obs matching
Getting fog data for YBAF derived using ROBS FCT
all_dates :	 	240 (264)
common_dates :	232	(129)   <-- common dates inc by ~ 100
not_common :	 8	(135)      (not common only 8!)
vins_only :	 	 0	(24)     
robs_only :	 	 8	(111)    Perfect - reduced by over 100

When Run for YBBN also get better results

YBBN
Getting fog data for YBBN derived using VINS auto obs matching
Getting fog data for YBBN derived using ROBS FCT
all_dates :	 	(90) 79  extra 11 days
common_dates :	(45) 45
not_common :	(45) 34
vins_only :	 	(44) 33  extra 11 days
robs_only :	     (1) 1

--------------------------------------------------------------

YAMB
Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using ROBS FCT
all_dates :	 856
common_dates :	 522
not_common :	 334
vins_only :	 48
robs_only :	 286

Getting fog data for YAMB derived using VINS auto obs matching
Getting fog data for YAMB derived using ROBS FCT
          [VIS<=10] vis<10 
all_dates :	 	[915]	904   (856)
common_dates :	 [741]	738	(522)   <-- common dates inc by ~ 230
not_common :	 [174]	166	(334)   not common is halved!
vins_only :	 	[107]	96		(48)
robs_only :	 	[67]	70  (286)     216 dates accounted for now!
probably pendulum swing bit far - I got more fog dates 107 that are not in Robs

Added about 10 fogs from roberts which look genuine to my set 
all_dates :	 	915
common_dates :	 752
not_common :	 163
vins_only :	 	107
robs_only :	 	56


check those 96 extra days I have - see if genuince

[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YAMB_dates_vin_only.txt`

[bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
27/01/2008|13/03/2008|31/03/2008|02/04/2008|05/06/2008|27/08/2008|04/10/2008|06/11/2008|13/03/2009|21/03/2009|25/03/2009|05/04/2009|
23/04/2009|04/05/2009|13/05/2009|10/08/2009|01/09/2009|15/11/2009|11/02/2010|22/02/2010|24/03/2010|02/04/2010|03/05/2010|05/06/2010|
17/06/2010|24/06/2010|09/07/2010|09/08/2010|17/08/2010|19/06/2011|20/06/2011|23/06/2011|27/06/2011|12/07/2011|04/09/2011|13/09/2011|
14/09/2011|08/12/2011|27/12/2011|30/03/2012|02/05/2012|15/05/2012|29/07/2012|20/08/2012|04/10/2012|03/04/2013|28/04/2013|02/06/2013|
04/06/2013|25/08/2013|27/08/2013|05/09/2013|08/09/2013|09/09/2013|22/09/2013|26/09/2013|09/10/2013|21/10/2013|22/10/2013|11/11/2013|
30/03/2014|03/04/2014|16/04/2014|21/04/2014|22/04/2014|23/04/2014|29/04/2014|19/05/2014|20/05/2014|26/06/2014|25/08/2014|26/08/2014|
29/09/2014|14/11/2014|30/01/2015|22/04/2015|07/08/2015|01/10/2015|15/10/2015|29/11/2015|06/04/2016|02/05/2016|31/08/2016|19/11/2016|
21/11/2016|13/05/2017|21/05/2017|22/07/2017|10/09/2017|15/02/2018|25/02/2018|07/04/2018|03/08/2018|13/09/2018|20/10/2018|23/10/2018

[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040004.txt | grep FG|awk -F , 
'{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24",
"$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' > compare_rob_vin_new/YAMB_days_vin_only_FG.csv

[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040004.txt |awk -F , 
'{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24",
"$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' |
 awk -F , '$10<1 {print ;}' > compare_rob_vin_new/YAMB_days_vin_only_visLT1.csv

[bou@bous-fed31 fog_climatology_2019]$ DATESTR=`sed -e "s/\[//" -e "s/\]//" -e "s/'//g" -e "s/ //g" -e "s/,/|/g"  ./compare_rob_vin_new/YAMB_dates_rob_only.txt`
[
bou@bous-fed31 fog_climatology_2019]$ echo $DATESTR 
10/01/2008|06/02/2008|29/06/2008|21/09/2008|18/12/2008|14/03/2009|18/05/2009|26/06/2009|03/01/2010|28/02/2010|04/05/2010|20/05/2010|
25/05/2010|28/05/2010|02/06/2010|17/07/2010|10/08/2010|23/08/2010|11/12/2010|24/12/2010|01/01/2011|05/01/2011|19/01/2011|31/01/2011|
04/03/2011|17/03/2011|18/03/2011|21/03/2011|30/03/2011|02/05/2011|23/05/2011|11/06/2011|07/12/2011|10/12/2011|19/02/2012|16/03/2012|
17/03/2012|01/06/2012|22/03/2013|24/04/2013|20/06/2013|07/07/2013|23/11/2013|16/08/2014|29/12/2014|21/03/2015|04/04/2015|15/04/2015|
16/04/2015|08/05/2015|17/10/2015|06/11/2015|26/12/2015|13/01/2016|14/02/2016|27/04/2016|05/05/2016|02/06/2016|08/07/2016|10/08/2016|
11/09/2016|01/12/2016|12/05/2017|08/02/2018|16/03/2018|18/03/2018|08/06/2018|11/08/2018|09/10/2018|23/12/2018

[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040004.txt | grep FG|awk -F , 
'{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24",
"$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' > compare_rob_vin_new/YAMB_days_rob_only_FG.csv

[bou@bous-fed31 fog_climatology_2019]$ egrep "$DATESTR" ./tcz_se_qld/HM01X_Data_040004.txt |awk -F , 
'{print $6","$8","$9","$65","$19","$18","$20","$40","$41","$42","$43","$45$46","$48$49","$51$52","$59","$60","$61","$21","$23","$24",
"$26","$27","$29","$33","$34","$35","$36","$37","$38","$39","$12","$14","$15","$64","$10","$11} ' | 
awk -F , '$10<1 {print ;}' > compare_rob_vin_new/YAMB_days_rob_only_visLT1.csv


----------------------------------------------------------------------------
The vis<=1 and T-Td<=1 is capturing more fog days 
when used in the else clause

(($vis_obs <= )||($vis_aws <= 1)) 
(($T-$Td) <= 1) &&

YBCG
Getting fog data for YBCG derived using VINS auto obs matching
Getting fog data for YBCG derived using ROBS FCT
all_dates :	 	(97)102  small loss <5 days!
common_dates :	(95)14  <-- huge capture extra 90 days
not_common :	(2) 88
vins_only :	 	(1)6
robs_only :	 	(1)82 <-- rob has extra 1 days only


YBSU
Getting fog data for YBSU derived using VINS auto obs matching
Getting fog data for YBSU derived using ROBS FCT
all_dates :	 	(275)283	small loss <10 days!
common_dates :	 (265)100  <-- huge capture extra 165 days
not_common :	 (10)183
vins_only :	 	(6)14
robs_only :	 	(4)169  <-- huge from 169 to only 4 extra days in Robs


YTWB
Getting fog data for YTWB derived using VINS auto obs matching
Getting fog data for YTWB derived using ROBS FCT
all_dates :	 	(776)1056		<-- bad  loose 300 days !!
common_dates :	(586)731	
not_common :	(190)325
vins_only :	 	 (16)296	  <--I capture 280 less days
robs_only :	 	(174) 29      <-- Rob captures extra 150 days


YBOK
Getting fog data for YBOK derived using VINS auto obs matching
Getting fog data for YBOK derived using ROBS FCT
all_dates :	 	(344) 370		small loss 26 days!
common_dates :	 (307) 206    <-- extra 100 days in common GOOD
not_common :	 (37) 164     <-- reduce by 130 days
vins_only :	 	(34) 60
robs_only :	 	(3) 104


YBRK
Getting fog data for YBRK derived using VINS auto obs matching
Getting fog data for YBRK derived using ROBS FCT
all_dates :	 	(416) 436	 small loss 20 days!
common_dates :	 (308) 277  <-- extra 30 days common
not_common :	 (108) 159  
vins_only :	 	(70) 90
robs_only :	 	(38) 69   <-- we capture 30 more days


Robs definition for FG
Station	Station.Number	Time.Zone	HAM.base	HAM.vis	Fog.Def
YAMB	40004	Australia/Brisbane	1959	7	D10
YBBN	40842	Australia/Brisbane	1337	7	F6
YBOK	41359	Australia/Brisbane	1635	6	A0.8
YBRK	39083	Australia/Brisbane	1544	7	C10

YBBN F6  ---> Vis < 6 AND SCT/BKN/OVC001
YAMB D10 ---> (Vis < 1) OR ( (Vis <= 10 AND SCT/BKN/OVC001 )
YBRK C10 ---> (Vis <= 1) OR ( (Vis <= 10 AND SCT/BKN/OVC001) )
YBOK A0.8 --> Vis <= 0.8
For YBCG, YBAF and YBSU, YTWB, YGLA Fog.Def in NA ???







# get 18z observations from tcz file


data_18Z = pd.read_csv(os.path.join('app','data', 'station_data_18Z.csv'), index_col=[1])

df.loc[df.index.hour == 18]
df_temp = df_temp.resample('D').first()




sonde_data = pickle.load( open(
            os.path.join('app','data','sonde_hank_final.pkl'), 'rb'))


if set(stations).intersection(set(['YBBN','YBAF','YAMB','YBSU','YBCG','YBOK','YTWB','YKRY'])):
        # load Brisbane sonde data file
        sonde_data = pickle.load( open(
            os.path.join('app','data','sonde_hank_final.pkl'), 'rb'))
        print("BEGIN PROCESSING TS FORECASTS FOR SEQ STATIONS\n",sonde_data.tail())
elif set(stations).intersection(set(['YBRK','YGLA','YTNG','YBUD','YHBA','YMYB','YEML','YCMT','YMRB','YBMK','YBPN','YBHM'])):
        # load Rockhampton sonde data file
        sonde_data = pickle.load( open(
        os.path.join('app','data','sonde_hank_YBRK.pkl'), 'rb'))
        print("BEGIN PROCESSING TS FORECASTS FOR CAPRICORN/CENTRAL HIGHLANDS COAST\n",sonde_data.tail())

# The UTC date for sonde data is actually one less than the calendar data
sonde_data.set_index(sonde_data.index - pd.Timedelta(str(1) + ' days'),inplace=bool(1))
sonde_data = sonde_data[['P900', 'wdir900', 'wspd900', 'P850', 'wdir850', 'wspd850']] #only grab these cols
sonde_data.rename(columns={'wdir900': '900_wdir','wspd900':'900_WS',\
            'wdir850': '850_wdir','wspd850':'850_WS'}, inplace = True)



df  = pickle.load(
                open(
                os.path.join('app','data', station+'_aws.pkl'), 'rb'))


fg_dates = get_fog_data_vins(station = station,get_dates_only='Yes')





Process

Read Brisbane sonde file - grab 1800Z obs (or 23Z) in no 18Z - 
should be similar I guess esp 950/900 level
We use Python installed work machine to read f160s off ADAMS

activate my python3.7 virt env
[vinorda@qld-rfc-ws45 Downloads]$ source /home/accounts/qdisplay/avguide/av_env/bin/activate

copy the function definitions across to my Downloads folder
(av_env) [vinorda@qld-rfc-ws45 Downloads]$ 
cp /home/accounts/qdisplay/avguide/utility_functions_sep2018.py ./

then

>> import utility_functions_sep2018 as bous

### If you edit your module - need to reload it for changes to be reflected acrss to REPL

>>> import importlib
<module 'utility_functions_sep2018' from '/home/bou/Downloads/utility_functions_sep2018.py'>

>>> importlib.reload(bous)

>>> bous.batch_download_F160_hanks(station="040842",start_date='2018-Feb-12',end_date='2020-Jun-02')
and Bobs your uncle


So we get 1000s of data files like this on your folder

[bou@bous-fed31 Downloads]$ ls f160
stn040842_2018-10-10.txt  stn040842_2018-8-26.txt   stn040842_2019-5-7.txt
stn040842_2018-10-11.txt  stn040842_2018-8-27.txt   stn040842_2019-5-8.txt
stn040842_2018-10-12.txt  stn040842_2018-8-28.txt   stn040842_2019-5-9.txt
stn040842_2018-10-13.txt  stn040842_2018-8-29.txt   stn040842_2019-6-10.txt
stn040842_2018-10-14.txt  stn040842_2018-8-2.txt    stn040842_2019-6-11.txt
stn040842_2018-10-15.txt  stn040842_2018-8-30.txt   stn040842_2019-6-12.txt
stn040842_2018-10-16.txt  stn040842_2018-8-31.txt   stn040842_2019-6-13.txt
stn040842_2018-10-17.txt  stn040842_2018-8-3.txt    stn040842_2019-6-14.txt
stn040842_2018-10-18.txt  stn040842_2018-8-4.txt    stn040842_2019-6-15.txt



>>> subprocess.run(["ls","-ltr","f160"])
....
-rw-rw-r--. 1 bou bou    3529 Jun  4 17:09 stn040842_2020-6-1.txt
-rw-rw-r--. 1 bou bou    3297 Jun  4 17:09 stn040842_2020-5-31.txt
-rw-rw-r--. 1 bou bou    2661 Jun  4 17:09 stn040842_2020-6-2.txt

can't do globs
>>> subprocess.run(["ls","-ltr", 'f160/stn040842_2020-6*.txt'])
ls: cannot access 'f160/stn040842_2020-6*.txt': No such file or directory


>>> subprocess.run(["ls","-ltr", 'f160/stn040842_2020-6-2.txt'])
-rw-rw-r--. 1 bou bou 2661 Jun  4 17:09 f160/stn040842_2020-6-2.txt


>>> subprocess.run(["cat",'f160/stn040842_2020-6-2.txt'])
[Header]
aifstime[18]="202006020000"
stationname[30]="Brisbane Ap"
stationnumber=040842
levels=52
fields=5
w_units=1
[$]

[Trace]
Pres,Geop,Temp,Dewp,Dir,Spd,AbsHum
1018.0,-9999.0,16.2,-0.8,250.0,20.6,-9999.0
1017.0,-9999.0,-9999.0,-9999.0,260.0,10.2,-9999.0
1000.0,-9999.0,13.8,0.8,250.0,18.6,-9999.0
974.0,-9999.0,11.4,0.4,-9999.0,-9999.0,-9999.0
946.0,-9999.0,-9999.0,-9999.0,245.0,22.6,-9999.0
943.0,-9999.0,9.0,0.0,245.0,22.6,-9999.0
926.0,-9999.0,8.2,-4.8,-9999.0,-9999.0,-9999.0
925.0,-9999.0,8.2,-5.8,245.0,38.0,-9999.0
918.0,-9999.0,-9999.0,-9999.0,250.0,45.2,-9999.0
910.0,-9999.0,-9999.0,-9999.0,250.0,44.2,-9999.0
909.0,-9999.0,10.8,-21.2,-9999.0,-9999.0,-9999.0



#++++++++++++++++++++++++++++++++++++++++++++++++++
# Now batch proces these sondes into a csv file

>>> sfc_500 = bous.batch_process_F160_hanks()

>>> sfc_500.head()
              Pres  Temp  Dewp    Dir   Spd   Pres  ...  Spd   Pres  Temp  Dewp  Dir  Spd
2000-02-05  1019.0  27.1  15.3   40.0  11.2  925.0  ...  NaN  500.0  -5.7 -54.0  NaN  NaN
2000-02-06  1019.0  26.9  15.9   90.0   6.0  925.0  ...  NaN  500.0  -6.3 -54.3  NaN  NaN
2000-02-07  1024.0  25.0  19.4  100.0  12.2  925.0  ...  NaN  500.0  -6.3 -44.8  NaN  NaN
2000-02-08  1024.0  26.8  17.3  100.0  14.4  925.0  ...  NaN  500.0  -7.6 -49.3  NaN  NaN
2000-02-09  1022.0  26.9  18.8  100.0  10.2  908.0  ...  NaN  500.0  -7.4 -39.0  NaN  NaN


>>> sfc_500.tail()
              Pres  Temp  Dewp    Dir   Spd   Pres  ...   Spd   Pres  Temp  Dewp    Dir   Spd
2020-05-29  1022.0  22.6  14.6  180.0   8.2  910.0  ...   9.2  500.0 -16.9 -49.9  280.0  35.0
2020-05-30  1025.0  18.6  11.6  210.0  10.2  910.0  ...  15.4  500.0 -15.1 -53.1  155.0  29.8
2020-05-31  1023.0  18.2  14.1  210.0   7.2  910.0  ...   6.2  500.0 -11.1 -34.1  280.0  38.0
2020-06-01  1019.0  17.2  15.6  235.0   5.2  910.0  ...   7.2  500.0 -13.3 -57.3  285.0  40.2
2020-06-02  1018.0  16.2  -0.8  250.0  20.6  910.0  ...  46.2  500.0 -17.3 -51.3  265.0  78.2


Now our sonde data is almost ready
just need to extra the 900 winds from it

>>> sfc_500.shape
(7424, 25)

>>> sfc_500.info()
<class 'pandas.core.frame.DataFrame'>
DatetimeIndex: 7424 entries, 2000-02-05 to 2020-06-02
Freq: D
Data columns (total 25 columns):
Pres    7407 non-null float64
Temp    7407 non-null float64
Dewp    7407 non-null float64
Dir     7407 non-null float64
Spd     7407 non-null float64
Pres    7407 non-null float64
Temp    486 non-null float64
Dewp    486 non-null float64
Dir     7196 non-null float64
Spd     7196 non-null float64
Pres    7407 non-null float64
Temp    7380 non-null float64
Dewp    7380 non-null float64
Dir     7135 non-null float64
Spd     7135 non-null float64
Pres    7407 non-null float64
Temp    7378 non-null float64
Dewp    7378 non-null float64
Dir     7258 non-null float64
Spd     7258 non-null float64
Pres    7407 non-null float64
Temp    7376 non-null float64
Dewp    7376 non-null float64
Dir     7320 non-null float64
Spd     7320 non-null float64
dtypes: float64(25)
memory usage: 1.5 MB


We could have better columns names!!!

>>> sfc_500.columns
Index(['Pres', 'Temp', 'Dewp', 'Dir', 'Spd', 'Pres', 'Temp', 'Dewp', 'Dir',
       'Spd', 'Pres', 'Temp', 'Dewp', 'Dir', 'Spd', 'Pres', 'Temp', 'Dewp',
       'Dir', 'Spd', 'Pres', 'Temp', 'Dewp', 'Dir', 'Spd'],
      dtype='object')

>>> sfc_500.iloc[:5,:10].head()
              Pres  Temp  Dewp    Dir   Spd   Pres  Temp  Dewp  Dir  Spd
2000-02-05  1019.0  27.1  15.3   40.0  11.2  925.0  17.8  11.6  NaN  NaN
2000-02-06  1019.0  26.9  15.9   90.0   6.0  925.0  17.6  11.4  NaN  NaN
2000-02-07  1024.0  25.0  19.4  100.0  12.2  925.0  17.8  13.9  NaN  NaN
2000-02-08  1024.0  26.8  17.3  100.0  14.4  925.0  17.0  11.4  NaN  NaN
2000-02-09  1022.0  26.9  18.8  100.0  10.2  908.0  16.6   9.4  NaN  NaN


>>> sfc_500.iloc[-5:,:10].tail()
              Pres  Temp  Dewp    Dir   Spd   Pres  Temp  Dewp    Dir   Spd
2020-05-29  1022.0  22.6  14.6  180.0   8.2  910.0   NaN   NaN  145.0  23.6
2020-05-30  1025.0  18.6  11.6  210.0  10.2  910.0   NaN   NaN  125.0  24.6
2020-05-31  1023.0  18.2  14.1  210.0   7.2  910.0   NaN   NaN   60.0   8.2
2020-06-01  1019.0  17.2  15.6  235.0   5.2  910.0   NaN   NaN  335.0  20.6
2020-06-02  1018.0  16.2  -0.8  250.0  20.6  910.0   NaN   NaN  250.0  44.2

Gradient wind is col 6, 8, 10

>>> sfc_500.iloc[-5:,[5,8,9]].tail()
             Pres    Dir   Spd
2020-05-29  910.0  145.0  23.6
2020-05-30  910.0  125.0  24.6
2020-05-31  910.0   60.0   8.2
2020-06-01  910.0  335.0  20.6
2020-06-02  910.0  250.0  44.2

Note data for Calendar day - so the data for 2020-06-02  910.0  250.0  44.2
is actually for 2020-06-01 2300 UTC sonde
so we need to reindex the data with one day offset dates  

>>> grad_winds = sfc_500.iloc[:,[5,8,9]]
>>> grad_winds.tail()
             Pres    Dir   Spd
2020-05-29  910.0  145.0  23.6
2020-05-30  910.0  125.0  24.6
2020-05-31  910.0   60.0   8.2
2020-06-01  910.0  335.0  20.6
2020-06-02  910.0  250.0  44.2



>>> (grad_winds.index.date - pd.Timedelta(str(1) + ' days'))[-5:]
array([datetime.date(2020, 5, 28), datetime.date(2020, 5, 29),
       datetime.date(2020, 5, 30), datetime.date(2020, 5, 31),
       datetime.date(2020, 6, 1)], dtype=object)


now using this to reindex the dataframe may not work - it will just drop the last obs 
for date/index 2020-06-02 rather than setting its date/index to 2020-06-01

>>> test = grad_winds
>>> test.head()
             Pres  Dir  Spd
2000-02-05  925.0  NaN  NaN
2000-02-06  925.0  NaN  NaN
2000-02-07  925.0  NaN  NaN
2000-02-08  925.0  NaN  NaN
2000-02-09  908.0  NaN  NaN

>>> test.tail()
             Pres    Dir   Spd
2020-05-29  910.0  145.0  23.6
2020-05-30  910.0  125.0  24.6
2020-05-31  910.0   60.0   8.2
2020-06-01  910.0  335.0  20.6
2020-06-02  910.0  250.0  44.2


Kind of risky as we modifying index (take away 1 day) on the fly and inplace
no need to drop columns as we not using any existin col

>>> test.set_index(keys=(test.index.date - pd.Timedelta(str(1) + ' days')), drop=False,inplace=bool(1))
>>> test.head()
             Pres  Dir  Spd
2000-02-04  925.0  NaN  NaN
2000-02-05  925.0  NaN  NaN
2000-02-06  925.0  NaN  NaN
2000-02-07  925.0  NaN  NaN
2000-02-08  908.0  NaN  NaN

>>> test.tail()
             Pres    Dir   Spd
2020-05-28  910.0  145.0  23.6
2020-05-29  910.0  125.0  24.6
2020-05-30  910.0   60.0   8.2
2020-05-31  910.0  335.0  20.6
2020-06-01  910.0  250.0  44.2


Looks like all good - it workd

grad_winds = sfc_500.iloc[:,[5,8,9]]

>>> test = grad_winds
>>> test['new_index'] = test.index.date - pd.Timedelta(str(1) + ' days')
__main__:1: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame.
Try using .loc[row_indexer,col_indexer] = value instead

See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
>>> test.head()
             Pres  Dir  Spd   new_index
2000-02-05  925.0  NaN  NaN  2000-02-04
2000-02-06  925.0  NaN  NaN  2000-02-05
2000-02-07  925.0  NaN  NaN  2000-02-06
2000-02-08  925.0  NaN  NaN  2000-02-07
2000-02-09  908.0  NaN  NaN  2000-02-08
>>> test.tail()
             Pres    Dir   Spd   new_index
2020-05-29  910.0  145.0  23.6  2020-05-28
2020-05-30  910.0  125.0  24.6  2020-05-29
2020-05-31  910.0   60.0   8.2  2020-05-30
2020-06-01  910.0  335.0  20.6  2020-05-31
2020-06-02  910.0  250.0  44.2  2020-06-01

>>> test.set_index(keys='new_index', drop=False,inplace=bool(1))
>>> test.head()
             Pres  Dir  Spd   new_index
new_index                              
2000-02-04  925.0  NaN  NaN  2000-02-04
2000-02-05  925.0  NaN  NaN  2000-02-05
2000-02-06  925.0  NaN  NaN  2000-02-06
2000-02-07  925.0  NaN  NaN  2000-02-07
2000-02-08  908.0  NaN  NaN  2000-02-08
>>> test.tail()
             Pres    Dir   Spd   new_index
new_index                                 
2020-05-28  910.0  145.0  23.6  2020-05-28
2020-05-29  910.0  125.0  24.6  2020-05-29
2020-05-30  910.0   60.0   8.2  2020-05-30
2020-05-31  910.0  335.0  20.6  2020-05-31
2020-06-01  910.0  250.0  44.2  2020-06-01

Excellent this works both ways so we can drop the 'new_index' col or just use the 1st method 


>>> grad_winds = sfc_500.iloc[:,[5,8,9]]
>>> 
>>> 
>>> grad_winds.head(2)
             Pres  Dir  Spd
2000-02-05  925.0  NaN  NaN
2000-02-06  925.0  NaN  NaN
>>> grad_winds.tail(2)
             Pres    Dir   Spd
2020-06-01  910.0  335.0  20.6
2020-06-02  910.0  250.0  44.2
>>> grad_winds.set_index(keys=(grad_winds.index.date - pd.Timedelta(str(1) + ' days')),inplace=bool(1))
>>> grad_winds.head(2)
             Pres  Dir  Spd
2000-02-04  925.0  NaN  NaN
2000-02-05  925.0  NaN  NaN
>>> grad_winds.tail(2)
             Pres    Dir   Spd
2020-05-31  910.0  335.0  20.6
2020-06-01  910.0  250.0  44.2


Note index no longer datetime now - make it datetime

>>> grad_winds.index
Index([2000-02-04, 2000-02-05, 2000-02-06, 2000-02-07, 2000-02-08, 2000-02-09,
       2000-02-10, 2000-02-11, 2000-02-12, 2000-02-13,
       ...
       2020-05-23, 2020-05-24, 2020-05-25, 2020-05-26, 2020-05-27, 2020-05-28,
       2020-05-29, 2020-05-30, 2020-05-31, 2020-06-01],
      dtype='object', length=7424)

>>> grad_winds.index = pd.to_datetime(grad_winds.index)

>>> grad_winds.index
DatetimeIndex(['2000-02-04', '2000-02-05', '2000-02-06', '2000-02-07',
               '2000-02-08', '2000-02-09', '2000-02-10', '2000-02-11',
               '2000-02-12', '2000-02-13',
               ...
               '2020-05-23', '2020-05-24', '2020-05-25', '2020-05-26',
               '2020-05-27', '2020-05-28', '2020-05-29', '2020-05-30',
               '2020-05-31', '2020-06-01'],
              dtype='datetime64[ns]', length=7424, freq=None)


Summary steps
++++++++++++
col_names=['P_gnd','T_gnd','Td_gnd','wdir_gnd','wspd_gnd',
    'P910','T910','Td910','wdir910','wspd910',
    'P850','T850','Td850','wdir850','wspd850',
    'P700','T700','Td700','wdir700','wspd700',
    'P500','T500','Td500','wdir500','wspd500']

sonde = pd.read_csv('f160/sonde_data_hanks_2000to2020.csv', 
    parse_dates=[0], 
    index_col=[0], 
    names=col_names,
    skiprows=1, 
    header=None)

sonde.set_index(keys=(sonde.index.date - pd.Timedelta(str(1) + ' days')), 
    drop=False,inplace=bool(1))

sonde.index = pd.to_datetime(sonde.index)

return sonde[['wdir910', 'wspd910']].astype(float, 'ignore')


>>> sonde = bous.get_gradient_level_winds_from_sonde()

>>> sonde.head()
            900_wdir  900_WS
2000-02-04       NaN     NaN
2000-02-05       NaN     NaN
2000-02-06       NaN     NaN
2000-02-07       NaN     NaN
2000-02-08       NaN     NaN

>>> sonde.tail()
            900_wdir  900_WS
2020-05-28     145.0    23.6
2020-05-29     125.0    24.6
2020-05-30      60.0     8.2
2020-05-31     335.0    20.6
2020-06-01     250.0    44.2



sonde = bous.get_sounding_data(station='YBBN',level=900)

Now we load station AWS data and extract only the 1800Z obs into a dataframe

The pickle files are old aws data files from 2018 - so no point loading them

df  = pickle.load( open( os.path.join('app','data', station+'_aws.pkl'), 'rb'))

http://tcz.bom.gov.au:8889/tcz/anon/HM01X_D.HM01X_page

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
userid:vinorda
pwd:usual
,40004,40211,40842,40717,40861,41359,41529,39083
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

We need to load data from the new tcz data grab in 2020

# df = process_climate_zone_csv(cur_dir+'tcz/'+aws_file)
bous.process_climate_zone_csv(file):

However the climate zone columns downloaded are slightly different to the data grab in 2016/2017 from YAMB!
so we need to re-factor the old "process_climate_zone_csv" function to correctly read new tcz files

The tcz cols are what we can see in 2nd col here

we only need these cols  identified by <<<<

use_tcz_cols =[5,7,8,64,18,17,19,39,40,41,42,44,45,47,48,50,51,58,59,60,20,22,
                   23,25,26,28,32,33,34,35,36,37,38,11,13,14,63,9,10]


>>> cols_compare
        new_col_names                                      tcz_col_names  ++++ Cols we keep 40 columns
0                  hm                             Record identifier - hm
1               StNum              Bureau of Meteorology Station Number.
2              Opened                  Month/Year site opened. (MM/YYYY)
3              Closed                  Month/Year site closed. (MM/YYYY)
4             WMO_Num  WMO (World Meteorological Organisation) Index ...
5                AvID                                       Aviation ID.   <<<<
6         Elevation_m             Height of station above mean sea level
7                 UTC  Day/Month/Year Hour24:Minutes in DD/MM/YYYY HH...   <<<<   becomes index/key
8              M_type                                       Message type   <<<<
9           pptn10min             Precipitation in last 10 minutes in mm   <<<<
10            pptn9am           Precipitation since 9am local time in mm   <<<<
11                  T                       Air Temperature in degrees C   <<<<
12                Twb                  Wet bulb temperature in degrees C   
13                 Td                 Dew point temperature in degrees C   <<<<
14                 RH                  Relative humidity in percentage %   <<<<
15                 VP                             Vapour pressure in hPa   
16                SVP                   Saturated vapour pressure in hPa   
17                 WS                                Wind speed in knots   <<<<
18               WDir                          Wind direction in degrees   <<<<
19       MaxGust10min  Speed of maximum wind gust in last 10 minutes ...   <<<<
20           CL1_amnt           Cloud amount (of first group) in eighths   <<<<
21           CL1_type                Cloud type (of first group) in code   
22             CL1_ht              Cloud height (of first group) in feet   <<<<
23           CL2_amnt          Cloud amount (of second group) in eighths   <<<<
24           CL2_type               Cloud type (of second group) in code
25             CL2_ht             Cloud height (of second group) in feet   <<<<
26           CL3_amnt           Cloud amount (of third group) in eighths   <<<<
27           CL3_type                Cloud type (of third group) in code
28             CL3_ht              Cloud height (of third group) in feet   <<<<
29           CL4_amnt          Cloud amount (of fourth group) in eighths
30           CL4_type               Cloud type (of fourth group) in code
31             CL4_ht             Cloud height (of fourth group) in feet
32         Ceil1_amnt  Ceilometer cloud amount (of first group) in ei...   <<<<
33           Ceil1_ht   Ceilometer cloud height (of first group) in feet   <<<<
34         Ceil2_amnt  Ceilometer cloud amount (of second group) in e...   <<<<
35           Ceil2_ht  Ceilometer cloud height (of second group) in feet   <<<<
36         Ceil3_amnt  Ceilometer cloud amount (of third group) in ei...   <<<<
37           Ceil3_ht   Ceilometer cloud height (of third group) in feet   <<<<
38        CeilSKCflag                          Ceilometer sky clear flag   <<<<
39                vis                        Horizontal visibility in km   <<<<
40        vis_min_dir         Direction of minimum visibility in degrees   <<<<
41            vis_aws         Automatic Weather Station visibility in km   <<<<
42                 PW                            Present weather in code   <<<<
43            PW1_int         Intensity of first present weather in code
44           PW1_desc        Descriptor of first present weather in code   <<<<
45           PW1_type              Type of first present weather in code   <<<<
46            PW2_int        Intensity of second present weather in code
47           PW2_desc       Descriptor of second present weather in code   <<<<
48           PW2_type             Type of second present weather in code   <<<<
49            PW3_int         Intensity of third present weather in code
50           PW3_desc        Descriptor of third present weather in code   <<<<
51           PW3_type                     Type of third  present in code   <<<<
52           RW1_desc         Descriptor of first recent weather in code
53           RW1_type               Type of first recent weather in code
54           RW2_desc        Descriptor of second recent weather in code
55           RW2_type              Type of second recent weather in code
56           RW3_desc         Descriptor of third recent weather in code
57           RW3_type               Type of third recent weather in code
58             AWS_PW                        AWS present weather in code   <<<<
59        AWS_PW15min            AWS weather for last 15 minutes in code   <<<<
60        AWS_PW60min            AWS weather for last 60 minutes in code   <<<<
61                MSL                     Mean sea level pressure in hPa   
62                SLP                      Station level pressure in hPa
63                QNH                                QNH pressure in hPa   <<<<
64           AWS_Flag                     Automatic Weather Station Flag   <<<<
65           Err_Flag                                         Error Flag
66           Date_mod                          Date of last modification
67           Data_src                                        Data source
68                MSG                                   Original Message
69             Coment                                       Comment text
70                RMK                                       Remarks text
71     CAVOK_SKC_flag                               CAVOK/SKY clear flag   <<<<
72  RWY_ws_shear_flag                             Runway wind shear flag
73                END                                           # symbol



NB If we read the data above - gives col name in wrong order 
so cols are read in numeric ascending order so the the association between numeric col index 
and col labels goes out of sync


def process_climate_zone_csv_2020(file):
    import pandas as pd
    
    use_tcz_cols =[5,7,8,64,18,17,19,39,40,41,42,44,45,47,48,50,51,58,59,60,20,22,
                   23,25,26,28,32,33,34,35,36,37,38,11,13,14,63,9,10]

    tcz_col_names = ['AvID', 'date','M_type', 'AWS_Flag', 'WDir', 'WS', 'MaxGust10min',
    'vis', 'vis_min_dir', 'vis_aws', 'PW', 'PW1_desc', 'PW1_type', 'PW2_desc','PW2_type',
    'PW3_desc', 'PW3_type', 'AWS_PW', 'AWS_PW15min', 'AWS_PW60min', 'CL1_amnt', 'CL1_ht',
    'CL2_amnt', 'CL2_ht','CL3_amnt', 'CL3_ht', 'Ceil1_amnt', 'Ceil1_ht', 'Ceil2_amnt', 'Ceil2_ht',
    'Ceil3_amnt', 'Ceil3_ht', 'CeilSKCflag', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']

    df = pd.read_csv(file,
      parse_dates=[7],            # col 8,9 datetime like
      dayfirst=True,                # format like '31/10/2012 23:30' in file
      infer_datetime_format = True, # faster parsing 5-10x
      usecols=use_tcz_cols,
      names=tcz_col_names,          # rename climatezone columns using cols list
      index_col=['date'],                # make UTC datetime index
      low_memory=False,
      skiprows=1,                   # skip 1 line(s) at the start of the file.
      header=None )                 # don't use headers - we skipped header row!


Solution
read col indices and names into frame and sort the indices - this sorts the labels as well in order and then use those col labels



>>> tcz_col_names = ['AvID', 'date','M_type', 'AWS_Flag', 'WDir', 'WS', 'MaxGust10min',
...     'vis', 'vis_min_dir', 'vis_aws', 'PW', 'PW1_desc', 'PW1_type', 'PW2_desc','PW2_type',
...     'PW3_desc', 'PW3_type', 'AWS_PW', 'AWS_PW15min', 'AWS_PW60min', 'CL1_amnt', 'CL1_ht',
...     'CL2_amnt', 'CL2_ht','CL3_amnt', 'CL3_ht', 'Ceil1_amnt', 'Ceil1_ht', 'Ceil2_amnt', 'Ceil2_ht',
...     'Ceil3_amnt', 'Ceil3_ht', 'CeilSKCflag', 'T', 'Td', 'RH', 'QNH', 'pptn10min','pptnSince9']

>>> use_tcz_cols =[5,7,8,64,18,17,19,39,40,41,42,44,45,47,48,50,51,58,59,60,20,22,
...                    23,25,26,28,32,33,34,35,36,37,38,11,13,14,63,9,10]

>>> name = pd.DataFrame(list(zip(use_tcz_cols,list(tcz_col_names))),
...     columns=['cols_nums', 'col_names'])

>>> name
    cols_nums     col_names
0           5          AvID
1           7          date
2           8        M_type
3          64      AWS_Flag
4          18          WDir
5          17            WS
6          19  MaxGust10min
7          39           vis
8          40   vis_min_dir
9          41       vis_aws
10         42            PW
11         44      PW1_desc
12         45      PW1_type
13         47      PW2_desc
14         48      PW2_type
15         50      PW3_desc
16         51      PW3_type
17         58        AWS_PW
18         59   AWS_PW15min
19         60   AWS_PW60min
20         20      CL1_amnt
21         22        CL1_ht
22         23      CL2_amnt
23         25        CL2_ht
24         26      CL3_amnt
25         28        CL3_ht
26         32    Ceil1_amnt
27         33      Ceil1_ht
28         34    Ceil2_amnt
29         35      Ceil2_ht
30         36    Ceil3_amnt
31         37      Ceil3_ht
32         38   CeilSKCflag
33         11             T
34         13            Td
35         14            RH
36         63           QNH
37          9     pptn10min
38         10    pptnSince9


>>> name.sort_values(by=['cols_nums'],ascending=True,axis='rows')
    cols_nums     col_names
0           5          AvID
1           7          date
2           8        M_type
37          9     pptn10min
38         10    pptnSince9
33         11             T
34         13            Td
35         14            RH
5          17            WS
4          18          WDir
6          19  MaxGust10min
20         20      CL1_amnt
21         22        CL1_ht
22         23      CL2_amnt
23         25        CL2_ht
24         26      CL3_amnt
25         28        CL3_ht
26         32    Ceil1_amnt
27         33      Ceil1_ht
28         34    Ceil2_amnt
29         35      Ceil2_ht
30         36    Ceil3_amnt
31         37      Ceil3_ht
32         38   CeilSKCflag
7          39           vis
8          40   vis_min_dir
9          41       vis_aws
10         42            PW
11         44      PW1_desc
12         45      PW1_type
13         47      PW2_desc
14         48      PW2_type
15         50      PW3_desc
16         51      PW3_type
17         58        AWS_PW
18         59   AWS_PW15min
19         60   AWS_PW60min
36         63           QNH
3          64      AWS_Flag


>>> name['col_names'].values
array(['AvID', 'date', 'M_type', 'pptn10min', 'pptnSince9', 'T', 'Td',
       'RH', 'WS', 'WDir', 'MaxGust10min', 'CL1_amnt', 'CL1_ht',
       'CL2_amnt', 'CL2_ht', 'CL3_amnt', 'CL3_ht', 'Ceil1_amnt',
       'Ceil1_ht', 'Ceil2_amnt', 'Ceil2_ht', 'Ceil3_amnt', 'Ceil3_ht',
       'CeilSKCflag', 'vis', 'vis_min_dir', 'vis_aws', 'PW', 'PW1_desc',
       'PW1_type', 'PW2_desc', 'PW2_type', 'PW3_desc', 'PW3_type',
       'AWS_PW', 'AWS_PW15min', 'AWS_PW60min', 'QNH', 'AWS_Flag'],
      dtype=object)


def process_climate_zone_csv_2020(file):
    import pandas as pd

    use_tcz_cols =[ 5,  7,  8,  9, 10, 11, 13, 14, 17, 18, 19, 20, 22, 23, 25, 26, 28,
       32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 48, 50, 51,
       58, 59, 60, 63, 64]
    
    tcz_col_names = ['AvID', 'date', 'M_type', 'pptn10min', 'pptnSince9', 'T', 'Td',
    'RH', 'WS', 'WDir', 'MaxGust10min', 'CL1_amnt', 'CL1_ht',
    'CL2_amnt', 'CL2_ht', 'CL3_amnt', 'CL3_ht', 'Ceil1_amnt',
    'Ceil1_ht', 'Ceil2_amnt', 'Ceil2_ht', 'Ceil3_amnt', 'Ceil3_ht',
    'CeilSKCflag', 'vis', 'vis_min_dir', 'vis_aws', 'PW', 'PW1_desc',
    'PW1_type', 'PW2_desc', 'PW2_type', 'PW3_desc', 'PW3_type',
    'AWS_PW', 'AWS_PW15min', 'AWS_PW60min', 'QNH', 'AWS_Flag']

    df = pd.read_csv(file,
    parse_dates=[7],            # col 8,9 datetime like
    dayfirst=True,                # format like '31/10/2012 23:30' in file
    infer_datetime_format = True, # faster parsing 5-10x
    usecols=use_tcz_cols,
    names=tcz_col_names,          # rename climatezone columns using cols list
    index_col=['date'],                # make UTC datetime index
    low_memory=False,
    skiprows=1,                   # skip 1 line(s) at the start of the file.
    header=None ) 



>>> importlib.reload(bous)
<module 'bous' from '/home/bou/Downloads/utility_function_sep2018.py'>
>>> df = bous.process_climate_zone_csv_2020('HM01X_Data_040842.txt')
>>> df.tail()
                    AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  MaxGust10min  CL1_amnt  ...  PW1_desc  PW1_type  PW2_desc  PW2_type  PW3_desc PW3_type  AWS_PW AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                                          ...                                                                                                              
29/03/2020 16:00  YBBN        m        0.0         0.0  19.6  18.6  94.0  5.1  230.0           6.0       NaN  ...                                                                                              1016.9       2.0
29/03/2020 16:30  YBBN        m        0.0         0.0  19.6  18.8  95.0  1.9  240.0           4.1       NaN  ...                                                                                              1016.7       2.0
29/03/2020 17:00  YBBN        m        0.0         0.0  19.8  18.8  94.0  4.1  210.0           5.1       NaN  ...                                                                                              1016.3       2.0
29/03/2020 17:30  YBBN        m        0.0         0.0  20.0  19.0  94.0  5.1  190.0           5.1       NaN  ...                                                                                              1016.1       2.0
29/03/2020 18:00  YBBN        m        0.0         0.0  20.0  18.9  93.0  5.1  190.0           6.0       NaN  ...                                                                                              1015.9       2.0

[5 rows x 38 columns]
>>> df.columns
Index(['AvID', 'M_type', 'pptn10min', 'pptnSince9', 'T', 'Td', 'RH', 'WS',
       'WDir', 'MaxGust10min', 'CL1_amnt', 'CL1_ht', 'CL2_amnt', 'CL2_ht',
       'CL3_amnt', 'CL3_ht', 'Ceil1_amnt', 'Ceil1_ht', 'Ceil2_amnt',
       'Ceil2_ht', 'Ceil3_amnt', 'Ceil3_ht', 'CeilSKCflag', 'vis',
       'vis_min_dir', 'vis_aws', 'PW', 'PW1_desc', 'PW1_type', 'PW2_desc',
       'PW2_type', 'PW3_desc', 'PW3_type', 'AWS_PW', 'AWS_PW15min',
       'AWS_PW60min', 'QNH', 'AWS_Flag'],
      dtype='object')

Now get the gpats data for these stations
(av_env) [vinorda@qld-rfc-ws45 avguide]$ pwd
/home/accounts/qdisplay/avguide

(av_env) [vinorda@qld-rfc-ws45 avguide]$ python ./extract_gpats_adamprd_2020.py

#######################################
Begin Processing YBBN
[-27.38413 153.11757]
-27.38413 153.11757
Reading gpats data for YBBN from ADAM...
Writing data to 	mp local filesystem...
/tmp/data/YBBN_gpats2020_10NM.csv


(av_env) [vinorda@qld-rfc-ws45 avguide]$ ls -ltr /tmp/data
-rw-rw-r--. 1 vinorda vinorda 1976777 Sep 29 23:19 YBBN_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 1784216 Sep 29 23:20 YBSU_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 1618918 Sep 29 23:20 YBCG_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2889035 Sep 29 23:20 YAMB_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2578036 Sep 29 23:21 YBAF_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2035526 Sep 29 23:21 YTWB_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2039678 Sep 29 23:21 YSSY_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2187647 Sep 29 23:21 YWOL_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2215055 Sep 29 23:22 YWLM_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2524571 Sep 29 23:22 YSRI_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2239958 Sep 29 23:47 YSBK_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2187920 Sep 29 23:48 YSCN_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2202361 Sep 29 23:48 YSHW_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 5448840 Sep 29 23:48 YPDN_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda  944026 Sep 29 23:49 YBAS_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda  691421 Sep 29 23:49 YMML_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda  321252 Sep 29 23:49 YPPH_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda  353213 Sep 29 23:49 YPAD_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 1169946 Sep 29 23:49 YSCB_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2461696 Sep 30 00:33 YBOK_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 2224992 Sep 30 00:33 YBWW_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 1146999 Sep 30 00:33 YBRK_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 3610244 Sep 30 00:34 YBRM_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda  621513 Sep 30 00:34 YPPD_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda 3455142 Sep 30 00:34 YPTN_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda    5749 Sep 30 00:34 YPCC_gpats2020_10NM.csv
-rw-rw-r--. 1 vinorda vinorda  125637 Sep 30 00:34 YPXM_gpats2020_10NM.csv


(av_env) [vinorda@qld-rfc-ws45 avguide]$ tar -cvzf gpats2020.tar.gz /tmp/data/
tar: Removing leading `/' from member names
/tmp/data/
/tmp/data/YSCN_gpats2020_10NM.csv
/tmp/data/YWOL_gpats2020_10NM.csv
/tmp/data/YSCB_gpats2020_10NM.csv
/tmp/data/YMML_gpats2020_10NM.csv
/tmp/data/YBSU_gpats2020_10NM.csv
/tmp/data/YSHW_gpats2020_10NM.csv
/tmp/data/YSRI_gpats2020_10NM.csv
/tmp/data/YSBK_gpats2020_10NM.csv
/tmp/data/YPDN_gpats2020_10NM.csv
/tmp/data/YBCG_gpats2020_10NM.csv
/tmp/data/YBRM_gpats2020_10NM.csv
/tmp/data/YBAS_gpats2020_10NM.csv
/tmp/data/YBRK_gpats2020_10NM.csv
/tmp/data/YWLM_gpats2020_10NM.csv
/tmp/data/YSSY_gpats2020_10NM.csv
/tmp/data/YPTN_gpats2020_10NM.csv
/tmp/data/YPPH_gpats2020_10NM.csv
/tmp/data/YPAD_gpats2020_10NM.csv
/tmp/data/YPPD_gpats2020_10NM.csv
/tmp/data/YBBN_gpats2020_10NM.csv
/tmp/data/YAMB_gpats2020_10NM.csv
/tmp/data/YPCC_gpats2020_10NM.csv
/tmp/data/YBWW_gpats2020_10NM.csv
/tmp/data/YBOK_gpats2020_10NM.csv
/tmp/data/YTWB_gpats2020_10NM.csv
/tmp/data/YBAF_gpats2020_10NM.csv
/tmp/data/YPXM_gpats2020_10NM.csv

(av_env) [vinorda@qld-rfc-ws45 avguide]$ ls -ltr *gz
-rw-rw-r--. 1 vinorda vinorda 14402897 Sep 30 00:37 gpats2020.tar.gz


resample data in each file to 30min then merge with aws data file - this gives us a ts flag
for each 1/2 hourly obs
We can use this flag to derive TS stats for each station 



To automate this process for multipe files

write short script

>>> import utility_functions_sep2018 as bous
>>> import pandas as pd
>>> tcz_file_map = {'YBBN':'040842', 'YBAF':'040211', 'YAMB':'040004',
...            'YBSU':'040861', 'YBCG':'040717', 'YTWB':'041529',
...            'YBOK':'041359', 'YBRK':'039083',
...            'YPCC':'200284','YPXM':'200790',
...            'YSSY':'066037','YSBK':'066136','YSRI':'067105',
...            'YSCN':'068192','YSHL':'068241'}

# read csv, extract relevant columns, do appropriate processing and then store picklised data on system
>>> for station,file in tcz_file_map.items():
...     bous.process_climate_zone_csv_2020(station)


(py37) [bou@bous-fed31 data]$ ls -ltr *newbie.pkl
-rw-rw-r--. 1 bou bou 86707415 Sep 23 15:59 YBBN_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 87766781 Sep 23 15:59 YBAF_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 77332368 Sep 23 16:00 YAMB_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 90855453 Sep 23 16:01 YBSU_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 89483560 Sep 23 16:02 YBCG_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 75752573 Sep 23 16:02 YTWB_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 75951898 Sep 23 16:03 YBOK_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 88463712 Sep 23 16:04 YBRK_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 76644234 Sep 23 16:05 YPCC_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 68629294 Sep 23 16:05 YPXM_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 87663305 Sep 23 16:06 YSSY_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 89222957 Sep 23 16:18 YSBK_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 92080519 Sep 23 16:19 YSRI_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 86373018 Sep 23 16:20 YSCN_aws_newbie.pkl
-rw-rw-r--. 1 bou bou 77069426 Sep 23 16:21 YSHL_aws_newbie.pkl

These pickled aws data is used during synop matching for fog and ts
But 1st we need to flag each observation with a TS and FG flag


TS flag is easier 

-------------------------------------------
Create TS Stats using tcz data file
USING merge_aws_gpats_data(sta,aws_file)
-------------------------------------------


[bou@bous-fed31 gpats]$ ls -l $(which rename)
-rwxr-xr-x. 1 root root 24360 Dec 17  2019 /usr/bin/rename


bou@bous-fed31 gpats]$ sudo cpan
[sudo] password for bou: 
Loading internal logger. Log::Log4perl recommended for better logging
Terminal does not support AddHistory.

To fix that, maybe try>  install Term::ReadLine::Perl

cpan shell -- CPAN exploration and modules installation (v2.28)
Enter 'h' for help.

cpan[1]> install File::Rename
Reading '/root/.local/share/.cpan/Metadata'

  RMBARKER/File-Rename-1.13.tar.gz
  ./Build test -- OK
Running Build install for RMBARKER/File-Rename-1.13.tar.gz
Building File-Rename
Installing /usr/local/share/man/man1/rename.1
Installing /usr/local/share/perl5/5.30/File/Rename.pm
Installing /usr/local/share/perl5/5.30/File/Rename/Options.pm
Installing /usr/local/share/man/man3/File::Rename::Options.3pm
Installing /usr/local/share/man/man3/File::Rename.3pm
Installing /usr/local/bin/rename


Note that this does not overwrites fedora OS supplied version of rename with perl version
as install is to local/bin

/usr/local/bin/rename


The old rename is still system wide default
[bou@bous-fed31 gpats]$ man /usr/bin/rename

Check
[bou@bous-fed31 gpats]$ /usr/bin/rename -n 's/gpats2020_//' *

Does nothing as does not support regex

local/bin/rename

bou@bous-fed31 gpats]$ /usr/local/bin/rename -vn 's/gpats2020_//' *
Unknown option: vn
Usage:
    rename [ -h|-m|-V ] [ -v ] [ -0 ] [ -n ] [ -f ] [ -d ]
    [ -e|-E perlexpr]*|perlexpr [ files ]

[bou@bous-fed31 gpats]$ /usr/local/bin/rename -n 's/gpats2020_//' *
rename(YAMB_gpats2020_10NM.csv, YAMB_10NM.csv)
rename(YBAF_gpats2020_10NM.csv, YBAF_10NM.csv)
rename(YBAS_gpats2020_10NM.csv, YBAS_10NM.csv)


To make perl rename the default system wide - just point the links for rename 
to new perl rename

https://www.freecodecamp.org/news/symlink-tutorial-in-linux-how-to-create-and-remove-a-symbolic-link/

ln -s <path to the file/folder to be linked> <new link or shortcut>

[bou@bous-fed31 gpats]$ ln -s /usr/local/bin/rename /usr/bin/rename
ln: failed to create symbolic link '/usr/bin/rename': File exists

[bou@bous-fed31 gpats]$ file /usr/bin/rename
/usr/bin/rename: ELF 64-bit LSB pie executable, x86-64, version 1 (SYSV), dynamically linked, interpreter /lib64/ld-linux-x86-64.so.2, BuildID[sha1]=676597bf9aa6916aa9f3ee5eaad921a79c596309, for GNU/Linux 3.2.0, stripped

[bou@bous-fed31 gpats]$ ls -Zaltr /usr/bin/rename
-rwxr-xr-x. 1 root root system_u:object_r:bin_t:s0 24360 Dec 17  2019 /usr/bin/rename


Can't be de as /usr/bin/rename  actually exists as file - so can't create a shortcutlink with same name!!

No probs we can still use our new remame byt using full path to specify which rename we wnat




[bou@bous-fed31 gpats]$ # Install prename (Perl version of rename):
[bou@bous-fed31 gpats]$ man prename
[bou@bous-fed31 gpats]$ # this perl version supports regex in renaming files
[bou@bous-fed31 gpats]$ sudo dnf install prename
Installed:
  prename-1.9-8.fc31.noarch                                                     



[bou@bous-fed31 gpats]$ ls -ltr *gpats2020*
-rw-rw-r--. 1 bou bou 1976777 Sep 29 23:19 YBBN_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 1784216 Sep 29 23:20 YBSU_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 1618918 Sep 29 23:20 YBCG_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2889035 Sep 29 23:20 YAMB_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2578036 Sep 29 23:21 YBAF_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2035526 Sep 29 23:21 YTWB_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2039678 Sep 29 23:21 YSSY_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2187647 Sep 29 23:21 YWOL_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2215055 Sep 29 23:22 YWLM_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2524571 Sep 29 23:22 YSRI_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2239958 Sep 29 23:47 YSBK_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2187920 Sep 29 23:48 YSCN_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2202361 Sep 29 23:48 YSHW_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 5448840 Sep 29 23:48 YPDN_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou  944026 Sep 29 23:49 YBAS_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou  691421 Sep 29 23:49 YMML_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou  321252 Sep 29 23:49 YPPH_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou  353213 Sep 29 23:49 YPAD_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 1169946 Sep 29 23:49 YSCB_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2461696 Sep 30 00:33 YBOK_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 2224992 Sep 30 00:33 YBWW_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 1146999 Sep 30 00:33 YBRK_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 3610244 Sep 30 00:34 YBRM_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou  621513 Sep 30 00:34 YPPD_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou 3455142 Sep 30 00:34 YPTN_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou    5749 Sep 30 00:34 YPCC_gpats2020_10NM.csv
-rw-rw-r--. 1 bou bou  125637 Sep 30 00:34 YPXM_gpats2020_10NM.csv


[bou@bous-fed31 gpats]$ /usr/local/bin/rename 's/gpats2020_//' *

[bou@bous-fed31 gpats]$ ls -ltr *gpats2020*
ls: cannot access '*gpats2020*': No such file or directory

[bou@bous-fed31 gpats]$ ls -ltr
total 49916
-rw-rw-r--. 1 bou bou 1976777 Sep 29 23:19 YBBN_10NM.csv
-rw-rw-r--. 1 bou bou 1784216 Sep 29 23:20 YBSU_10NM.csv
-rw-rw-r--. 1 bou bou 1618918 Sep 29 23:20 YBCG_10NM.csv
-rw-rw-r--. 1 bou bou 2889035 Sep 29 23:20 YAMB_10NM.csv
-rw-rw-r--. 1 bou bou 2578036 Sep 29 23:21 YBAF_10NM.csv
-rw-rw-r--. 1 bou bou 2035526 Sep 29 23:21 YTWB_10NM.csv
-rw-rw-r--. 1 bou bou 2039678 Sep 29 23:21 YSSY_10NM.csv
-rw-rw-r--. 1 bou bou 2187647 Sep 29 23:21 YWOL_10NM.csv
-rw-rw-r--. 1 bou bou 2215055 Sep 29 23:22 YWLM_10NM.csv
-rw-rw-r--. 1 bou bou 2524571 Sep 29 23:22 YSRI_10NM.csv
-rw-rw-r--. 1 bou bou 2239958 Sep 29 23:47 YSBK_10NM.csv
-rw-rw-r--. 1 bou bou 2187920 Sep 29 23:48 YSCN_10NM.csv
-rw-rw-r--. 1 bou bou 2202361 Sep 29 23:48 YSHW_10NM.csv
-rw-rw-r--. 1 bou bou 5448840 Sep 29 23:48 YPDN_10NM.csv
-rw-rw-r--. 1 bou bou  944026 Sep 29 23:49 YBAS_10NM.csv
-rw-rw-r--. 1 bou bou  691421 Sep 29 23:49 YMML_10NM.csv
-rw-rw-r--. 1 bou bou  321252 Sep 29 23:49 YPPH_10NM.csv
-rw-rw-r--. 1 bou bou  353213 Sep 29 23:49 YPAD_10NM.csv
-rw-rw-r--. 1 bou bou 1169946 Sep 29 23:49 YSCB_10NM.csv
-rw-rw-r--. 1 bou bou 2461696 Sep 30 00:33 YBOK_10NM.csv
-rw-rw-r--. 1 bou bou 2224992 Sep 30 00:33 YBWW_10NM.csv
-rw-rw-r--. 1 bou bou 1146999 Sep 30 00:33 YBRK_10NM.csv
-rw-rw-r--. 1 bou bou 3610244 Sep 30 00:34 YBRM_10NM.csv
-rw-rw-r--. 1 bou bou  621513 Sep 30 00:34 YPPD_10NM.csv
-rw-rw-r--. 1 bou bou 3455142 Sep 30 00:34 YPTN_10NM.csv
-rw-rw-r--. 1 bou bou    5749 Sep 30 00:34 YPCC_10NM.csv
-rw-rw-r--. 1 bou bou  125637 Sep 30 00:34 YPXM_10NM.csv


[bou@bous-fed31 data]$ pwd
/home/bou/shared/stats-R/flask_projects/avguide/app/data

Actualy we want gpats word in the filename - sp put it back

[bou@bous-fed31 data]$ ls -ltr *10NM.csv
-rw-rw-r--. 1 bou bou 1976777 Sep 29 23:19 YBBN_10NM.csv
-rw-rw-r--. 1 bou bou 1784216 Sep 29 23:20 YBSU_10NM.csv
-rw-rw-r--. 1 bou bou 1618918 Sep 29 23:20 YBCG_10NM.csv
-rw-rw-r--. 1 bou bou 2889035 Sep 29 23:20 YAMB_10NM.csv
-rw-rw-r--. 1 bou bou 2578036 Sep 29 23:21 YBAF_10NM.csv
-rw-rw-r--. 1 bou bou 2035526 Sep 29 23:21 YTWB_10NM.csv
-rw-rw-r--. 1 bou bou 2039678 Sep 29 23:21 YSSY_10NM.csv
-rw-rw-r--. 1 bou bou 2187647 Sep 29 23:21 YWOL_10NM.csv
-rw-rw-r--. 1 bou bou 2215055 Sep 29 23:22 YWLM_10NM.csv
-rw-rw-r--. 1 bou bou 2524571 Sep 29 23:22 YSRI_10NM.csv
-rw-rw-r--. 1 bou bou 2239958 Sep 29 23:47 YSBK_10NM.csv
-rw-rw-r--. 1 bou bou 2187920 Sep 29 23:48 YSCN_10NM.csv
-rw-rw-r--. 1 bou bou 2202361 Sep 29 23:48 YSHW_10NM.csv
-rw-rw-r--. 1 bou bou 5448840 Sep 29 23:48 YPDN_10NM.csv
-rw-rw-r--. 1 bou bou  944026 Sep 29 23:49 YBAS_10NM.csv
-rw-rw-r--. 1 bou bou  691421 Sep 29 23:49 YMML_10NM.csv
-rw-rw-r--. 1 bou bou  321252 Sep 29 23:49 YPPH_10NM.csv
-rw-rw-r--. 1 bou bou  353213 Sep 29 23:49 YPAD_10NM.csv
-rw-rw-r--. 1 bou bou 1169946 Sep 29 23:49 YSCB_10NM.csv
-rw-rw-r--. 1 bou bou 2461696 Sep 30 00:33 YBOK_10NM.csv
-rw-rw-r--. 1 bou bou 2224992 Sep 30 00:33 YBWW_10NM.csv
-rw-rw-r--. 1 bou bou 1146999 Sep 30 00:33 YBRK_10NM.csv
-rw-rw-r--. 1 bou bou 3610244 Sep 30 00:34 YBRM_10NM.csv
-rw-rw-r--. 1 bou bou  621513 Sep 30 00:34 YPPD_10NM.csv
-rw-rw-r--. 1 bou bou 3455142 Sep 30 00:34 YPTN_10NM.csv
-rw-rw-r--. 1 bou bou    5749 Sep 30 00:34 YPCC_10NM.csv
-rw-rw-r--. 1 bou bou  125637 Sep 30 00:34 YPXM_10NM.csv


[bou@bous-fed31 data]$ /usr/local/bin/rename 's/Y/gpats_Y/' *10NM.csv


[bou@bous-fed31 data]$ ls -ltr *10NM.csv
-rw-rw-r--. 1 bou bou 1976777 Sep 29 23:19 gpats_YBBN_10NM.csv
-rw-rw-r--. 1 bou bou 1784216 Sep 29 23:20 gpats_YBSU_10NM.csv
-rw-rw-r--. 1 bou bou 1618918 Sep 29 23:20 gpats_YBCG_10NM.csv
-rw-rw-r--. 1 bou bou 2889035 Sep 29 23:20 gpats_YAMB_10NM.csv
-rw-rw-r--. 1 bou bou 2578036 Sep 29 23:21 gpats_YBAF_10NM.csv
-rw-rw-r--. 1 bou bou 2035526 Sep 29 23:21 gpats_YTWB_10NM.csv
-rw-rw-r--. 1 bou bou 2039678 Sep 29 23:21 gpats_YSSY_10NM.csv
-rw-rw-r--. 1 bou bou 2187647 Sep 29 23:21 gpats_YWOL_10NM.csv
-rw-rw-r--. 1 bou bou 2215055 Sep 29 23:22 gpats_YWLM_10NM.csv
-rw-rw-r--. 1 bou bou 2524571 Sep 29 23:22 gpats_YSRI_10NM.csv
-rw-rw-r--. 1 bou bou 2239958 Sep 29 23:47 gpats_YSBK_10NM.csv
-rw-rw-r--. 1 bou bou 2187920 Sep 29 23:48 gpats_YSCN_10NM.csv
-rw-rw-r--. 1 bou bou 2202361 Sep 29 23:48 gpats_YSHW_10NM.csv
-rw-rw-r--. 1 bou bou 5448840 Sep 29 23:48 gpats_YPDN_10NM.csv
-rw-rw-r--. 1 bou bou  944026 Sep 29 23:49 gpats_YBAS_10NM.csv
-rw-rw-r--. 1 bou bou  691421 Sep 29 23:49 gpats_YMML_10NM.csv
-rw-rw-r--. 1 bou bou  321252 Sep 29 23:49 gpats_YPPH_10NM.csv
-rw-rw-r--. 1 bou bou  353213 Sep 29 23:49 gpats_YPAD_10NM.csv
-rw-rw-r--. 1 bou bou 1169946 Sep 29 23:49 gpats_YSCB_10NM.csv
-rw-rw-r--. 1 bou bou 2461696 Sep 30 00:33 gpats_YBOK_10NM.csv
-rw-rw-r--. 1 bou bou 2224992 Sep 30 00:33 gpats_YBWW_10NM.csv
-rw-rw-r--. 1 bou bou 1146999 Sep 30 00:33 gpats_YBRK_10NM.csv
-rw-rw-r--. 1 bou bou 3610244 Sep 30 00:34 gpats_YBRM_10NM.csv
-rw-rw-r--. 1 bou bou  621513 Sep 30 00:34 gpats_YPPD_10NM.csv
-rw-rw-r--. 1 bou bou 3455142 Sep 30 00:34 gpats_YPTN_10NM.csv
-rw-rw-r--. 1 bou bou    5749 Sep 30 00:34 gpats_YPCC_10NM.csv
-rw-rw-r--. 1 bou bou  125637 Sep 30 00:34 gpats_YPXM_10NM.csv




(py37) [bou@bous-fed31 data]$ ls -ltr /home/bou/shared/stats-R/fog-project/tcz_data/HM01X_99999999_9852610

-rw-r--r--. 1 bou bou     30691 Sep 21 04:58 HM01X_Notes_999999999852610.txt
-rw-r--r--. 1 bou bou 458277652 Sep 21 05:00 HM01X_Data_040004.txt
-rw-r--r--. 1 bou bou 520117451 Sep 21 05:02 HM01X_Data_040211.txt
-rw-r--r--. 1 bou bou 513836663 Sep 21 05:04 HM01X_Data_040842.txt
-rw-r--r--. 1 bou bou 530291278 Sep 21 05:07 HM01X_Data_040717.txt
-rw-r--r--. 1 bou bou 538421225 Sep 21 05:09 HM01X_Data_040861.txt
-rw-r--r--. 1 bou bou 450096608 Sep 21 05:11 HM01X_Data_041359.txt
-rw-r--r--. 1 bou bou 448918615 Sep 21 05:13 HM01X_Data_041529.txt
-rw-r--r--. 1 bou bou 524245260 Sep 21 05:16 HM01X_Data_039083.txt
-rw-r--r--. 1 bou bou 519501525 Sep 21 05:18 HM01X_Data_066037.txt
-rw-r--r--. 1 bou bou 534907961 Sep 21 05:21 HM01X_Data_061078.txt
-rw-r--r--. 1 bou bou 545682523 Sep 21 05:23 HM01X_Data_067105.txt
-rw-r--r--. 1 bou bou 528745939 Sep 21 05:26 HM01X_Data_066137.txt
-rw-r--r--. 1 bou bou 511857690 Sep 21 05:28 HM01X_Data_068192.txt
-rw-r--r--. 1 bou bou 456722646 Sep 21 05:30 HM01X_Data_068241.txt
-rw-r--r--. 1 bou bou 523235749 Sep 21 05:33 HM01X_Data_014015.txt
-rw-r--r--. 1 bou bou 516265842 Sep 21 05:35 HM01X_Data_015590.txt
-rw-r--r--. 1 bou bou 515531150 Sep 21 05:38 HM01X_Data_003003.txt
-rw-r--r--. 1 bou bou 549146071 Sep 21 05:40 HM01X_Data_004032.txt
-rw-r--r--. 1 bou bou 406702826 Sep 21 05:42 HM01X_Data_200790.txt
-rw-r--r--. 1 bou bou 454200940 Sep 21 05:44 HM01X_Data_200284.txt
-rw-r--r--. 1 bou bou      3775 Sep 21 05:46 HM01X_StnDet_999999999852610.txt
-rw-r--r--. 1 bou bou 509768237 Sep 21 05:46 HM01X_Data_014932_YPTN.txt


Rename climate zone data files files using linux rename and regex pattern matching

Lets apend av_id to each of the tcz file - that way we can process them all automaticall y at once

This would replace all files with 5 digits in file name to bou
[bou@bous-fed31 HM01X_99999999_9852610]$ /usr/local/bin/rename -n 's/HM01X_Data_\d{5}/bou/' *

[bou@bous-fed31 HM01X_99999999_9852610]$ /usr/local/bin/rename -n 's/HM01X_Data_\d{5}/bou/' *
rename(HM01X_Data_003003.txt, bou3.txt)
rename(HM01X_Data_004032.txt, bou2.txt)
rename(HM01X_Data_014015.txt, bou5.txt)

This greps the av_id

[bou@bous-fed31 HM01X_99999999_9852610]$ sed -n '2 p' HM01X_Data_200284.txt |awk -F , '{ print $6 }'
YPCC  

and we need 

/usr/local/bin/rename 's/[/d]5/[/d]5\$1/' *


https://www.thetopsites.net/article/50866409.shtml
https://stackoverflow.com/questions/34823263/how-to-pass-a-variable-to-the-mv-command-to-rename-a-file-text-with-spaces-and-t
https://linuxhint.com/trim_string_bash/

https://linuxhint.com/

https://linuxconfig.org/how-to-extract-a-number-from-a-string-using-bash-example

[bou@bous-fed31 HM01X_99999999_9852610]$ sta=$(sed -n '2 p' HM01X_Data_200284.txt |awk -F , '{ print $6 }')

[bou@bous-fed31 HM01X_99999999_9852610]$ echo $sta
YPCC

Note this leave some trailing whitespaces which we can remove usig se before saving variable

sta=$(sed -n '2 p' HM01X_Data_200284.txt |awk -F , '{ print $6 }'|sed 's/ *$//g')


[bou@bous-fed31 HM01X_99999999_9852610]$ mv HM01X_Data_12345.txt "HM01X_Data_12345_${sta}.txt"

[bou@bous-fed31 HM01X_99999999_9852610]$ ls -ltr HM01X_Data_12345*
-rw-r--r--. 1 bou bou 454200940 Oct  2 11:34  HM01X_Data_12345.txt
-rw-r--r--. 1 bou bou 454200940 Oct  2 11:40 'HM01X_Data_12345_YPCC  .txt'
-rw-r--r--. 1 bou bou 454200940 Oct  2 11:47  HM01X_Data_12345_YPCC.txt   <<----------FIXED


https://linuxize.com/post/bash-for-loop/

for file in ./HM01X_Data*txt; do 
    sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g')
    num=$(echo $file|sed 's/[0-9]*//g')
    echo Rename $file using station id $sta and number $num
    cp $file "HM01X_Data_$(num)_${sta}.txt"
done

[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do 
> sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g')
> num=$(echo $file|sed 's/[^0-9]*//g')
> echo Rename $file using station id $sta and number $num
> done
Rename ./HM01X_Data_12345.txt using station id YPCC and number 0112345
Rename ./HM01X_Data_12346.txt using station id YPCC and number 0112346


It is adding the 01 digits from HM01X to file name as well because [^0-9]*  grabing digits from start $
chnge to $ to grab from end


[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do
  sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g');
  num=$(echo $file|sed 's/[0-9$]*//g');
  echo Rename $file using station id $sta and number $num;
  done
Rename ./HM01X_Data_12345.txt using station id YPCC and number ./HMX_Data_.txt
Rename ./HM01X_Data_12346.txt using station id YPCC and number ./HMX_Data_.txt

Nah
remove greedy

[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do 
 sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g');
 num=$(echo $file|sed 's/[^0-9]*//');
 echo Rename $file using station id $sta and number $num; done
Rename ./HM01X_Data_12345.txt using station id YPCC and number 01X_Data_12345.txt
Rename ./HM01X_Data_12346.txt using station id YPCC and number 01X_Data_12346.txt


$ for from end
[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do
  sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g');
 num=$(echo $file|sed 's/[$0-9]*//');
 echo Rename $file using station id $sta and number $num; done
Rename ./HM01X_Data_12345.txt using station id YPCC and number ./HM01X_Data_12345.txt
Rename ./HM01X_Data_12346.txt using station id YPCC and number ./HM01X_Data_12346.txt


num=$(echo $file|sed 's/[0-9$]*//')  same whether put $ front or end
num=$(echo $file|sed 's/\d*//')      replace [0-9] with short form \d



[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do
  sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g');
 num=$(echo $file|sed 's/[$0-9]*//g');
 echo Rename $file using station id $sta and number $num; done
Rename ./HM01X_Data_12345.txt using station id YPCC and number ./HMX_Data_.txt
Rename ./HM01X_Data_12346.txt using station id YPCC and number ./HMX_Data_.txt



[bou@bous-fed31 what]$ NUMBER=$(echo "I am 999 years old born 1972."| grep -o -E '[0-9]+')
[bou@bous-fed31 what]$ echo $NUMBER
999 1972


[bou@bous-fed31 what]$ NUMBER=$(echo "I am 999 years old born 1972."| grep -o -E '[0-9]{4}')
[bou@bous-fed31 what]$ echo $NUMBER
1972

great


[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g'); num=$(echo $file|grep -o -E '[0-9]{5}'); echo Rename $file using station id $sta and number $num; done
Rename ./HM01X_Data_12345.txt using station id YPCC and number 12345
Rename ./HM01X_Data_12346.txt using station id YPCC and number 12346


[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do
  sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g')
  num=$(grep -o -E '[0-9]{5}' $file)
  echo Rename $file using station id $sta and number $num
  cp $file "HM01X_Data_$(num)_${sta}.txt"
  done

[bou@bous-fed31 what]$ ls -ltr
total 1330672
-rw-r--r--. 1 bou bou 454200940 Oct  2 11:34 HM01X_Data_12345.txt
-rw-r--r--. 1 bou bou 454200940 Oct  2 11:52 HM01X_Data_12346.txt
-rw-r--r--. 1 bou bou 454200940 Oct  2 12:39 HM01X_Data__YPCC.txt
[bou@bous-fed31 what]$ 



does not look likes it workig 
lets do manula rename and move on



[bou@bous-fed31 what]$ for file in ./HM01X_Data*txt; do
  sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g')
  num=$(grep -o -E '[0-9]{5}' $file)
  echo Rename $file using station id $sta and number $num
  cp $file "HM01X_Data_${sta}.txt"
  done

this works

[bou@bous-fed31 HM01X_99999999_9852610]$ ls -ltr ./HM01X_Data*txt
-rw-r--r--. 1 bou bou 458277652 Sep 21 05:00 ./HM01X_Data_040004.txt
-rw-r--r--. 1 bou bou 520117451 Sep 21 05:02 ./HM01X_Data_040211.txt
-rw-r--r--. 1 bou bou 513836663 Sep 21 05:04 ./HM01X_Data_040842.txt
-rw-r--r--. 1 bou bou 530291278 Sep 21 05:07 ./HM01X_Data_040717.txt
-rw-r--r--. 1 bou bou 538421225 Sep 21 05:09 ./HM01X_Data_040861.txt
-rw-r--r--. 1 bou bou 450096608 Sep 21 05:11 ./HM01X_Data_041359.txt
-rw-r--r--. 1 bou bou 448918615 Sep 21 05:13 ./HM01X_Data_041529.txt
-rw-r--r--. 1 bou bou 524245260 Sep 21 05:16 ./HM01X_Data_039083.txt
-rw-r--r--. 1 bou bou 519501525 Sep 21 05:18 ./HM01X_Data_066037.txt
-rw-r--r--. 1 bou bou 534907961 Sep 21 05:21 ./HM01X_Data_061078.txt
-rw-r--r--. 1 bou bou 545682523 Sep 21 05:23 ./HM01X_Data_067105.txt
-rw-r--r--. 1 bou bou 528745939 Sep 21 05:26 ./HM01X_Data_066137.txt
-rw-r--r--. 1 bou bou 511857690 Sep 21 05:28 ./HM01X_Data_068192.txt
-rw-r--r--. 1 bou bou 456722646 Sep 21 05:30 ./HM01X_Data_068241.txt
-rw-r--r--. 1 bou bou 523235749 Sep 21 05:33 ./HM01X_Data_014015.txt
-rw-r--r--. 1 bou bou 516265842 Sep 21 05:35 ./HM01X_Data_015590.txt
-rw-r--r--. 1 bou bou 515531150 Sep 21 05:38 ./HM01X_Data_003003.txt
-rw-r--r--. 1 bou bou 549146071 Sep 21 05:40 ./HM01X_Data_004032.txt
-rw-r--r--. 1 bou bou 406702826 Sep 21 05:42 ./HM01X_Data_200790.txt
-rw-r--r--. 1 bou bou 454200940 Sep 21 05:44 ./HM01X_Data_200284.txt
-rw-r--r--. 1 bou bou 509768237 Sep 21 05:46 ./HM01X_Data_014932.txt

[bou@bous-fed31 HM01X_99999999_9852610]$ for file in ./HM01X_Data*txt; do
> sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g')
> echo Rename $file using station id $sta 
> cp $file "HM01X_Data_${sta}.txt"
> done

Rename ./HM01X_Data_003003.txt using station id YBRM
Rename ./HM01X_Data_004032.txt using station id YPPD
Rename ./HM01X_Data_014015.txt using station id YPDN
Rename ./HM01X_Data_014932.txt using station id YPTN
Rename ./HM01X_Data_015590.txt using station id YBAS
Rename ./HM01X_Data_039083.txt using station id YBRK
Rename ./HM01X_Data_040004.txt using station id YAMB
Rename ./HM01X_Data_040211.txt using station id YBAF
Rename ./HM01X_Data_040717.txt using station id YBCG
Rename ./HM01X_Data_040842.txt using station id YBBN
Rename ./HM01X_Data_040861.txt using station id YBSU
Rename ./HM01X_Data_041359.txt using station id YBOK
Rename ./HM01X_Data_041529.txt using station id YTWB
Rename ./HM01X_Data_061078.txt using station id YWLM
Rename ./HM01X_Data_066037.txt using station id YSSY
Rename ./HM01X_Data_066137.txt using station id YSBK
Rename ./HM01X_Data_067105.txt using station id YSRI
Rename ./HM01X_Data_068192.txt using station id YSCN
Rename ./HM01X_Data_068241.txt using station id YSHL
Rename ./HM01X_Data_200284.txt using station id YPCC
Rename ./HM01X_Data_200790.txt using station id YPXM

[bou@bous-fed31 HM01X_99999999_9852610]$ ls -ltr ./HM01X_Data*txt
-rw-r--r--. 1 bou bou 458277652 Sep 21 05:00 ./HM01X_Data_040004.txt
-rw-r--r--. 1 bou bou 520117451 Sep 21 05:02 ./HM01X_Data_040211.txt
-rw-r--r--. 1 bou bou 513836663 Sep 21 05:04 ./HM01X_Data_040842.txt
-rw-r--r--. 1 bou bou 530291278 Sep 21 05:07 ./HM01X_Data_040717.txt
-rw-r--r--. 1 bou bou 538421225 Sep 21 05:09 ./HM01X_Data_040861.txt
-rw-r--r--. 1 bou bou 450096608 Sep 21 05:11 ./HM01X_Data_041359.txt
-rw-r--r--. 1 bou bou 448918615 Sep 21 05:13 ./HM01X_Data_041529.txt
-rw-r--r--. 1 bou bou 524245260 Sep 21 05:16 ./HM01X_Data_039083.txt
-rw-r--r--. 1 bou bou 519501525 Sep 21 05:18 ./HM01X_Data_066037.txt
-rw-r--r--. 1 bou bou 534907961 Sep 21 05:21 ./HM01X_Data_061078.txt
-rw-r--r--. 1 bou bou 545682523 Sep 21 05:23 ./HM01X_Data_067105.txt
-rw-r--r--. 1 bou bou 528745939 Sep 21 05:26 ./HM01X_Data_066137.txt
-rw-r--r--. 1 bou bou 511857690 Sep 21 05:28 ./HM01X_Data_068192.txt
-rw-r--r--. 1 bou bou 456722646 Sep 21 05:30 ./HM01X_Data_068241.txt
-rw-r--r--. 1 bou bou 523235749 Sep 21 05:33 ./HM01X_Data_014015.txt
-rw-r--r--. 1 bou bou 516265842 Sep 21 05:35 ./HM01X_Data_015590.txt
-rw-r--r--. 1 bou bou 515531150 Sep 21 05:38 ./HM01X_Data_003003.txt
-rw-r--r--. 1 bou bou 549146071 Sep 21 05:40 ./HM01X_Data_004032.txt
-rw-r--r--. 1 bou bou 406702826 Sep 21 05:42 ./HM01X_Data_200790.txt
-rw-r--r--. 1 bou bou 454200940 Sep 21 05:44 ./HM01X_Data_200284.txt
-rw-r--r--. 1 bou bou 509768237 Sep 21 05:46 ./HM01X_Data_014932.txt
-rw-r--r--. 1 bou bou 515531150 Oct  2 12:55 ./HM01X_Data_YBRM.txt
-rw-r--r--. 1 bou bou 549146071 Oct  2 12:55 ./HM01X_Data_YPPD.txt
-rw-r--r--. 1 bou bou 523235749 Oct  2 12:55 ./HM01X_Data_YPDN.txt
-rw-r--r--. 1 bou bou 509768237 Oct  2 12:55 ./HM01X_Data_YPTN.txt
-rw-r--r--. 1 bou bou 516265842 Oct  2 12:55 ./HM01X_Data_YBAS.txt
-rw-r--r--. 1 bou bou 524245260 Oct  2 12:55 ./HM01X_Data_YBRK.txt
-rw-r--r--. 1 bou bou 458277652 Oct  2 12:55 ./HM01X_Data_YAMB.txt
-rw-r--r--. 1 bou bou 520117451 Oct  2 12:55 ./HM01X_Data_YBAF.txt
-rw-r--r--. 1 bou bou 530291278 Oct  2 12:55 ./HM01X_Data_YBCG.txt
-rw-r--r--. 1 bou bou 513836663 Oct  2 12:55 ./HM01X_Data_YBBN.txt
-rw-r--r--. 1 bou bou 538421225 Oct  2 12:55 ./HM01X_Data_YBSU.txt
-rw-r--r--. 1 bou bou 450096608 Oct  2 12:55 ./HM01X_Data_YBOK.txt
-rw-r--r--. 1 bou bou 448918615 Oct  2 12:55 ./HM01X_Data_YTWB.txt
-rw-r--r--. 1 bou bou 534907961 Oct  2 12:55 ./HM01X_Data_YWLM.txt
-rw-r--r--. 1 bou bou 519501525 Oct  2 12:55 ./HM01X_Data_YSSY.txt
-rw-r--r--. 1 bou bou 528745939 Oct  2 12:55 ./HM01X_Data_YSBK.txt
-rw-r--r--. 1 bou bou 545682523 Oct  2 12:55 ./HM01X_Data_YSRI.txt
-rw-r--r--. 1 bou bou 511857690 Oct  2 12:55 ./HM01X_Data_YSCN.txt
-rw-r--r--. 1 bou bou 456722646 Oct  2 12:55 ./HM01X_Data_YSHL.txt
-rw-r--r--. 1 bou bou 454200940 Oct  2 12:55 ./HM01X_Data_YPCC.txt
-rw-r--r--. 1 bou bou 406702826 Oct  2 12:55 ./HM01X_Data_YPXM.txt

==================================================================
Do same at work machine 
[vinorda@qld-rfc-ws45 .ssh]$ cd ~/../qdisplay/avguide

(av_env) [vinorda@qld-rfc-ws45 tmp]$ ls -ltr
-rw-rw-r--. 1 vinorda vinorda 765969699 Sep 21 07:04 HM01X_99999999_9852610.zip
drwxrwxr-x. 2 vinorda vinorda      4096 Sep 30 00:34 gpats
drwxrwxr-x. 2 vinorda vinorda      4096 Oct  5 17:56 HM01X_99999999_9852610

(av_env) [vinorda@qld-rfc-ws45 tmp]$ cd HM01X_99999999_9852610

(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ ls -ltr
total 10309180
-rw-r--r--. 1 vinorda vinorda     30691 Sep 21 04:58 HM01X_Notes_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 458277652 Sep 21 05:00 HM01X_Data_040004_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 520117451 Sep 21 05:02 HM01X_Data_040211_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 513836663 Sep 21 05:04 HM01X_Data_040842_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 530291278 Sep 21 05:07 HM01X_Data_040717_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 538421225 Sep 21 05:09 HM01X_Data_040861_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 450096608 Sep 21 05:11 HM01X_Data_041359_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 448918615 Sep 21 05:13 HM01X_Data_041529_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 524245260 Sep 21 05:16 HM01X_Data_039083_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 519501525 Sep 21 05:18 HM01X_Data_066037_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 534907961 Sep 21 05:21 HM01X_Data_061078_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 545682523 Sep 21 05:23 HM01X_Data_067105_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 528745939 Sep 21 05:26 HM01X_Data_066137_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 511857690 Sep 21 05:28 HM01X_Data_068192_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 456722646 Sep 21 05:30 HM01X_Data_068241_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 523235749 Sep 21 05:33 HM01X_Data_014015_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 516265842 Sep 21 05:35 HM01X_Data_015590_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 515531150 Sep 21 05:38 HM01X_Data_003003_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 549146071 Sep 21 05:40 HM01X_Data_004032_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 406702826 Sep 21 05:42 HM01X_Data_200790_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 454200940 Sep 21 05:44 HM01X_Data_200284_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda      3775 Sep 21 05:46 HM01X_StnDet_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 509768237 Sep 21 05:46 HM01X_Data_014932_999999999852610.txt

(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ for file in ./HM01X_Data*txt; do
> sta=$(sed -n '2 p' $file |awk -F , '{ print $6 }'|sed 's/ *$//g')
> echo Rename $file using station id $sta
> cp $file "HM01X_Data_${sta}.txt"
> done

Rename ./HM01X_Data_003003_999999999852610.txt using station id YBRM
Rename ./HM01X_Data_004032_999999999852610.txt using station id YPPD
Rename ./HM01X_Data_014015_999999999852610.txt using station id YPDN
Rename ./HM01X_Data_014932_999999999852610.txt using station id YPTN
Rename ./HM01X_Data_015590_999999999852610.txt using station id YBAS
Rename ./HM01X_Data_039083_999999999852610.txt using station id YBRK
Rename ./HM01X_Data_040004_999999999852610.txt using station id YAMB
Rename ./HM01X_Data_040211_999999999852610.txt using station id YBAF
Rename ./HM01X_Data_040717_999999999852610.txt using station id YBCG
Rename ./HM01X_Data_040842_999999999852610.txt using station id YBBN
Rename ./HM01X_Data_040861_999999999852610.txt using station id YBSU
Rename ./HM01X_Data_041359_999999999852610.txt using station id YBOK
Rename ./HM01X_Data_041529_999999999852610.txt using station id YTWB
Rename ./HM01X_Data_061078_999999999852610.txt using station id YWLM
Rename ./HM01X_Data_066037_999999999852610.txt using station id YSSY
Rename ./HM01X_Data_066137_999999999852610.txt using station id YSBK
Rename ./HM01X_Data_067105_999999999852610.txt using station id YSRI
Rename ./HM01X_Data_068192_999999999852610.txt using station id YSCN
Rename ./HM01X_Data_068241_999999999852610.txt using station id YSHL
Rename ./HM01X_Data_200284_999999999852610.txt using station id YPCC
Rename ./HM01X_Data_200790_999999999852610.txt using station id YPXM


(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ ls -ltr
total 20618336
-rw-r--r--. 1 vinorda vinorda     30691 Sep 21 04:58 HM01X_Notes_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 458277652 Sep 21 05:00 HM01X_Data_040004_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 520117451 Sep 21 05:02 HM01X_Data_040211_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 513836663 Sep 21 05:04 HM01X_Data_040842_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 530291278 Sep 21 05:07 HM01X_Data_040717_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 538421225 Sep 21 05:09 HM01X_Data_040861_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 450096608 Sep 21 05:11 HM01X_Data_041359_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 448918615 Sep 21 05:13 HM01X_Data_041529_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 524245260 Sep 21 05:16 HM01X_Data_039083_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 519501525 Sep 21 05:18 HM01X_Data_066037_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 534907961 Sep 21 05:21 HM01X_Data_061078_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 545682523 Sep 21 05:23 HM01X_Data_067105_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 528745939 Sep 21 05:26 HM01X_Data_066137_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 511857690 Sep 21 05:28 HM01X_Data_068192_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 456722646 Sep 21 05:30 HM01X_Data_068241_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 523235749 Sep 21 05:33 HM01X_Data_014015_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 516265842 Sep 21 05:35 HM01X_Data_015590_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 515531150 Sep 21 05:38 HM01X_Data_003003_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 549146071 Sep 21 05:40 HM01X_Data_004032_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 406702826 Sep 21 05:42 HM01X_Data_200790_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 454200940 Sep 21 05:44 HM01X_Data_200284_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda      3775 Sep 21 05:46 HM01X_StnDet_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 509768237 Sep 21 05:46 HM01X_Data_014932_999999999852610.txt
-rw-r--r--. 1 vinorda vinorda 515531150 Oct  5 18:00 HM01X_Data_YBRM.txt
-rw-r--r--. 1 vinorda vinorda 549146071 Oct  5 18:00 HM01X_Data_YPPD.txt
-rw-r--r--. 1 vinorda vinorda 523235749 Oct  5 18:01 HM01X_Data_YPDN.txt
-rw-r--r--. 1 vinorda vinorda 509768237 Oct  5 18:01 HM01X_Data_YPTN.txt
-rw-r--r--. 1 vinorda vinorda 516265842 Oct  5 18:01 HM01X_Data_YBAS.txt
-rw-r--r--. 1 vinorda vinorda 524245260 Oct  5 18:01 HM01X_Data_YBRK.txt
-rw-r--r--. 1 vinorda vinorda 458277652 Oct  5 18:01 HM01X_Data_YAMB.txt
-rw-r--r--. 1 vinorda vinorda 520117451 Oct  5 18:01 HM01X_Data_YBAF.txt
-rw-r--r--. 1 vinorda vinorda 530291278 Oct  5 18:02 HM01X_Data_YBCG.txt
-rw-r--r--. 1 vinorda vinorda 513836663 Oct  5 18:02 HM01X_Data_YBBN.txt
-rw-r--r--. 1 vinorda vinorda 538421225 Oct  5 18:02 HM01X_Data_YBSU.txt
-rw-r--r--. 1 vinorda vinorda 450096608 Oct  5 18:02 HM01X_Data_YBOK.txt
-rw-r--r--. 1 vinorda vinorda 448918615 Oct  5 18:02 HM01X_Data_YTWB.txt
-rw-r--r--. 1 vinorda vinorda 534907961 Oct  5 18:03 HM01X_Data_YWLM.txt
-rw-r--r--. 1 vinorda vinorda 519501525 Oct  5 18:03 HM01X_Data_YSSY.txt
-rw-r--r--. 1 vinorda vinorda 528745939 Oct  5 18:03 HM01X_Data_YSBK.txt
-rw-r--r--. 1 vinorda vinorda 545682523 Oct  5 18:03 HM01X_Data_YSRI.txt
-rw-r--r--. 1 vinorda vinorda 511857690 Oct  5 18:03 HM01X_Data_YSCN.txt
-rw-r--r--. 1 vinorda vinorda 456722646 Oct  5 18:03 HM01X_Data_YSHL.txt
-rw-r--r--. 1 vinorda vinorda 454200940 Oct  5 18:04 HM01X_Data_YPCC.txt
-rw-r--r--. 1 vinorda vinorda 406702826 Oct  5 18:04 HM01X_Data_YPXM.txt

(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ rm *2610.txt --exclude HM01X_StnDet* HM01X_Notes*
rm: unrecognized option '--exclude'
Try `rm --help' for more information.
(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ rm -exclude HM01X_StnDet* HM01X_Notes* *2610.txt
rm: invalid option -- 'e'
Try `rm --help' for more information.
(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ rm --exclude HM01X_StnDet* HM01X_Notes* *2610.txt
rm: unrecognized option '--exclude'
Try `rm --help' for more information.
(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ rm -f *2610.txt
(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ 


(av_env) [vinorda@qld-rfc-ws45 HM01X_99999999_9852610]$ ls -ltr
total 10309156
-rw-r--r--. 1 vinorda vinorda 515531150 Oct  5 18:00 HM01X_Data_YBRM.txt
-rw-r--r--. 1 vinorda vinorda 549146071 Oct  5 18:00 HM01X_Data_YPPD.txt
-rw-r--r--. 1 vinorda vinorda 523235749 Oct  5 18:01 HM01X_Data_YPDN.txt
-rw-r--r--. 1 vinorda vinorda 509768237 Oct  5 18:01 HM01X_Data_YPTN.txt
-rw-r--r--. 1 vinorda vinorda 516265842 Oct  5 18:01 HM01X_Data_YBAS.txt
-rw-r--r--. 1 vinorda vinorda 524245260 Oct  5 18:01 HM01X_Data_YBRK.txt
-rw-r--r--. 1 vinorda vinorda 458277652 Oct  5 18:01 HM01X_Data_YAMB.txt
-rw-r--r--. 1 vinorda vinorda 520117451 Oct  5 18:01 HM01X_Data_YBAF.txt
-rw-r--r--. 1 vinorda vinorda 530291278 Oct  5 18:02 HM01X_Data_YBCG.txt
-rw-r--r--. 1 vinorda vinorda 513836663 Oct  5 18:02 HM01X_Data_YBBN.txt
-rw-r--r--. 1 vinorda vinorda 538421225 Oct  5 18:02 HM01X_Data_YBSU.txt
-rw-r--r--. 1 vinorda vinorda 450096608 Oct  5 18:02 HM01X_Data_YBOK.txt
-rw-r--r--. 1 vinorda vinorda 448918615 Oct  5 18:02 HM01X_Data_YTWB.txt
-rw-r--r--. 1 vinorda vinorda 534907961 Oct  5 18:03 HM01X_Data_YWLM.txt
-rw-r--r--. 1 vinorda vinorda 519501525 Oct  5 18:03 HM01X_Data_YSSY.txt
-rw-r--r--. 1 vinorda vinorda 528745939 Oct  5 18:03 HM01X_Data_YSBK.txt
-rw-r--r--. 1 vinorda vinorda 545682523 Oct  5 18:03 HM01X_Data_YSRI.txt
-rw-r--r--. 1 vinorda vinorda 511857690 Oct  5 18:03 HM01X_Data_YSCN.txt
-rw-r--r--. 1 vinorda vinorda 456722646 Oct  5 18:03 HM01X_Data_YSHL.txt
-rw-r--r--. 1 vinorda vinorda 454200940 Oct  5 18:04 HM01X_Data_YPCC.txt
-rw-r--r--. 1 vinorda vinorda 406702826 Oct  5 18:04 HM01X_Data_YPXM.txt



then from inside python do this to get all aws files

>>> for sta in stations:
...     bous.merge_aws_gpats_data(sta)

(av_env) [vinorda@qld-rfc-ws45 ~]$ pwd
/home/accounts/vinorda
(av_env) [vinorda@qld-rfc-ws45 ~]$ cd ../qdisplay/avguide
(av_env) [vinorda@qld-rfc-ws45 avguide]$ python
Python 3.6.8 (default, May  5 2019, 11:59:42) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-23)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import utility_functions_sep2018 as bous
>>> stations=['YBBN','YBAF','YAMB','YBSU','YBCG','YTWB','YBOK',\
...                       'YBRK','YSHL','YSCN','YSRI','YSBK','YSSY','YWLM',\
...                       'YBAS','YPTN','YPDN','YPPD','YBRM','YPCC','YPXM']

>>> import importlib
>>> importlib.reload(bous)
<module 'utility_functions_sep2018' from '/home/accounts/qdisplay/avguide/utility_functions_sep2018.py'>

>>> for sta in stations:
...     bous.merge_aws_gpats_data(sta)
... 

Processing sta=YBBN,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBBN.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YBBN_10NM.csv

Processing aviation location YBBN, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBBN.txt
Reading YBBN gpats file /home/accounts/qdisplay/avguide/tmp/gpats_YBBN_10NM.csv
Getting storm dates for: YBBN from aws metar/speci and gpats data.
/home/accounts/qdisplay/avguide/utility_functions_sep2018.py:685: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version
  daily = gpats.resample('D').apply(aggregator).dropna()
Storm dates derived from gpats data for YBBN = 323
Storm dates derived from observer reported PW groups for YBBN = 481
Total TS dates found for YBBN = 617

Processing sta=YBAF,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBAF.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YBAF_10NM.csv

Processing aviation location YBAF, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBAF.txt
Reading YBAF gpats file /home/accounts/qdisplay/avguide/tmp/gpats_YBAF_10NM.csv
Getting storm dates for: YBAF from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBAF = 339
Total TS dates found for YBAF = 339

Processing sta=YAMB,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YAMB.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YAMB_10NM.csv

Processing aviation location YAMB, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YAMB.txt
Reading YAMB gpats file /home/accounts/qdisplay/avguide/tmp/gpats_YAMB_10NM.csv
Getting storm dates for: YAMB from aws metar/speci and gpats data.
Storm dates derived from gpats data for YAMB = 376
Storm dates derived from observer reported PW groups for YAMB = 284
Total TS dates found for YAMB = 554

Processing sta=YBSU,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBSU.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YBSU_10NM.csv

Processing aviation location YBSU, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBSU.txt
Reading YBSU gpats file /home/accounts/qdisplay/avguide/tmp/gpats_YBSU_10NM.csv
Getting storm dates for: YBSU from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBSU = 296
Total TS dates found for YBSU = 296

Processing sta=YBCG,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBCG.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YBCG_10NM.csv

Processing aviation location YBCG, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YBCG.txt
Reading YBCG gpats file /home/accounts/qdisplay/avguide/tmp/gpats_YBCG_10NM.csv
Getting storm dates for: YBCG from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBCG = 331
Total TS dates found for YBCG = 331

Processing sta=YTWB,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YTWB.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YTWB_10NM.csv

Processing aviation location YTWB, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YTWB.txt

crashed for YSHL                 cause its called YWOL in gpats file I think

fix and run only for YSHL again



for sta in ['YSHL']:
...     bous.merge_aws_gpats_data(sta)

Processing sta=YSHL,
Climate Zone data file:/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YSHL.txt",        
Gpats file: /home/accounts/qdisplay/avguide/tmp/gpats_YSHL_10NM.csv

Processing aviation location YSHL, file=/home/accounts/qdisplay/avguide/tmp/HM01X_Data_YSHL.txt
Reading YSHL gpats file /home/accounts/qdisplay/avguide/tmp/gpats_YSHL_10NM.csv
Getting storm dates for: YSHL from aws metar/speci and gpats data.
/home/accounts/qdisplay/avguide/utility_functions_sep2018.py:685: FutureWarning: using a dict with renaming is deprecated and will be removed in a future version
  daily = gpats.resample('D').apply(aggregator).dropna()
Storm dates derived from gpats data for YSHL = 347
Total TS dates found for YSHL = 347






Now need to download and process Sydney sonde files

def batch_download_F160_hanks(station:str="040842",start_date='None',end_date='None',hour='1700',exact='on'):

    # print("Start date",start_date)
    # if no date supplied grab todays sonde file
    cur_dir='/home/accounts/vinorda/Downloads/'
    if start_date == 'None':
        start_date = pd.datetime.today().strftime("%Y-%m-%d")
        end_date = start_date

    f160_url = 'http://aifs-sa.bom.gov.au/cgi-bin/extract_skewt_from_adam.pl'
    #dates = pd.date_range(start='2000-Feb-01', end='2018-Feb-13', freq='D')
    dates = pd.date_range(start=start_date, end=end_date, freq='D')
    print("Fetching sonde data for {} from {} to {}"\
        .format(station,start_date, end_date))

    for day in dates:
        day, month, year = day.day,day.month,day.year
        payload = {'d':day,'m':month,'y':year,'h':hour,'stn': station,'exact':exact,\
            'plot':'go','prev':'None','ascii':'on','ascii2':'off','reverse':'','crap':9558}
        # print(payload['stn'])
        # print(day.day,day.month,day.year)
        '''?plot=go&d=1&m=1&y=2010&h=2300&stn=040842&prev=None&exact=&ascii=on&ascii2=off&reverse=&crap=9558'''

        f160_response = requests.get(f160_url, params=payload)

        print (f160_response.url)  #check if url formed correctly

        if (f160_response.status_code == requests.codes.ok):
            '''
            print ("Found file resource")
            print (f160_response.headers.get('content-type'),"\n")'''

            '''build file name as string then save response file '''
            f160_file = cur_dir+'f160_'+str(hour)+'/stn'+str(payload['stn'])+'_'+\
            str(year)+'-'+str(month)+'-'+str(day)+'.txt'
            with open(f160_file, 'wb') as f:
                f.write(f160_response.content)



/home/accounts/vinorda/Downloads

drwxrwxr-x. 2 vinorda vinorda   352256 Oct  8 12:53 f160_0200
drwxrwxr-x. 2 vinorda vinorda   352256 Oct  8 15:41 f160_0300

cd f160_0300

 for file in *txt; do
  [[ `diff ${HOME}/file1 ${HOME}/file2` ]] &&  
   (echo "files different") ||
   (echo "files same")


https://stackoverflow.com/questions/5369368/compare-all-file-sizes-of-2-directories-in-bash

[vinorda@qld-ams-ws01 Downloads]$ stat ./f160_0200/stn066037_2020-2-2.txt
  File: `./f160_0200/stn066037_2020-2-2.txt'
  Size: 3519      	Blocks: 8          IO Block: 262144 regular file
Device: 18h/24d	Inode: 1539483     Links: 1
Access: (0664/-rw-rw-r--)  Uid: (12308/ vinorda)   Gid: (12308/ vinorda)
Access: 2020-10-08 12:36:26.129894794 +1000
Modify: 2020-10-08 12:36:26.130894801 +1000
Change: 2020-10-08 12:36:26.130894801 +1000


Just get the size

[vinorda@qld-ams-ws01 Downloads]$ stat -c%s ./f160_0200/stn066037_2020-2-2.txt
3519



#######################################
source=./f160_0200n
target=./f160_0300n


for i in "$source"/*
do
 f1=`stat -c%s $i`
 f2=`stat -c%s $target/${i##*/}`
  if [ "$f1" > "$f2" ]; then
        echo "$i" "$f1" VS "$target/${i##*/}" "$f1" "is larger" "Copy to target  cp f1 f2"
        cp  "$i" $target/${i##*/}
  else
        echo "$i" "$f1" VS "$target/${i##*/}" " We got larger file in target - do nothing"
  fi
done  
#######################################


Test 

[vinorda@qld-ams-ws01 f160_0200n]$ ls -ltr
-rw-rw-r--. 1 vinorda vinorda  792 Oct  8 11:20 stn066037_2001-4-21.txt
-rw-rw-r--. 1 vinorda vinorda 1052 Oct  8 11:34 stn066037_2004-8-12.txt
-rw-rw-r--. 1 vinorda vinorda  412 Oct  8 11:39 stn066037_2005-11-23.txt
-rw-rw-r--. 1 vinorda vinorda 1133 Oct  8 12:12 stn066037_2014-3-16.txt
-rw-rw-r--. 1 vinorda vinorda  216 Oct  8 12:32 stn066037_2019-1-31.txt
-rw-rw-r--. 1 vinorda vinorda 1471 Oct  8 12:36 stn066037_2020-1-20.txt

[vinorda@qld-ams-ws01 f160_0300n]$ ls -ltr 
-rw-rw-r--. 1 vinorda vinorda 1052 Oct  8 17:55 stn066037_2004-8-12.txt
-rw-rw-r--. 1 vinorda vinorda 2329 Oct  8 17:55 stn066037_2005-11-23.txt
-rw-rw-r--. 1 vinorda vinorda 1247 Oct  8 17:56 stn066037_2014-3-16.txt
-rw-rw-r--. 1 vinorda vinorda  216 Oct  8 17:56 stn066037_2019-1-31.txt
-rw-rw-r--. 1 vinorda vinorda 1471 Oct  8 17:56 stn066037_2020-1-20.txt


[vinorda@qld-ams-ws01 Downloads]$ source=./f160_0200n
[vinorda@qld-ams-ws01 Downloads]$ target=./f160_0300n

for i in "$source"/*
> do
> f1=`stat -c%s $i`
> f2=`stat -c%s $target/${i##*/}`
> if [ "$f1" > "$f2" ]; then
> echo "$i" "$f1" VS "$target/${i##*/}" "$f1" "is larger" "Copy to target  cp f1 f2"
> cp  "$i" $target/${i##*/}
> fi
> done 

stat: cannot stat `./f160_0300n/stn066037_2001-4-21.txt': No such file or directory
bash: : No such file or directory
./f160_0200n/stn066037_2004-8-12.txt 1052 VS ./f160_0300n/stn066037_2004-8-12.txt 1052 is larger Copy to target  cp f1 f2
./f160_0200n/stn066037_2005-11-23.txt 412 VS ./f160_0300n/stn066037_2005-11-23.txt 412 is larger Copy to target  cp f1 f2
./f160_0200n/stn066037_2014-3-16.txt 1133 VS ./f160_0300n/stn066037_2014-3-16.txt 1133 is larger Copy to target  cp f1 f2
./f160_0200n/stn066037_2019-1-31.txt 216 VS ./f160_0300n/stn066037_2019-1-31.txt 216 is larger Copy to target  cp f1 f2
./f160_0200n/stn066037_2020-1-20.txt 1471 VS ./f160_0300n/stn066037_2020-1-20.txt 1471 is larger Copy to target  cp f1 f2

Result - not good!!!!!!!

[vinorda@qld-ams-ws01 Downloads]$ ls -ltr ./f160_0200n
total 24
-rw-rw-r--. 1 vinorda vinorda  792 Oct  8 11:20 stn066037_2001-4-21.txt
-rw-rw-r--. 1 vinorda vinorda 1052 Oct  8 11:34 stn066037_2004-8-12.txt
-rw-rw-r--. 1 vinorda vinorda  412 Oct  8 11:39 stn066037_2005-11-23.txt
-rw-rw-r--. 1 vinorda vinorda 1133 Oct  8 12:12 stn066037_2014-3-16.txt
-rw-rw-r--. 1 vinorda vinorda  216 Oct  8 12:32 stn066037_2019-1-31.txt
-rw-rw-r--. 1 vinorda vinorda 1471 Oct  8 12:36 stn066037_2020-1-20.txt
[vinorda@qld-ams-ws01 Downloads]$ ls -ltr ./f160_0300n
total 20
-rw-rw-r--. 1 vinorda vinorda 1052 Oct  8 18:09 stn066037_2004-8-12.txt
-rw-rw-r--. 1 vinorda vinorda  412 Oct  8 18:09 stn066037_2005-11-23.txt
-rw-rw-r--. 1 vinorda vinorda 1133 Oct  8 18:09 stn066037_2014-3-16.txt
-rw-rw-r--. 1 vinorda vinorda  216 Oct  8 18:09 stn066037_2019-1-31.txt
-rw-rw-r--. 1 vinorda vinorda 1471 Oct  8 18:09 stn066037_2020-1-20.txt



1) compare the size of the same file (i.e. p2_000001.gz).
2) if the file size (p2_000001.gz) under /P2/backup/ is larger than that under /P2/log/cerner_prod/millennium/archive/, then skip it
3) if if the file size (p2_000001.gz) under /P2/backup/ is smaller than that under /P2/log/cerner_prod/millennium/archive/, then copy or overwrite

In this case, /P2/backup/ will have always the latest file.


#!/bin/sh
cd /P2/log/cerner_prod/millennium/archive/
for f in *
do      [ -f "$f" ] || continue     #If this isn't a regular file, skip it.
        [ -e "/P2/backup/$f" ] && continue  #If a backup already exists, skip it.
        cp "$f" /P2/backup/          # Make a backup copy.
done


works absolutely charming  - i.e with this code in the fo loop - absolutely no idea how it works.

With bash and stat: 
 (( ($(stat -c"%s-" "$f" /P2/backup/"$f") 0)  > 0 )) && cp "$f" /P2/backup/
No bash required:
expr $(stat -c"%s >" "$f") $(stat -c"%s" /P2/backup/"$f") && cp "$f" /P2/backup/





Mission
When download sonde data for sydney 
data is for either 0200 or 0300 or 0500 sonde flights
how do we merger this data files so we keep only data that has
full sonde data
for e.g if 0300 sonde has aall the data (bigger file) then we keep the 0300
sonde data and remore/delete the 0200 file

[bou@bous-fed31 Downloads]$ ls -ltr f160_0200n
total 20
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou  412 Oct  9 10:13 stn066037_2005-11-23.txt
-rw-rw-r--. 1 bou bou 1133 Oct  9 10:13 stn066037_2014-3-16.txt
-rw-rw-r--. 1 bou bou  216 Oct  9 10:14 stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 stn066037_2020-1-20.txt


[bou@bous-fed31 Downloads]$ ls -ltr f160_0300n
total 20
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou 2329 Oct  9 10:13 stn066037_2005-11-23.txt
-rw-rw-r--. 1 bou bou 1247 Oct  9 10:13 stn066037_2014-3-16.txt
-rw-rw-r--. 1 bou bou  216 Oct  9 10:13 stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 stn066037_2020-1-20.txt


[bou@bous-fed31 Downloads]$ source=./f160_0200n
[bou@bous-fed31 Downloads]$ target=./f160_0300n

[bou@bous-fed31 Downloads]$ ls -ltr $source/*
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 ./f160_0200n/stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou  412 Oct  9 10:13 ./f160_0200n/stn066037_2005-11-23.txt
-rw-rw-r--. 1 bou bou 1133 Oct  9 10:13 ./f160_0200n/stn066037_2014-3-16.txt
-rw-rw-r--. 1 bou bou  216 Oct  9 10:14 ./f160_0200n/stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 ./f160_0200n/stn066037_2020-1-20.txt

[bou@bous-fed31 Downloads]$ ls -ltr "$source"/*
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 ./f160_0200n/stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou  412 Oct  9 10:13 ./f160_0200n/stn066037_2005-11-23.txt
-rw-rw-r--. 1 bou bou 1133 Oct  9 10:13 ./f160_0200n/stn066037_2014-3-16.txt
-rw-rw-r--. 1 bou bou  216 Oct  9 10:14 ./f160_0200n/stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 ./f160_0200n/stn066037_2020-1-20.txt

Single qoutes fails
[bou@bous-fed31 Downloads]$ ls -ltr '$source'/*
ls: cannot access '$source/*': No such file or directory




[bou@bous-fed31 Downloads]$ f1=stat -c%s $i
bash: -c%s: command not found...
[bou@bous-fed31 Downloads]$ f1=`stat -c%s $i`
stat: missing operand
Try 'stat --help' for more information.
[bou@bous-fed31 Downloads]$ stat -c%s ./f160_0200n/stn066037_2004-8-12.txt
1052
[bou@bous-fed31 Downloads]$ stat -c%s ./f160_0200n/stn066037_2005-11-23.txt
412
[bou@bous-fed31 Downloads]$ stat -c%s ./f160_0300n/stn066037_2005-11-23.txt
2329
[bou@bous-fed31 Downloads]$ # sonde data file in 0300Z is larger size
[bou@bous-fed31 Downloads]$ i=./f160_0200n/stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ i
bash: i: command not found...
[bou@bous-fed31 Downloads]$ echo $i
./f160_0200n/stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ f1=stat -c%s $i
bash: -c%s: command not found...
[bou@bous-fed31 Downloads]$ f1=`stat -c%s $i`
[bou@bous-fed31 Downloads]$ echo $f1
412
[bou@bous-fed31 Downloads]$ f2=`stat -c%s $target/${i##*/}`
[bou@bous-fed31 Downloads]$ echo $f2
2329
[bou@bous-fed31 Downloads]$ echo $i
./f160_0200n/stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ echo `$i{i##*/}`
bash: ./f160_0200n/stn066037_2005-11-23.txt{i##*/}: No such file or directory

[bou@bous-fed31 Downloads]$ echo "$i{i##*/}"
./f160_0200n/stn066037_2005-11-23.txt{i##*/}
[bou@bous-fed31 Downloads]$ echo "${i##*/}"
stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ # this appears to grab only the filename only 
[bou@bous-fed31 Downloads]$ # excludes everything upto last fwd slash /
[bou@bous-fed31 Downloads]$ echo "${i##*/}"
stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ # this creates full path of filename of same file in target folder
[bou@bous-fed31 Downloads]$ f1=`stat -c%s $i`
[bou@bous-fed31 Downloads]$ f1
bash: f1: command not found...
[bou@bous-fed31 Downloads]$ echo $f1
412
[bou@bous-fed31 Downloads]$ f2=`stat -c%s $target/${i##*/}`
[bou@bous-fed31 Downloads]$ # this calls stat to find size of same file in target folder
[bou@bous-fed31 Downloads]$ "$f1" > "$f2"
bash: 412: command not found...
[bou@bous-fed31 Downloads]$ echo "$f1" > "$f2"
[bou@bous-fed31 Downloads]$ `"$f1" > "$f2"`
bash: 412: command not found...
[bou@bous-fed31 Downloads]$ diff = {$f1 > $f2}
diff: =: No such file or directory
diff: {412: No such file or directory
[bou@bous-fed31 Downloads]$ diff = `{$f1 > $f2}`
bash: {412: command not found...
diff: missing operand after '='
diff: Try 'diff --help' for more information.
[bou@bous-fed31 Downloads]$ diff=`{$f1 > $f2}`
bash: {412: command not found...
[bou@bous-fed31 Downloads]$ diff=`{"$f1" > "$f2"}`
bash: {412: command not found...
[bou@bous-fed31 Downloads]$ diff={"$f1" > "$f2"}
[bou@bous-fed31 Downloads]$ echo $diff
{412
[bou@bous-fed31 Downloads]$ $target/${i##*/}
bash: ./f160_0300n/stn066037_2005-11-23.txt: Permission denied
[bou@bous-fed31 Downloads]$ ls $target/${i##*/}
./f160_0300n/stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ ls $i
./f160_0200n/stn066037_2005-11-23.txt
[bou@bous-fed31 Downloads]$ # if file in $i (source) ir larger than one in target - cp source to taget
[bou@bous-fed31 Downloads]$ 




[bou@bous-fed31 Downloads]$ for i in "$source"/*
> do
> f1=`stat -c%s $i`
> f2=`stat -c%s $target/${i##*/}`
> if [ "$f1" > "$f2" ]; then
> echo "$i" "$f1" VS "$target/${i##*/}" "$f2"  "Delete" "$target/${i##*/}"
> else
> echo "$i" "$f1" VS "$target/${i##*/}" "$f2"  "Delete" "$i"
> fi
> done
./f160_0200n/stn066037_2004-8-12.txt 1052 VS ./f160_0300n/stn066037_2004-8-12.txt 1052 Delete ./f160_0300n/stn066037_2004-8-12.txt
./f160_0200n/stn066037_2005-11-23.txt 412 VS ./f160_0300n/stn066037_2005-11-23.txt 2329 Delete ./f160_0300n/stn066037_2005-11-23.txt
./f160_0200n/stn066037_2014-3-16.txt 1133 VS ./f160_0300n/stn066037_2014-3-16.txt 1247 Delete ./f160_0300n/stn066037_2014-3-16.txt
./f160_0200n/stn066037_2019-1-31.txt 216 VS ./f160_0300n/stn066037_2019-1-31.txt 216 Delete ./f160_0300n/stn066037_2019-1-31.txt
./f160_0200n/stn066037_2020-1-20.txt 1471 VS ./f160_0300n/stn066037_2020-1-20.txt 1471 Delete ./f160_0300n/stn066037_2020-1-20.txt


if [ "$f1" > "$f2" ]  - not right as this is always going to delete files in target 

 
https://www.tecmint.com/best-linux-file-diff-tools-comparison/




https://unix.stackexchange.com/questions/496109/compare-files-and-select-bigger-one

#!/usr/bin/env zsh

zmodload -F zsh/stat b:zstat

for file2 in dir2/*(.); do
    file1="dir1/${file2##*/}"

    if [ -f "$file1" ] &&
       [ "$( zstat +size "$file2" )" -gt "$( zstat +size "$file1" )" ]
    then
        printf '%s is bigger than %s\n' "$file2" "$file1"
    fi
done

This is a zsh shell script that uses the built-in command zstat to portably get the file sizes.

The script will loop over all regular files with non-hidden names in the dir2 directory. For each file in dir2 it will construct the corresponding pathname for a file in dir1. If the file in dir1 exists and is a regular file (or a symbolic link to a regular file), the size of the two files are compared. If the file in dir2 is strictly bigger, a short message is outputted.

The pattern dir2/*(.) will match only non-hidden names of regular files in the dir2 directory. The (.) is a zsh-specific modifier for * that makes it match only regular files.

The expression "dir1/${file2##*/}" will expand to a pathname starting with dir1/ and then containing the value of $file2 with everything before and including the last / removed. This could be changed to "dir1/$( basename "$file2" )".


for i in "$source"/*; do
  f1=`stat -c%s $i`
  f2=`stat -c%s $target/${i##*/}`
  if [ "$f1" -gt "$f2" ]; then
  	echo "$i" "$f1" VS "$target/${i##*/}" "$f2"  "Delete" "$target/${i##*/}"
  else
  	echo "$i" "$f1" VS "$target/${i##*/}" "$f2"  "Delete" "$i"
  fi
  done


we try new approach
source=./f160_0200n
target=./f160_0300n

for file1 in "$source"/*
 do
  file2="$target/${file1##*/}"
  if [ -f "$file1" ] && 
     [ "$( stat -c%s "$file2" )" -gt "$( stat -c%s "$file1" )" ]
  then
     echo "$file2 is bigger then $file1 - we will delete $file1
  else
    echo "$file1 is bigger then $file2 - we will delete $file2
  fi
done




Finally




++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

#!/usr/bin/bash


<< 'MULTILINE-COMMENT'

When we download sonde data for Sydney
data is for either 0200 or 0300 or 0500 sonde flights
how do we merge this data files so we keep only data for that has full sonde info
for e.g if 0300 sonde is bigger file then we keep the 0300 for same date and remore/delete the 0200 file
for same date.

This script will loop over all regular files with non-hidden names in the source directory. For each file in source it will construct the corresponding pathname for a file in target directory. If the file in target exists and is a regular file (or a symbolic link to a regular file), the size of the two files are compared. If the file in source is strictly bigger, a short message is outputted.

The pattern source/*(.) will match only non-hidden names of regular files in the source directory. The (.) is a zsh-specific modifier for * that makes it match only regular files -none exist for bash!

The expression "target/${file1##*/}" will expand to a pathname starting with target/ and then containing the value of $file2 with everything before and including the last / removed. This could be changed to "target/$( basename "$file2" )".

This script is based on similar script found here
https://unix.stackexchange.com/questions/496109/compare-files-and-select-bigger-one

Nice BASH tutorial here
https://ryanstutorials.net/bash-scripting-tutorial/bash-script.php

TODO: pass the source and target folders as command line args.

MULTILINE-COMMENT


source=./f160_0200
target=./f160_0300

# run script again after switching folders to delete same but smaller files from other folder 

for file1 in $source/*; do
    file2="$target/${file1##*/}"

    file1_size=`stat -c%s $file1`
    if [ -f "$file2" ] 
    then
        file2_size=`stat -c%s $file2`
    fi

    printf 'Comparing %s with %s\n' "$file1" "$file2"
    printf 'Size %s with %s\n' "$file1_size" "$file2_size"

    
    #printf 'Difference %s\n' $(($file1_size-$file2_size))
    if [ -f "$file2" ] 
    then
        diff=$(($file1_size-$file2_size))
    fi
    #printf 'Difference %s\n' "$diff"

    #if  [ $diff -lt 0 ]
    #if [ $file1_size -lt $file2_size ]
    #if [ `stat -c%s $file1` -lt `stat -c%s $file2` ]
    # check see if file2 exists or not first (file1 will always exist!) 
    if [ -f "$file2" ] &&
       [ "$( stat -c%s "$file1" )" -lt "$( stat -c%s "$file2" )" ]
    then
        echo "$file1 is smaller then $file2 - we will delete $file1"
        #printf '%s is smaller than %s\n' "$file1" "$file2"
        rm -f $file1
    elif  [ "$( stat -c%s "$file2" )" -lt "$( stat -c%s "$file1" )" ]
    then
        echo "$file2 is smaller then $file1 - we will delete $file2"
        #printf '%s is smaller than %s\n' "$file2" "$file1"
        rm -f $file2
    fi
done

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

[bou@bous-fed31 Downloads]$ ./sonde_fix.sh
Comparing ./f160_0200n/stn066037_2004-8-12.txt with ./f160_0300n/stn066037_2004-8-12.txt
Size 1052 with 1052
Difference 0
Difference 0
Comparing ./f160_0200n/stn066037_2005-11-23.txt with ./f160_0300n/stn066037_2005-11-23.txt
Size 412 with 2329
Difference -1917
Difference -1917
./f160_0200n/stn066037_2005-11-23.txt is smaller then ./f160_0300n/stn066037_2005-11-23.txt - we will delete ./f160_0200n/stn066037_2005-11-23.txt
./f160_0300n/stn066037_2005-11-23.txt is bigger than ./f160_0200n/stn066037_2005-11-23.txt
Comparing ./f160_0200n/stn066037_2014-3-16.txt with ./f160_0300n/stn066037_2014-3-16.txt
Size 1133 with 1247
Difference -114
Difference -114
./f160_0200n/stn066037_2014-3-16.txt is smaller then ./f160_0300n/stn066037_2014-3-16.txt - we will delete ./f160_0200n/stn066037_2014-3-16.txt
./f160_0300n/stn066037_2014-3-16.txt is bigger than ./f160_0200n/stn066037_2014-3-16.txt
Comparing ./f160_0200n/stn066037_2019-1-31.txt with ./f160_0300n/stn066037_2019-1-31.txt
Size 216 with 216
Difference 0
Difference 0
Comparing ./f160_0200n/stn066037_2020-1-20.txt with ./f160_0300n/stn066037_2020-1-20.txt
Size 1471 with 1471
Difference 0
Difference 0

[bou@bous-fed31 Downloads]$ ls -ltr ./f160_0300n/*
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 ./f160_0300n/stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou 2329 Oct  9 10:13 ./f160_0300n/stn066037_2005-11-23.txt
-rw-rw-r--. 1 bou bou 1247 Oct  9 10:13 ./f160_0300n/stn066037_2014-3-16.txt
-rw-rw-r--. 1 bou bou  216 Oct  9 10:13 ./f160_0300n/stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 ./f160_0300n/stn066037_2020-1-20.txt

[bou@bous-fed31 Downloads]$ ls -ltr ./f160_0200n/*
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 ./f160_0200n/stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou  412 Oct  9 10:13 ./f160_0200n/stn066037_2005-11-23.txt  <<---- deleted
-rw-rw-r--. 1 bou bou 1133 Oct  9 10:13 ./f160_0200n/stn066037_2014-3-16.txt  <<---- deleted
-rw-rw-r--. 1 bou bou  216 Oct  9 10:14 ./f160_0200n/stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 ./f160_0200n/stn066037_2020-1-20.txt


[bou@bous-fed31 Downloads]$  ls -ltr ./f160_0200n/*
-rw-rw-r--. 1 bou bou 1052 Oct  9 10:12 ./f160_0200n/stn066037_2004-8-12.txt
-rw-rw-r--. 1 bou bou  216 Oct  9 10:14 ./f160_0200n/stn066037_2019-1-31.txt
-rw-rw-r--. 1 bou bou 1471 Oct  9 10:14 ./f160_0200n/stn066037_2020-1-20.txt



[bou@bous-fed31 Downloads]$ stat ./f160_0200/
  File: ./f160_0200/
  Size: 12288     	Blocks: 24         IO Block: 4096   directory
Device: fd03h/64771d	Inode: 1837626     Links: 2
Access: (0775/drwxrwxr-x)  Uid: ( 1000/     bou)   Gid: ( 1000/     bou)
Context: unconfined_u:object_r:user_home_t:s0
Access: 2020-10-09 10:05:11.416244119 +1000
Modify: 2020-10-09 10:04:19.893852097 +1000
Change: 2020-10-09 10:05:11.391243963 +1000
 Birth: 2020-10-09 10:04:19.813851307 +1000


[bou@bous-fed31 Downloads]$ stat ./f160_0300/
  File: ./f160_0300/
  Size: 131072    	Blocks: 256        IO Block: 4096   directory
Device: fd03h/64771d	Inode: 1714358     Links: 2
Access: (0775/drwxrwxr-x)  Uid: ( 1000/     bou)   Gid: ( 1000/     bou)
Context: unconfined_u:object_r:user_home_t:s0
Access: 2020-10-09 10:05:11.395243988 +1000
Modify: 2020-10-09 10:04:19.886852027 +1000
Change: 2020-10-09 10:05:11.390243957 +1000
 Birth: 2020-10-09 10:04:19.813851307 +1000

[bou@bous-fed31 Downloads]$ ./sonde_fix.sh


[bou@bous-fed31 Downloads]$ stat ./f160_0200/
  File: ./f160_0200/
  Size: 12288     	Blocks: 24         IO Block: 4096   directory
Device: fd03h/64771d	Inode: 1837626     Links: 2
Access: (0775/drwxrwxr-x)  Uid: ( 1000/     bou)   Gid: ( 1000/     bou)
Context: unconfined_u:object_r:user_home_t:s0
Access: 2020-10-10 08:39:10.805221910 +1000
Modify: 2020-10-10 08:39:10.965222936 +1000
Change: 2020-10-10 08:39:10.965222936 +1000
 Birth: 2020-10-09 10:04:19.813851307 +1000


[bou@bous-fed31 Downloads]$ stat ./f160_0300/
  File: ./f160_0300/
  Size: 131072    	Blocks: 256        IO Block: 4096   directory
Device: fd03h/64771d	Inode: 1714358     Links: 2
Access: (0775/drwxrwxr-x)  Uid: ( 1000/     bou)   Gid: ( 1000/     bou)
Context: unconfined_u:object_r:user_home_t:s0
Access: 2020-10-09 10:05:11.395243988 +1000
Modify: 2020-10-09 10:04:19.886852027 +1000
Change: 2020-10-09 10:05:11.390243957 +1000
 Birth: 2020-10-09 10:04:19.813851307 +1000










######################################################


#####################################################
## WARNING : IF U CHANGE f160 FOLDER - FEW THINGS BREAK!!!

def batch_process_F160_hanks():
    import os
    from glob import glob
    cur_dir = "/home/accounts/vinorda/Downloads/"   # ONLY FOR STANDALONE CALLS
    print("f160 data dir = ", cur_dir+'f160_2300/')
    filenames = glob(cur_dir + 'f160_2300/stn*txt')
    print(len(filenames), filenames[-5:])
    dataframes = []
    f_dates=[]

    # dataframes = [pd.read_csv(f) for f in filenames]

    for f in filenames:
        '''some files don't have any data
        files > 215KB are OK THRU trial/error
        '''

        # print('file size',os.path.getsize(f))

        try:
            if os.path.getsize(f) > 215:

                # Non empty file exists, extract date from file name
                # NB IF U CHANGE f160 FOLDER - CAN'T GET DATES!!!
                # cur_dir = '/home/accounts/vinorda/sr-ask-11966/data/' split(7)!!

                print(f.split('/')[6].split('.')[0][10:])          #<<< CHANGE HERE 
                f_dates.append(f.split('/')[6].split('.')[0][10:]) #<<< AND HERE 
                dat = pd.read_csv(f, skiprows=10, skip_blank_lines=True)

                # drop these 2 cols - have no actual data
                dat.drop(['Geop', 'AbsHum'], axis=1,inplace=True)

                # drop rows with 2 or more NAN/missing
                # dat.dropna(axis=0, thresh=2,inplace=True)

                # force convert all data to numeric
                for col in  dat.columns:
                    dat[col] = pd.to_numeric(dat[col], errors='coerce')

                dataframes.append(dat)

            else:
                # If file is empty - skip
                print("Empty file {}!!".format(f))
                continue # go to next file
        except OSError as e:
            # File does not exists or is non accessible
            print("What da ..Empty file {}".format(f))
        else:
            print("Processing file {}".format(f))

    # concat/join all daily data into 1 big file
    df = pd.concat(dataframes,axis=0, keys=f_dates)





=======================================================================================


ls -ltr *NEW_aws.pkl
-rw-rw-r--. 1 bou bou 104941530 Oct  2 14:34 YBBN_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 106223862 Oct  2 14:35 YBAF_NEW_aws.pkl
-rw-rw-r--. 1 bou bou  93595081 Oct  2 14:36 YAMB_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 109961978 Oct  2 14:36 YBSU_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 108301622 Oct  2 14:37 YBCG_NEW_aws.pkl
-rw-rw-r--. 1 bou bou  91683339 Oct  2 14:38 YTWB_NEW_aws.pkl
-rw-rw-r--. 1 bou bou  91924323 Oct  2 14:39 YBOK_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 107067164 Oct  2 14:40 YBRK_NEW_aws.pkl
-rw-rw-r--. 1 bou bou  93277095 Oct  2 15:09 YSHL_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 104537021 Oct  2 15:10 YSCN_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 111444757 Oct  2 15:10 YSRI_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 107986150 Oct  2 15:11 YSBK_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 106098429 Oct  2 15:12 YSSY_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 109244764 Oct  2 15:13 YWLM_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 105437616 Oct  2 15:14 YBAS_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 104110425 Oct  2 15:15 YPTN_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 106860968 Oct  2 15:16 YPDN_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 112152416 Oct  2 15:17 YPPD_NEW_aws.pkl
-rw-rw-r--. 1 bou bou 105287525 Oct  2 15:17 YBRM_NEW_aws.pkl
-rw-rw-r--. 1 bou bou  92762350 Oct  2 15:18 YPCC_NEW_aws.pkl
-rw-rw-r--. 1 bou bou  83062059 Oct  2 15:19 YPXM_NEW_aws.pkl


[bou@bous-fed31 data]$ /usr/local/bin/rename 's/NEW_aws/aws/' *NEW_aws.pkl

-rw-rw-r--. 1 bou bou 104941530 Oct  2 14:34 YBBN_aws.pkl
-rw-rw-r--. 1 bou bou 106223862 Oct  2 14:35 YBAF_aws.pkl
-rw-rw-r--. 1 bou bou  93595081 Oct  2 14:36 YAMB_aws.pkl
-rw-rw-r--. 1 bou bou 109961978 Oct  2 14:36 YBSU_aws.pkl
-rw-rw-r--. 1 bou bou 108301622 Oct  2 14:37 YBCG_aws.pkl
-rw-rw-r--. 1 bou bou  91683339 Oct  2 14:38 YTWB_aws.pkl
-rw-rw-r--. 1 bou bou  91924323 Oct  2 14:39 YBOK_aws.pkl
-rw-rw-r--. 1 bou bou 107067164 Oct  2 14:40 YBRK_aws.pkl
-rw-rw-r--. 1 bou bou  93277095 Oct  2 15:09 YSHL_aws.pkl
-rw-rw-r--. 1 bou bou 104537021 Oct  2 15:10 YSCN_aws.pkl
-rw-rw-r--. 1 bou bou 111444757 Oct  2 15:10 YSRI_aws.pkl
-rw-rw-r--. 1 bou bou 107986150 Oct  2 15:11 YSBK_aws.pkl
-rw-rw-r--. 1 bou bou 106098429 Oct  2 15:12 YSSY_aws.pkl
-rw-rw-r--. 1 bou bou 109244764 Oct  2 15:13 YWLM_aws.pkl
-rw-rw-r--. 1 bou bou 105437616 Oct  2 15:14 YBAS_aws.pkl
-rw-rw-r--. 1 bou bou 104110425 Oct  2 15:15 YPTN_aws.pkl
-rw-rw-r--. 1 bou bou 106860968 Oct  2 15:16 YPDN_aws.pkl
-rw-rw-r--. 1 bou bou 112152416 Oct  2 15:17 YPPD_aws.pkl
-rw-rw-r--. 1 bou bou 105287525 Oct  2 15:17 YBRM_aws.pkl
-rw-rw-r--. 1 bou bou  92762350 Oct  2 15:18 YPCC_aws.pkl
-rw-rw-r--. 1 bou bou  83062059 Oct  2 15:19 YPXM_aws.pkl






Modify utility_functions so we can process all tcz files and add ts status to each observation at the sme time


def merge_aws_gpats_data(sta):

    #cur_dir = "/home/accounts/qdisplay/avguide/app/data/gpats"
    cur_dir='/home/bou/shared/stats-R/flask_projects/avguide/app/data'
    '''In the app,  cur_dir will come from environment config
    gpats fil ename like       --> gpats_YPXM_10NM.csv
    tcz climate zone data file --> HM01X_Data_YPXM.txt'''
    # gpats_file = f'gpats_{sta}_10NM.csv'
    # file = os.path.join(cur_dir,'app','data',data_file)
    gpats_file = os.path.join(cur_dir, f'gpats_{sta}_10NM.csv')
    tcz_file = os.path.join(cur_dir, f'HM01X_Data_{sta}.txt')

    print(f'\nProcessing sta={sta},\nClimate Zone data file:{tcz_file}",\
        \nGpats file: {gpats_file}')

    # load climate zone data file, except for Brisbane
    #if sta is 'YBBN':
    #    df = pickle.load(open(os.path.join(cur_dir,'tcz', aws_file), 'rb'))
    #else:
    #df = process_climate_zone_csv(cur_dir+'tcz/'+aws_file)
    df = process_climate_zone_csv_2020(sta)
    print(df.head(1))
    print(df.tail(1))

    # we check both aws metar/speci and gpats for ts days now
    # this not just checks fo gpats lightning but also observer PW groups
    gpats =    get_gpats_data(cur_dir, sta)
    print(gpats.head(1))
    print(gpats.tail(1))

    ts_dates = get_storm_dates(gpats,df)

    # print(len(ts_dates), ts_dates)
    df = df[cols_2_keep]

    # FLAG AWS OBS WITH TS FLAG IF TS ON THAT DATE
    df['date'] = pd.to_datetime(df.index.date)
    df['any_ts'] = df['date'].isin(ts_dates)

    '''for stations without manual obs, unless TS has caused a SPECI
    (vis/ceil reductions or wind gust),no SPECI will be generated.
    In this case we use gpats observation and flag the aws obs closest
    (in time) to the gpats record with gpats['AMP'] count.
    non zero values of this count would indicate presence of lightning.

    fn get_gpats_data() expects gpats folder and station avID
    def get_gpats_data(gpats_dir,sta):
      gpats_file = gpats_dir+sta+'_gpats_10NM.csv'

    # gpats.resample('D')['AMP'].count() <- daily lightning counts
    '''
    # RESAMPLE GPATS TO 1MIN LIGHTNING COUNTS AND MERGE WITH CLOSEST AWS OBS
    # gpats = get_gpats_data(cur_dir, sta)
    df = pd.merge( left=df,
                  right=gpats.resample('1min')[['AMP']].count(),
                  left_index=True, right_index=True,how='left')

    '''potential data redundancy 'any_ts' flags all obs for day with TS status
    'AMP' lightning count assigned to aws obs if gpats around time of obs'''
    file = os.path.join(cur_dir, f'{sta}_NEW_aws.pkl')
    with open(file, 'wb') as f:
        pickle.dump(df, f)




def process_climate_zone_csv_2020(station:str='YBBN'):
    cur_dir='/home/bou/shared/stats-R/flask_projects/avguide/app/data'
    '''In the app,  cur_dir will come from environment config'''
    # data_file = f'HM01X_Data_{tcz_file_map[station]}.txt'
    
    data_file = os.path.join(cur_dir, f'HM01X_Data_{station}.txt')

    print(f'\nProcessing aviation location {station}, file={data_file}')
    use_tcz_cols =[ 5,  7,  8,  9, 10, 11, 13, 14, 17, 18, 19, 20, 22, 23, 25, 26, 28,
    32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 47, 48, 50, 51,
    58, 59, 60, 63, 64]

    tcz_col_names = ['AvID', 'date', 'M_type', 'pptn10min', 'pptnSince9', 'T', 'Td',
    'RH', 'WS', 'WDir', 'MaxGust10min', 'CL1_amnt', 'CL1_ht',
    'CL2_amnt', 'CL2_ht', 'CL3_amnt', 'CL3_ht', 'Ceil1_amnt',
    'Ceil1_ht', 'Ceil2_amnt', 'Ceil2_ht', 'Ceil3_amnt', 'Ceil3_ht',
    'CeilSKCflag', 'vis', 'vis_min_dir', 'vis_aws', 'PW', 'PW1_desc',
    'PW1_type', 'PW2_desc', 'PW2_type', 'PW3_desc', 'PW3_type',
    'AWS_PW', 'AWS_PW15min', 'AWS_PW60min', 'QNH', 'AWS_Flag']

    cols_num = ['pptn10min', 'pptnSince9', 'T','Td', 'RH','WS', 'WDir', 'MaxGust10min',
    'CL1_amnt', 'CL1_ht', 'CL2_amnt', 'CL2_ht','CL3_amnt','CL3_ht','Ceil1_ht', 'Ceil2_ht',
    'Ceil3_ht','vis', 'vis_min_dir', 'vis_aws','PW', 'QNH', 'AWS_Flag',
    'CeilSKCflag']

    cols_str = ['AvID','M_type','Ceil1_amnt','Ceil2_amnt', 'Ceil3_amnt',
    'PW1_desc', 'PW1_type','PW2_desc', 'PW2_type', 'PW3_desc','PW3_type',
    'AWS_PW', 'AWS_PW15min', 'AWS_PW60min','CeilSKCflag']

    df = pd.read_csv(data_file,
    parse_dates=[7],            # col 8,9 datetime like
    dayfirst=True,                # format like '31/10/2012 23:30' in file
    infer_datetime_format = True, # faster parsing 5-10x
    usecols=use_tcz_cols,
    names=tcz_col_names,          # rename climatezone columns using cols list
    index_col=['date'],           # make UTC datetime index
    low_memory=False,
    skiprows=1,                   # skip 1 line(s) at the start of the file.
    header=None )                 # don't use headers - we skipped header row!

    # convert these cols expected to be float/int to numeric
    # df[cols_num] = df[cols_num].astype(float, 'ignore')
    # too many errors with above vectorized approach

    for col in  cols_num:
        df[col] = pd.to_numeric(df[col], errors='coerce')

    # convert text data to string
    df[cols_str] = df[cols_str].astype(str)

    df.index = pd.to_datetime(df.index)

    '''  Created new aws data Sept 2020
    We now do not write data file until merged with gpats
    cur_dir = '/home/bou/shared/stats-R/flask_projects/avguide'

    with open(
        os.path.join(cur_dir,'app','data',station+'_aws.pkl') , 'wb') as f:
        pickle.dump(df, f)
    '''
    return (df)



def get_gpats_data(gpats_dir,sta):

    # gpats_file = f'gpats_{sta}_10NM.csv'
    # file = os.path.join(cur_dir,'app','data',data_file)
    gpats_file = os.path.join(gpats_dir, f'gpats_{sta}_10NM.csv')
    print("Reading {} gpats file {}".format(sta, gpats_file))

    gpats = pd.read_csv(gpats_file,
                 parse_dates=True, index_col='TM')

    # Non-standard datetime
    # use pd.to_datetime after pd.read_csv
    # apply custom string format to datetime,
    # this ignore/drops higher precision microseconds
    gpats_new_index = gpats.index.strftime('%Y:%m:%d %H:%M:%S')

    # convert string series of dateime like objects
    # back to datetime index and set as new index
    gpats.index = pd.to_datetime(gpats_new_index,
                                 format='%Y:%m:%d %H:%M:%S',
                                 errors='coerce')
    return (gpats)


def get_storm_dates(gpats,df):

    #storm_dates = []
    storm_dates_pw = []
    storm_dates_gpats=[]

    sta = (df.iloc[-1]['AvID']).strip()
    print("Getting storm dates for:", sta,"from aws metar/speci and gpats data.")

    #gpats = get_gpats_data(cur_dir + 'gpats/', sta)
    #print("Reading gpats file:",(cur_dir + 'gpats/', sta))

    gpats_ts_stats = get_gpats_start_end_duration(gpats)
    storm_dates_gpats = gpats_ts_stats.index.unique()
    print("Storm dates derived from gpats data for {} = {}"\
          .format(sta, len(storm_dates_gpats)))

    #storm_dates = storm_dates_gpats
    '''
    If manual station, check observers Present Weather groups
    AWS (Automatic Weather Station) flag
    0 Manned    1 Automatic     2 Hybrid '''
    # if (df.iloc[-1]['AWS_Flag'] == 2): #tis wud not alwys work!!
    hybrid_sta_list = ['YBBN','YAMB','YBOK','YTBL','YBCS','YSSY','YBCS','YPDN','YBTL']
    if sta in hybrid_sta_list:
        # build dict key:val, where key is wx-codes and value is wx-desc
        pw = {91: 'TS', 92: 'TS', 93: 'TS', 94: 'TS', 95: 'TS', \
          96: 'TSGS', 99: 'TSGR', 97: '+TS', 98: 'TSDU', \
          17: 'Thunder', 13: 'Lightning', 29: 'Recent TS', \
          27: 'GSGR', 89: 'SHGS', 90: 'SHGR'}

        '''NB: Hail falls as SH from CB only, generally with TS activity
        Hail GR >=5mm diameter (GS<=5mm- small hail/snow pellets)        '''

        dat = df.copy()  # make copy so as not to corrupt orignal df

        # for the other reported present weatqher groups
        dat['pw1'] = str_join(dat, '', 'PW1_desc', 'PW1_type')
        dat['pw2'] = str_join(dat, '', 'PW2_desc', 'PW2_type')
        dat['pw3'] = str_join(dat, '', 'PW3_desc', 'PW3_type')
        # We no longer processing RW grops !!!
        #dat['rw1'] = str_join(dat, '', 'RW1_desc', 'RW1_type')
        #dat['rw2'] = str_join(dat, '', 'RW2_desc', 'RW2_type')
        #dat['rw3'] = str_join(dat, '', 'RW3_desc', 'RW3_type')

        # join groups using ':'
        dat['pw1to3'] = str_join(dat, ':', 'pw1', 'pw2', 'pw3')
        #dat['rw1to3'] = str_join(dat, ':', 'rw1', 'rw2', 'rw3')

        # join PWs to RWs using '|'
        dat['pw_rw'] = str_join(dat, '|', 'pw1to3') #, 'rw1to3')

        # get TS start and end times and duration
        #wx_cols = ['T', 'Td', 'RH', 'WS', 'WDIR', 'vis', 'vis_aws',
        #           'PW', 'MaxGust10min', 'pptn10min', 'QNH']

        pw_ts_mask = dat['PW'].isin(list(pw.keys()))
        pwrw_ts_mask = dat['pw_rw'].str.contains(r'TS')
        wx_ts_mask = pw_ts_mask | pwrw_ts_mask

        wx_ts_specials = dat.loc[wx_ts_mask]
        ts_start_end = get_ts_start_end_duration(wx_ts_specials)
        storm_dates_pw = ts_start_end.index.unique()
        print("Storm dates derived from observer reported PW groups for {} = {}" \
              .format(sta, len(storm_dates_pw)))

        # WE could also do fog same way
        # pwrw_fg_mask = dat['pw_rw'].str.contains(r'FG')
        '''# get FOG stats -> start and end times and duration
        wx_fg_specials = dat.loc[pwrw_fg_mask,wx_cols]
        fg_start_end = get_ts_start_end_duration(wx_fg_specials)
        fg_vis1km = fg_start_end[fg_start_end.min_vis < 1]'''

    print("Total TS dates found for {} = {}" \
              .format(sta,
                len(set(storm_dates_pw).union(set(storm_dates_gpats))), # union two sets
                len(np.unique(list(set().union(storm_dates_pw,storm_dates_gpats))))))   # union two lists

    '''A python set is a dictionary that contains only keys (and no values).
    Dictionary keys are, by definition, unique. Duplicate items are weeded out automatically'''
    return  list(set(storm_dates_pw).union(set(storm_dates_gpats)))



def get_gpats_start_end_duration(gpat):

    import numpy as np

    '''define multiple aggregations per group
    use the 'AMP' column for counting strikes
    use 'Time' col to grab first and last gpats for a given day
    '''
    gpats = gpat.copy()
    gpats['Time'] = gpats.index

    aggregator = {'AMP' : {'gpats_cnt':'count'}, 'Time' : ['first','last']}
    daily = gpats.resample('D').apply(aggregator).dropna()
    # NB using a dict with renaming is deprecated
    # and will be removed in a future version!!

    tmp = daily['Time']
    daily.loc[:, ('Time','duration')] = round(\
        (tmp['last'] - tmp['first'])/np.timedelta64(1, 'h') , 1)

    # drop outer-most column index level 0 ['CNT','Time']
    daily.columns = daily.columns.droplevel(0)

    # get rid of zero duration events
    # daily = daily[daily['duration'] > '00:00:00']
    daily = daily[daily['duration'] > 0]

    return( daily[['gpats_cnt','duration','first','last']])





[bou@bous-fed31 avguide]$ source ~/py37/bin/activate
(py37) [bou@bous-fed31 avguide]$ python
Python 3.7.9 (default, Aug 19 2020, 17:05:11) 
[GCC 9.3.1 20200408 (Red Hat 9.3.1-2)] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> import utility_functions_sep2018 as bous
>>> df=bous.process_climate_zone_csv_2020('YBBN')

Processing aviation location YBBN, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBBN.txt
>>> cur_dir='/home/bou/shared/stats-R/flask_projects/avguide/app/data'

>>> gpats=bous.get_gpats_data(cur_dir,'YBBN')
Reading YBBN gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBBN_10NM.csv
>>> gpats.head()
                     LATITUDE  LONGITUDE   AMP
2008-03-28 04:37:06 -27.26361  152.97498 -16.3
2008-03-28 04:39:42 -27.30041  153.10553   9.6
2008-03-28 04:41:33 -27.24941  153.03023 -99.4
2008-03-28 04:43:08 -27.30812  153.01167  -3.6
2008-03-28 04:44:49 -27.29779  153.00990 -10.8


>>> gpats_30min=gpats.resample('30min')['AMP'].count()

>>> gpats_30min.loc[gpats_30min>0]
2008-03-28 04:30:00     9
2008-03-28 05:00:00    33
2008-03-28 05:30:00     1
2008-03-28 06:00:00     2
2008-04-07 02:00:00     1
                       ..
2020-04-19 01:30:00     2
2020-05-14 04:30:00     1
2020-09-25 11:30:00     8
2020-09-25 12:00:00     3
2020-09-25 13:00:00     1
Name: AMP, Length: 1383, dtype: int64

# easy way to get days when have TS
>>> gpats_daily = gpats.resample('D')['AMP'].count()

>>> gpats_daily.loc[gpats_daily>0]
2008-03-28    45
2008-04-07     1
2008-04-14    37
2008-05-14     7
2008-05-17    62
              ..
2020-03-06    34
2020-04-04     2
2020-04-19    16
2020-05-14     1
2020-09-25    12
Name: AMP, Length: 371, dtype: int64

Lets study this day 2020-04-19  with  16 strikes in day

>>> gpats.loc['2020-04-19']
                     LATITUDE  LONGITUDE   AMP

2020-04-19 00:54:08 -27.45262  152.96706 -29.3
2020-04-19 00:58:21 -27.33538  152.97119 -51.0

2020-04-19 01:01:32 -27.34158  153.02942 -43.3
2020-04-19 01:01:32 -27.34942  153.01424 -82.9
2020-04-19 01:01:32 -27.30629  152.99992 -48.1
2020-04-19 01:01:32 -27.30636  152.99686 -39.2
2020-04-19 01:01:32 -27.30462  152.99963 -32.8
2020-04-19 01:13:46 -27.41352  152.97476 -38.1
2020-04-19 01:13:46 -27.40233  153.15678 -87.5
2020-04-19 01:13:46 -27.51841  153.21625 -34.9
2020-04-19 01:16:41 -27.44967  153.06673 -35.2
2020-04-19 01:23:22 -27.41922  153.18452  22.3
2020-04-19 01:26:38 -27.47115  153.21776  43.8
2020-04-19 01:27:26 -27.39917  153.09073 -38.6

2020-04-19 01:50:53 -27.39063  153.27675 -54.7
2020-04-19 01:52:20 -27.44975  153.26437 -55.6



>>> gpats_30min.loc['2020-04-19'].head()
2020-04-19 00:00:00     0   <<--- in the 30min from 00:00 to 00:30 0 strikes
2020-04-19 00:30:00     2   <<--- in the 30min from 00:30 to 01:00 2 strikes
2020-04-19 01:00:00    12   <<--- in the 30min from 01:00 to 01:30 12 strikes
2020-04-19 01:30:00     2   <<--- in the 30min from 01:30 to 02:00 2 strikes
2020-04-19 02:00:00     0   <<--- in the 30min from 02:00 to 02:30 0 strikes
Freq: 30T, Name: AMP, dtype: int64

Climate zone data for this period only

cols=['AvID','M_type','pptn10min','T','Td','MaxGust10min','vis','vis_aws','PW']


>>> df.loc['2020-04-19',cols].head(10)
                       AvID M_type  pptn10min     T    Td  MaxGust10min   vis  vis_aws    PW
date                                                                                        
2020-04-19 00:00:00  YBBN        m        0.0  21.3  13.5           7.0  10.0     10.0   NaN
2020-04-19 00:30:00  YBBN        m        0.0  22.1  12.8           8.0  10.0     10.0   NaN
2020-04-19 01:00:00  YBBN        m        0.0  22.2  12.3           7.0  10.0     10.0   NaN
2020-04-19 01:21:00  YBBN        s        1.2  19.1  16.6          15.0   6.0      6.0  95.0
2020-04-19 01:30:00  YBBN        s        2.4  18.4  14.6          15.0   5.0      3.2  81.0
2020-04-19 01:49:00  YBBN        s        0.6  18.1  14.5           6.0  10.0      5.0  81.0
2020-04-19 02:00:00  YBBN        m        0.0  19.0  15.6           7.0  10.0     10.0  80.0
2020-04-19 02:30:00  YBBN        m        0.0  20.2  15.1           4.1  10.0     10.0   NaN
2020-04-19 03:00:00  YBBN        m        0.0  21.7  15.9           5.1  10.0     10.0   NaN
2020-04-19 03:30:00  YBBN        m        0.0  22.4  16.7           5.1  10.0     10.0   NaN


>>> d=pd.merge(left=df[cols],right=gpats_30min,left_index=True, right_index=True,how='left').loc['2020-04-19'].head(10)
>>> d
                       AvID M_type  pptn10min     T    Td  MaxGust10min   vis  vis_aws    PW   AMP
date                                                                                              
2020-04-19 00:00:00  YBBN        m        0.0  21.3  13.5           7.0  10.0     10.0   NaN   0.0  <-- from gpats30min
2020-04-19 00:30:00  YBBN        m        0.0  22.1  12.8           8.0  10.0     10.0   NaN   2.0  <-- from gpats30min
2020-04-19 01:00:00  YBBN        m        0.0  22.2  12.3           7.0  10.0     10.0   NaN  12.0  <-- from gpats30min
2020-04-19 01:21:00  YBBN        s        1.2  19.1  16.6          15.0   6.0      6.0  95.0   NaN  <-- index not in gpats
2020-04-19 01:30:00  YBBN        s        2.4  18.4  14.6          15.0   5.0      3.2  81.0   2.0  <-- from gpats30min
2020-04-19 01:49:00  YBBN        s        0.6  18.1  14.5           6.0  10.0      5.0  81.0   NaN  <-- index not in gpats
2020-04-19 02:00:00  YBBN        m        0.0  19.0  15.6           7.0  10.0     10.0  80.0   0.0
2020-04-19 02:30:00  YBBN        m        0.0  20.2  15.1           4.1  10.0     10.0   NaN   0.0
2020-04-19 03:00:00  YBBN        m        0.0  21.7  15.9           5.1  10.0     10.0   NaN   0.0
2020-04-19 03:30:00  YBBN        m        0.0  22.4  16.7           5.1  10.0     10.0   NaN   0.0


storm dates from gpats_daily

>>> gpats_daily.loc[gpats_daily>0].index
DatetimeIndex(['2008-03-28', '2008-04-07', '2008-04-14', '2008-05-14',
               '2008-05-17', '2008-06-03', '2008-06-20', '2008-08-22',
               '2008-09-12', '2008-09-20',
               ...
               '2020-02-11', '2020-02-12', '2020-02-19', '2020-02-27',
               '2020-02-28', '2020-03-06', '2020-04-04', '2020-04-19',
               '2020-05-14', '2020-09-25'],
              dtype='datetime64[ns]', length=371, freq=None)



Lets see if we get same dates from 30min resample gpats data
Note we need to do sum() not count() - as count here would count all rows

gpats_30min=gpats.resample('30min')['AMP'].count()  <-- count works here as we only have rows with gpats


gpats_d=gpats_30min.resample('D').count()   

resample('D') would also introduce days in dataset where we did not even had any gpats atrikes 
in original dataset


would count all rows 24 hours* 2 = 48 given 48 half hour stpes in a day or daily resample
and we get at most 48 strikes for every day - this is wrong 
so better do sum() not count() here OKEEE!!!

>>> gpats_30min.resample('D').count().head()
2008-03-28    39
2008-03-29    48  <--- max 48 hal hourly steps in a day
2008-03-30    48
2008-03-31    48
2008-04-01    48
Freq: D, Name: AMP, Length: 4565, dtype: int64


>>> gpats_30min.resample('D').sum()
2008-03-28    45
2008-03-29     0
2008-03-30     0
2008-03-31     0
2008-04-01     0
              ..
2020-09-21     0
2020-09-22     0
2020-09-23     0
2020-09-24     0
2020-09-25    12
Freq: D, Name: AMP, Length: 4565, dtype: int64

>>> gpats_d=gpats_30min.resample('D').sum()

>>> gpats_d.loc[gpats_d>0].index
DatetimeIndex(['2008-03-28', '2008-04-07', '2008-04-14', '2008-05-14',
               '2008-05-17', '2008-06-03', '2008-06-20', '2008-08-22',
               '2008-09-12', '2008-09-20',
               ...
               '2020-02-11', '2020-02-12', '2020-02-19', '2020-02-27',
               '2020-02-28', '2020-03-06', '2020-04-04', '2020-04-19',
               '2020-05-14', '2020-09-25'],
              dtype='datetime64[ns]', length=371, freq=None)


set symmetric_difference - dates which are not common to both sets (associative) dates which appear in either one of the sets
>>> len(set(gpats_d.loc[gpats_d>0].index).symmetric_difference(set(gpats_daily.loc[gpats_daily>0].index)))
0
Thats is both approach give same TS dates  - great


>>> len(set(gpats_d.loc[gpats_d>0].index).intersection(set(gpats_daily.loc[gpats_daily>0].index)))
371


Note we found 371 days above - below gives only 323 TS days from gpats 

>>> ts_dates = bous.get_storm_dates(gpats,df)
Getting storm dates for: YBBN from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBBN = 323
Storm dates derived from observer reported PW groups for YBBN = 481
Total TS dates found for YBBN = 617



This occurs BECAUSE of
def get_gpats_start_end_duration(gpat):
    # get rid of zero duration events
    daily = daily[daily['duration'] > 0]    <<-------------  

If we remark this  - get 371 dates now
We need duration to be greater than zero to filter out spurios TS days with only 1 or 2 strikes
 
>>> importlib.reload(bous)
<module 'utility_functions_sep2018' from '/mnt/shared/stats-R/flask_projects/avguide/utility_functions_sep2018.py'>
>>> ts_dates = bous.get_storm_dates(gpats,df)
Getting storm dates for: YBBN from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBBN = 371
Storm dates derived from observer reported PW groups for YBBN = 481
Total TS dates found for YBBN = 655


>>> len(ts_dates)
617

>>> ts_dates[:10]
[Timestamp('2004-10-19 00:00:00'), Timestamp('2018-01-13 00:00:00'), Timestamp('2005-02-02 00:00:00'), 
Timestamp('2007-10-10 00:00:00'), Timestamp('2004-09-11 00:00:00'), Timestamp('2011-12-27 00:00:00'), 
Timestamp('2011-06-01 00:00:00'), Timestamp('2004-01-28 00:00:00'), Timestamp('2016-06-12 00:00:00'), 
Timestamp('2005-11-30 00:00:00')]



>>> gpats_ts_stats = bous.get_gpats_start_end_duration(gpats)

>>> gpats_ts_stats
            gpats_cnt  duration               first                last
2008-03-28         45       1.5 2008-03-28 04:37:06 2008-03-28 06:05:34
2008-04-14         37       1.8 2008-04-14 06:37:16 2008-04-14 08:26:17
2008-05-14          7       0.6 2008-05-14 03:03:14 2008-05-14 03:38:29
2008-05-17         62       0.5 2008-05-17 06:40:09 2008-05-17 07:10:42
2008-06-20          4       0.5 2008-06-20 04:16:18 2008-06-20 04:46:39
...               ...       ...                 ...                 ...
2020-02-28         10       1.7 2020-02-28 04:22:00 2020-02-28 06:02:03
2020-03-06         34       2.5 2020-03-06 06:12:32 2020-03-06 08:42:38
2020-04-04          2       0.1 2020-04-04 10:54:10 2020-04-04 10:58:45
2020-04-19         16       1.0 2020-04-19 00:54:08 2020-04-19 01:52:20
2020-09-25         12       1.4 2020-09-25 11:38:29 2020-09-25 13:01:37

[323 rows x 4 columns]
>>> gpats_ts_stats.index
DatetimeIndex(['2008-03-28', '2008-04-14', '2008-05-14', '2008-05-17',
               '2008-06-20', '2008-08-22', '2008-09-20', '2008-09-21',
               '2008-10-15', '2008-10-21',
               ...
               '2020-02-10', '2020-02-11', '2020-02-12', '2020-02-19',
               '2020-02-27', '2020-02-28', '2020-03-06', '2020-04-04',
               '2020-04-19', '2020-09-25'],
              dtype='datetime64[ns]', length=323, freq=None)















NOW run this for loop in python to process all climate zone files and also update/add TS flag to all observations


>>> stations=['YBBN','YBAF','YAMB','YBSU','YBCG','YTWB','YBOK',\
			  'YBRK','YSHL','YSCN','YSRI','YSBK','YSSY','YWLM',\
			  'YBAS','YPTN','YPDN','YPPD','YBRM','YPCC','YPXM']
>>> for sta in stations:
...     bous.merge_aws_gpats_data(sta)


Processing sta=YBBN,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBBN.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBBN_10NM.csv

Processing aviation location YBBN, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBBN.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min QNH  AWS_Flag
date                                                                             ...                                                                                        
2000-01-01  YBBN        M        0.0         0.0  25.0  17.3  62.0  12.1  150.0  ...                                                                           NaN       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 18:30:00  YBBN        m        0.0         0.0  17.7  17.0  96.0  1.9  310.0  ...                                                                            1016.5       1.0

[1 rows x 38 columns]
Reading YBBN gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBBN_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-03-28 04:37:06 -27.26361  152.97498 -16.3
                     LATITUDE  LONGITUDE   AMP
2020-09-25 13:01:37 -27.31754  153.23424 -21.4
Getting storm dates for: YBBN from aws metar/speci and gpats data.
/mnt/shared/stats-R/flask_projects/avguide/utility_functions_sep2018.py:684: FutureWarning: using a dict with renaming is deprecated and will be removed
in a future version.

For column-specific groupby renaming, use named aggregation

    >>> df.groupby(...).agg(name=('column', aggfunc))

  daily.loc[:, ('Time','duration')] = round(\
Storm dates derived from gpats data for YBBN = 323
Storm dates derived from observer reported PW groups for YBBN = 481
Total TS dates found for YBBN = 617




Processing sta=YBAF,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBAF.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBAF_10NM.csv

Processing aviation location YBAF, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBAF.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                             ...                                                                                            
2000-01-01  YBAF        M        0.0         0.0  24.7  15.9  58.0  12.1  120.0  ...                                                                            1010.6       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 18:30:00  YBAF        s        0.0         0.0  16.8  15.7  93.0  0.0   0.0  ...                                                                            1016.4       1.0

[1 rows x 38 columns]
Reading YBAF gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBAF_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-03-28 05:11:18 -27.47399  153.13519 -11.0
                     LATITUDE  LONGITUDE   AMP
2020-09-25 11:50:54 -27.54911  153.17325 -10.6
Getting storm dates for: YBAF from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBAF = 339
Total TS dates found for YBAF = 339



Processing sta=YAMB,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YAMB.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YAMB_10NM.csv

Processing aviation location YAMB, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YAMB.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                           ...                                                                                            
2000-01-01  YAMB        M        0.0         0.0  24.9  16.7  60.0  6.0  90.0  ...                                                                            1010.6       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 18:30:00  YAMB        s        0.0         0.2  14.5  13.9  96.0  0.0   0.0  ...                                                                            1016.2       1.0

[1 rows x 38 columns]
Reading YAMB gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YAMB_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-03-26 05:32:49 -27.54843  152.56316 -4.7
                     LATITUDE  LONGITUDE   AMP
2020-08-15 02:16:29 -27.63288  152.87495 -24.5
Getting storm dates for: YAMB from aws metar/speci and gpats data.
Storm dates derived from gpats data for YAMB = 376
Storm dates derived from observer reported PW groups for YAMB = 284
Total TS dates found for YAMB = 554



Processing sta=YBSU,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBSU.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBSU_10NM.csv

Processing aviation location YBSU, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBSU.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                             ...                                                                                            
2000-01-01  YBSU        M        0.0         0.0  25.2  20.3  74.0  16.9  140.0  ...                                                                            1009.8       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 18:30:00  YBSU        m        0.0         0.0  17.9  17.3  96.0  2.9  330.0  ...                                                                            1016.5       1.0

[1 rows x 38 columns]
Reading YBSU gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBSU_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-03-09 20:58:32 -26.46825   153.0542 -7.4
                     LATITUDE  LONGITUDE   AMP
2020-09-22 15:31:39 -26.47221  153.25255  32.6
Getting storm dates for: YBSU from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBSU = 296
Total TS dates found for YBSU = 296



Processing sta=YBCG,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBCG.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBCG_10NM.csv

Processing aviation location YBCG, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBCG.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                             ...                                                                                            
2000-01-01  YBCG        M        0.0         0.0  23.1  16.2  65.0  14.0  180.0  ...                                                                            1010.8       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 18:30:00  YBCG        m        0.0         0.0  20.2  17.6  85.0  8.0  320.0  ...                                                                            1016.0       1.0

[1 rows x 38 columns]
Reading YBCG gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBCG_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-03-02 13:58:57  -28.1578  153.38339 -6.5
                     LATITUDE  LONGITUDE   AMP
2020-08-15 11:49:33 -28.25612  153.65898 -21.9
Getting storm dates for: YBCG from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBCG = 331
Total TS dates found for YBCG = 331



Processing sta=YTWB,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YTWB.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YTWB_10NM.csv

Processing aviation location YTWB, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YTWB.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YTWB        M        0.0         0.0  20.4  15.4  73.0  16.9  90.0  ...                                                                            1012.9       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td     RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 18:30:00  YTWB        s        0.0         0.0  15.1  15.1  100.0  8.9  70.0  ...                                                                            1018.3       1.0

[1 rows x 38 columns]
Reading YTWB gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YTWB_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-03-26 07:19:25 -27.67275  151.96312 -13.3
                     LATITUDE  LONGITUDE   AMP
2020-08-15 08:15:41 -27.53332  151.90802  17.2
Getting storm dates for: YTWB from aws metar/speci and gpats data.
Storm dates derived from gpats data for YTWB = 372
Total TS dates found for YTWB = 372



Processing sta=YBOK,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBOK.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBOK_10NM.csv

Processing aviation location YBOK, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBOK.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YBOK        M        0.0         0.0  23.8  15.3  59.0  11.1  70.0  ...                                                                            1011.0       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 18:30:00  YBOK        m        0.0         0.0  15.3  14.2  93.0  5.1  90.0  ...                                                                            1016.6       1.0

[1 rows x 38 columns]
Reading YBOK gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBOK_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-03-21 03:32:36 -27.51869   151.5789 -2.5
                     LATITUDE  LONGITUDE   AMP
2020-08-15 07:31:23 -27.48377  151.80341  28.3
Getting storm dates for: YBOK from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBOK = 365
Storm dates derived from observer reported PW groups for YBOK = 298
Total TS dates found for YBOK = 552



Processing sta=YBRK,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBRK.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBRK_10NM.csv

Processing aviation location YBRK, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBRK.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YBRK        M        0.0         0.0  30.1  21.2  59.0  7.0  150.0  ...                                                                            1006.4       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 18:30:00  YBRK        m        0.0         0.0  19.0  17.5  91.0  0.0   0.0  ...                                                                            1016.1       1.0

[1 rows x 38 columns]
Reading YBRK gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBRK_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-04-05 08:42:58 -23.29469  150.31342 -14.5
                     LATITUDE  LONGITUDE   AMP
2020-09-23 16:35:33 -23.23415  150.61125 -91.7
Getting storm dates for: YBRK from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBRK = 222
Total TS dates found for YBRK = 222


Processing sta=YSHL,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSHL.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSHL_10NM.csv

Processing aviation location YSHL, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSHL.txt
              AvID M_type  pptn10min  pptnSince9     T   Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YSHL        M        0.0         0.0  20.1  9.4  50.0  13.0  180.0  ...                                                                            1014.2       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 19:00:00  YSHL        m        0.0         5.2  11.3  11.1  99.0  0.0   0.0  ...                                                                            1011.0       1.0

[1 rows x 38 columns]
Reading YSHL gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSHL_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-02-26 04:03:27 -34.65754  150.67993 -5.1
                     LATITUDE  LONGITUDE   AMP
2020-09-21 11:04:31 -34.41569   150.8206  34.7
Getting storm dates for: YSHL from aws metar/speci and gpats data.
Storm dates derived from gpats data for YSHL = 347
Total TS dates found for YSHL = 347

Processing sta=YSCN,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSCN.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSCN_10NM.csv

Processing aviation location YSCN, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSCN.txt
              AvID M_type  pptn10min  pptnSince9     T   Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                           ...                                                                                            
2000-01-01  YSCN        M        0.0         0.0  19.5  6.6  43.0  8.9  180.0  ...                                                                            1014.0       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td     RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 19:00:00  YSCN        s        0.0         7.2  11.4  11.4  100.0  0.0   0.0  ...                                                                            1011.6       1.0

[1 rows x 38 columns]
Reading YSCN gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSCN_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-02-26 05:06:31 -33.99715  150.57294 -8.0
                     LATITUDE  LONGITUDE   AMP
2020-09-25 06:16:36 -34.11857  150.74467 -18.2
Getting storm dates for: YSCN from aws metar/speci and gpats data.
Storm dates derived from gpats data for YSCN = 335
Total TS dates found for YSCN = 335

Processing sta=YSRI,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSRI.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSRI_10NM.csv

Processing aviation location YSRI, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSRI.txt
              AvID M_type  pptn10min  pptnSince9     T   Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                           ...                                                                                            
2000-01-01  YSRI        M        0.0         0.0  19.6  9.5  52.0  8.9  190.0  ...                                                                            1014.5       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 19:00:00  YSRI        s        0.0         3.0  12.2  12.0  99.0  1.9  210.0  ...                                                                            1011.4       1.0

[1 rows x 38 columns]
Reading YSRI gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSRI_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-02-26 04:08:43 -33.54882  150.61436 -4.3
                     LATITUDE  LONGITUDE   AMP
2020-09-04 10:51:09   -33.456  150.92389  23.9
Getting storm dates for: YSRI from aws metar/speci and gpats data.
Storm dates derived from gpats data for YSRI = 386
Total TS dates found for YSRI = 386

Processing sta=YSBK,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSBK.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSBK_10NM.csv

Processing aviation location YSBK, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSBK.txt
              AvID M_type  pptn10min  pptnSince9     T   Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YSBK        M        0.0         0.0  20.1  9.9  52.0  12.1  160.0  ...                                                                            1014.2       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 19:00:00  YSBK        m        0.0         9.0  11.9  11.1  95.0  0.0   0.0  ...                                                                            1011.3       1.0

[1 rows x 38 columns]
Reading YSBK gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSBK_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-02-26 05:51:57 -34.05914   150.9171 -20.0
                     LATITUDE  LONGITUDE   AMP
2020-09-21 11:28:29 -34.05155  151.14589  13.7
Getting storm dates for: YSBK from aws metar/speci and gpats data.
Storm dates derived from gpats data for YSBK = 346
Total TS dates found for YSBK = 346

Processing sta=YSSY,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSSY.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSSY_10NM.csv

Processing aviation location YSSY, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YSSY.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                             ...                                                                                            
2000-01-01  YSSY        M        0.0         0.0  18.7  12.0  65.0  19.0  180.0  ...                                                                            1013.8       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 18:30:00  YSSY        m        0.0         8.6  16.1  14.3  89.0  4.1  310.0  ...                                                                            1011.3       2.0

[1 rows x 38 columns]
Reading YSSY gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YSSY_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-02-26 06:13:58 -34.09302   151.0611 -10.4
                     LATITUDE  LONGITUDE   AMP
2020-09-21 11:28:29 -34.05155  151.14589  13.7
Getting storm dates for: YSSY from aws metar/speci and gpats data.
Storm dates derived from gpats data for YSSY = 374
Storm dates derived from observer reported PW groups for YSSY = 546
Total TS dates found for YSSY = 711

Processing sta=YWLM,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YWLM.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YWLM_10NM.csv

Processing aviation location YWLM, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YWLM.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                             ...                                                                                            
2000-01-01  YWLM        M        0.0         1.4  18.5  12.2  67.0  16.9  190.0  ...                                                                            1013.2       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td     RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                      ...                                                                                            
2020-09-20 19:08:00  YWLM        s        0.0         8.6  12.8  12.8  100.0  4.1  330.0  ...                                                                            1012.6       1.0

[1 rows x 38 columns]
Reading YWLM gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YWLM_10NM.csv
                     LATITUDE  LONGITUDE  AMP
2008-02-26 05:46:07 -32.64296  151.98978 -4.0
                     LATITUDE  LONGITUDE   AMP
2020-09-21 13:09:30 -32.72565  151.85527  22.3
Getting storm dates for: YWLM from aws metar/speci and gpats data.
Storm dates derived from gpats data for YWLM = 332
Total TS dates found for YWLM = 332

Processing sta=YBAS,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBAS.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBAS_10NM.csv

Processing aviation location YBAS, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBAS.txt
              AvID M_type  pptn10min  pptnSince9     T   Td    RH   WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                          ...                                                                                            
2000-01-01  YBAS        M        0.0         0.0  25.8  1.2  20.0  8.9  80.0  ...                                                                            1015.3       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T   Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 19:00:00  YBAS        m        0.0         0.0  20.2 -0.4  25.0  2.9  270.0  ...                                                                            1012.4       1.0

[1 rows x 38 columns]
Reading YBAS gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBAS_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-02-29 05:37:10 -23.67583  133.87646 -38.7
                     LATITUDE  LONGITUDE   AMP
2020-08-05 12:49:28 -23.94582  133.97607  24.9
Getting storm dates for: YBAS from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBAS = 253
Total TS dates found for YBAS = 253

Processing sta=YPTN,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPTN.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPTN_10NM.csv

Processing aviation location YPTN, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPTN.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YPTN        M        0.0         0.0  25.9  21.9  79.0  4.1  100.0  ...                                                                            1009.7       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 19:00:00  YPTN        m        0.0         0.0  27.2  21.2  70.0  8.0  320.0  ...                                                                            1011.1       1.0

[1 rows x 38 columns]
Reading YPTN gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPTN_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-02-28 04:56:23 -14.64317   132.2849 -29.0
                     LATITUDE  LONGITUDE   AMP
2020-09-21 18:25:32 -14.40296  132.34995  34.8
Getting storm dates for: YPTN from aws metar/speci and gpats data.
Storm dates derived from gpats data for YPTN = 631
Total TS dates found for YPTN = 631

Processing sta=YPDN,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPDN.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPDN_10NM.csv

Processing aviation location YPDN, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPDN.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YPDN        M        0.0         0.0  25.7  20.7  74.0  5.1  140.0  ...                                                                            1007.2       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 19:00:00  YPDN        m        0.0        17.6  26.4  24.4  89.0  2.9  360.0  ...                                                                            1010.6       1.0

[1 rows x 38 columns]
Reading YPDN gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPDN_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-03-02 06:39:55 -12.52191  130.99374 -73.4
                     LATITUDE  LONGITUDE   AMP
2020-09-27 06:52:11 -12.49969  130.72334 -50.9
Getting storm dates for: YPDN from aws metar/speci and gpats data.
Storm dates derived from gpats data for YPDN = 1008
Storm dates derived from observer reported PW groups for YPDN = 1426
Total TS dates found for YPDN = 1855

Processing sta=YPPD,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPPD.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPPD_10NM.csv

Processing aviation location YPPD, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPPD.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS  WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YPPD        M        0.0         0.0  30.0  20.5  57.0  15.0  90.0  ...                                                                            1005.9       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T   Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                    ...                                                                                            
2020-09-20 19:00:00  YPPD        m        0.0         0.0  19.8  3.1  33.0  8.9  160.0  ...                                                                            1012.7       1.0

[1 rows x 38 columns]
Reading YPPD gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPPD_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-03-08 07:00:39  -20.5083  118.64297 -43.5
                     LATITUDE  LONGITUDE   AMP
2020-04-19 13:01:29 -20.52752  118.52028 -54.5
Getting storm dates for: YPPD from aws metar/speci and gpats data.
Storm dates derived from gpats data for YPPD = 178
Total TS dates found for YPPD = 178

Processing sta=YBRM,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBRM.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBRM_10NM.csv

Processing aviation location YBRM, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YBRM.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YBRM        M        0.0         0.0  29.8  21.9  63.0  9.9  100.0  ...                                                                            1006.0       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                     ...                                                                                            
2020-09-20 19:00:00  YBRM        m        0.0         0.0  26.0  21.8  78.0  7.0  220.0  ...                                                                            1010.9       1.0

[1 rows x 38 columns]
Reading YBRM gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YBRM_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2008-03-05 07:59:38 -18.07514  122.25332 -45.6
                     LATITUDE  LONGITUDE   AMP
2020-05-25 18:32:47 -17.89541  122.36571  47.3
Getting storm dates for: YBRM from aws metar/speci and gpats data.
Storm dates derived from gpats data for YBRM = 451
Total TS dates found for YBRM = 451

Processing sta=YPCC,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPCC.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPCC_10NM.csv

Processing aviation location YPCC, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPCC.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                             ...                                                                                            
2000-01-01  YPCC        M        0.0         0.0  26.2  21.6  76.0  13.0  100.0  ...                                                                            1008.5       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                      ...                                                                                            
2020-09-20 19:00:00  YPCC        m        0.0         NaN  26.4  23.3  83.0  12.1  110.0  ...                                                                            1011.7       1.0

[1 rows x 38 columns]
Reading YPCC gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPCC_10NM.csv
                     LATITUDE  LONGITUDE   AMP
2011-12-26 05:30:14 -12.06473   96.76108 -67.1
                     LATITUDE  LONGITUDE   AMP
2016-05-18 03:17:57 -12.28911   96.92107 -50.7
Getting storm dates for: YPCC from aws metar/speci and gpats data.
Storm dates derived from gpats data for YPCC = 18
Total TS dates found for YPCC = 18

Processing sta=YPXM,
Climate Zone data file:/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPXM.txt",        
Gpats file: /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPXM_10NM.csv

Processing aviation location YPXM, file=/home/bou/shared/stats-R/flask_projects/avguide/app/data/HM01X_Data_YPXM.txt
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                            ...                                                                                            
2000-01-01  YPXM        M        0.2        39.2  23.8  23.5  98.0  7.0  320.0  ...                                                                            1006.4       NaN

[1 rows x 38 columns]
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH    WS   WDir  ...  PW2_desc  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag
date                                                                                      ...                                                                                            
2020-09-20 19:00:00  YPXM        s        0.0         1.4  24.1  22.5  91.0  14.0  130.0  ...                                                                            1013.4       1.0

[1 rows x 38 columns]
Reading YPXM gpats file /home/bou/shared/stats-R/flask_projects/avguide/app/data/gpats_YPXM_10NM.csv
                     LATITUDE  LONGITUDE    AMP
2008-03-20 11:45:23 -10.54257  105.63374 -127.7
                     LATITUDE  LONGITUDE   AMP
2020-08-29 16:17:55 -10.61269  105.66137 -63.4
Getting storm dates for: YPXM from aws metar/speci and gpats data.
Storm dates derived from gpats data for YPXM = 107
Total TS dates found for YPXM = 107






Now when processing TS predictions get error
Processing TS prediction for station:YBBN for 2020-10-02
[2020-10-02 15:40:43,160] ERROR in app: Exception on /thunderstorm_predictions/ [POST]


    raise ValueError("cannot reindex from a duplicate axis")
ValueError: cannot reindex from a duplicate axis

 File "./app/__init__.py", line 1605, in storm_predict
    storm_predictions = bous.get_ts_predictions_stations(stations,sonde_data)
  File "./utility_functions_sep2018.py", line 3046, in get_ts_predictions_stations
    left = df.resample('D')['AvID','Td','QNH','any_ts','AMP'].first(),

3036    for station in stations:
        print("\n\nProcessing TS prediction for station:{} for {}"\
            .format(station,day.strftime("%Y-%m-%d")))
        # get station aws archive data
        df  = pickle.load(
                open(
                os.path.join('app','data', station+'_aws.pkl'), 'rb'))
        # print(df.tail(1))
        # merge with closest radiosonde upper data archive
        aws_sonde_daily = pd.merge(
3046        left = df.resample('D')['AvID','Td','QNH','any_ts','AMP'].first(),
        right=sonde_data[['wdir500','wspd500','T500', 'tmp_rate850_500']],
        left_index=True, right_index=True,how='left')\
        .rename(columns={'QNH': 'P','any_ts':'TS'})

[pid: 36251|app: 0|req: 9/9] 127.0.0.1 () {60 vars in 1403 bytes} [Fri Oct  2 15:40:42 2020] POST /thunderstorm_predictions/ => generated 290 bytes in 777 msecs (HTTP/1.1 500) 4 headers in 586 bytes (1 switches on core 0)

=====================================================================

FIX

with old data and 

df.resample('D')[['AvID','Td','QNH','any_ts','AMP']].first()

0 	YBBN 	2020-10-03 	1420 	NaN 	NaN 	-100.00 	inconclusive
1 	YBAF 	2020-10-03 	1420 	NaN 	NaN 	-100.00 	inconclusive
2 	YAMB 	2020-10-03 	1420 	21.0 	2.0 	9.52 	slight chance fog (5% to 10% chance)
3 	YBSU 	2020-10-03 	1420 	17.0 	1.0 	5.88 	slight chance fog (5% to 10% chance)
4 	YBCG 	2020-10-03 	1420 	24.0 	1.0 	4.17 	fog unlikely
5 	YBOK 	2020-10-03 	1420 	NaN 	NaN 	-100.00 	inconclusive
6 	YTWB 	2020-10-03 	1420 	18.0 	3.0 	16.67 	fog possible (PROB10 to PROB30)


 	Airport 	Date 	Days Searched 	Days match 	TS Days 	TS Chance 	Prediction 	TS Stats
0 	YBBN 	2020-10-03 	1785 	9 	1 	11.11 	LIKELY 	YBBN
1 	YBAF 	2020-10-03 	1785 	9 	0 	0.00 	NO CHANCE 	YBAF
2 	YAMB 	2020-10-03 	1785 	9 	0 	0.00 	NO CHANCE 	YAMB
3 	YBSU 	2020-10-03 	1785 	8 	0 	0.00 	NO CHANCE 	YBSU
4 	YBCG 	2020-10-03 	1785 	9 	0 	0.00 	NO CHANCE 	YBCG
5 	YBOK 	2020-10-03 	1785 	9 	0 	0.00 	NO CHANCE 	YBOK
6 	YTWB 	2020-10-03 	1785 	9 	0 	0.00 	NO CHANCE 	YTWB


with new data and
df.resample('D')[['AvID','Td','QNH','any_ts','AMP']].first()

 	sta 	date 	synop_days 	matches 	fog_cnt 	prob 	pred
0 	YBBN 	2020-10-03 	1443 	NaN 	NaN 	-100.00 	inconclusive
1 	YBAF 	2020-10-03 	1443 	NaN 	NaN 	-100.00 	inconclusive
2 	YAMB 	2020-10-03 	1443 	21.0 	2.0 	9.52 	slight chance fog (5% to 10% chance)
3 	YBSU 	2020-10-03 	1443 	17.0 	1.0 	5.88 	slight chance fog (5% to 10% chance)
4 	YBCG 	2020-10-03 	1443 	24.0 	1.0 	4.17 	fog unlikely
5 	YBOK 	2020-10-03 	1443 	NaN 	NaN 	-100.00 	inconclusive
6 	YTWB 	2020-10-03 	1443 	18.0 	3.0 	16.67 	fog possible (PROB10 to PROB30)

fogger works - more matches cause longer data record 3 to 4 months more

ts predictions fails for SE QLD Tafs
  File "./app/__init__.py", line 1605, in storm_predict
    storm_predictions = bous.get_ts_predictions_stations(stations,sonde_data)
  File "./utility_functions_sep2018.py", line 3047, in get_ts_predictions_stations
    # merge with closest radiosonde upper data archive


Also for sinegle station TS predictions

 File "./app/__init__.py", line 1763, in results_TS_station
    left=df.resample('D')[['AvID', 'WDir', 'WS','T', 'Td', 'QNH', 'any_ts', 'AMP']].first(),

    raise ValueError("cannot reindex from a duplicate axis")
ValueError: cannot reindex from a duplicate axis


However with change to code

from df.resample('D")[['AvID', 'WDir', 'WS','T', 'Td', 'QNH', 'any_ts', 'AMP']].first(),

to

left=df.between_time('00:00', '01:00').head(1)[['AvID', 'WDir', 'WS','T', 'Td', 'QNH', 'any_ts', 'AMP']],

Not getting any days matching even season 
so definitely our code before is all good
guessing problem is with new tcz data



 	Airport 	Date 	Days Searched 	Days match 	TS Days 	TS Chance 	Prediction 	TS Stats
0 	YBBN 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YBBN
1 	YBAF 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YBAF
2 	YAMB 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YAMB
3 	YBSU 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YBSU
4 	YBCG 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YBCG
5 	YBOK 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YBOK
6 	YTWB 	2020-10-03 	0 	0 	0 	0.0 	NO CHANCE 	YTWB


BUT  how does fog predictions still work 
how come/??   - so must be some issue with aws data when we created ts flag maybe ???



 	sta 	date 	synop_days 	matches 	fog_cnt 	prob 	pred
0 	YBBN 	2020-10-03 	1443 	NaN 	NaN 	-100.00 	inconclusive
1 	YBAF 	2020-10-03 	1443 	NaN 	NaN 	-100.00 	inconclusive
2 	YAMB 	2020-10-03 	1443 	21.0 	2.0 	9.52 	slight chance fog (5% to 10% chance)
3 	YBSU 	2020-10-03 	1443 	17.0 	1.0 	5.88 	slight chance fog (5% to 10% chance)
4 	YBCG 	2020-10-03 	1443 	24.0 	1.0 	4.17 	fog unlikely
5 	YBOK 	2020-10-03 	1443 	NaN 	NaN 	-100.00 	inconclusive
6 	YTWB 	2020-10-03 	1443 	18.0 	3.0 	16.67 	fog possible (PROB10 to PROB30)



This is how fog preds/pattern match done 
Note here we have fog stats data stored seperately in a file
which we grab here

fg_aut=get_fog_data_vins(station=station,auto_obs='Yes')
and then just grab fog flag from here to mergr with sonde data 
fg_dates = fg_aut.index.date
sonde_data = pd.merge(left=snd, right=fg_aut[['fogflag']],how='left',\
                      left_index=True,right_index=True)


and we use .between_time().resample('D').firsrt()
to grab the synop times we want to match against


df_temp = df.between_time('16:45','17:15').resample('D').first()
df_temp = df_temp.loc[:df.index.date[-1]] 


SHOW that  ******************************************
df.between_time('16:45','17:15').resample('D').first()

GIVES SAME RESULT AS

df.between_time('16:45','17:15').head(1)



>>> df.loc['2000-01-01':'2000-01-05'].between_time('16:45','17:15')
                       AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  ...  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag       date  any_ts
date                                                                                     ...                                                                                           
2000-01-01 17:00:00  YBBN        M        0.0         0.0  19.2  16.7  85.0  6.0  210.0  ...                                                           NaN       NaN 2000-01-01   False
2000-01-02 17:00:00  YBBN        M        0.0         0.0  22.6  18.3  77.0  6.0  170.0  ...                                                           NaN       NaN 2000-01-02   False
2000-01-03 17:00:00  YBBN        M        0.0         1.0  20.9  18.4  86.0  5.1  200.0  ...                                                        1016.9       NaN 2000-01-03   False
2000-01-04 17:00:00  YBBN        M        0.0         0.0  22.2  19.6  85.0  4.1  120.0  ...                                                        1017.6       NaN 2000-01-04   False
2000-01-05 17:00:00  YBBN        M        0.0        10.4  22.0  20.9  93.0  9.9  120.0  ...                                                        1016.1       NaN 2000-01-05   False

[5 rows x 40 columns]


>>> df.loc['2000-01-01':'2000-01-05'].between_time('16:45','17:15').resample('D').first()
              AvID M_type  pptn10min  pptnSince9     T    Td    RH   WS   WDir  MaxGust10min  ...  PW2_type  PW3_desc  PW3_type  AWS_PW  AWS_PW15min  AWS_PW60min     QNH  AWS_Flag       date  any_ts
date                                                                                          ...                                                                                                     
2000-01-01  YBBN        M        0.0         0.0  19.2  16.7  85.0  6.0  210.0           8.9  ...                                                                     NaN       NaN 2000-01-01   False
2000-01-02  YBBN        M        0.0         0.0  22.6  18.3  77.0  6.0  170.0           8.0  ...                                                                     NaN       NaN 2000-01-02   False
2000-01-03  YBBN        M        0.0         1.0  20.9  18.4  86.0  5.1  200.0           8.0  ...                                                                  1016.9       NaN 2000-01-03   False
2000-01-04  YBBN        M        0.0         0.0  22.2  19.6  85.0  4.1  120.0           5.1  ...                                                                  1017.6       NaN 2000-01-04   False
2000-01-05  YBBN        M        0.0        10.4  22.0  20.9  93.0  9.9  120.0          18.1  ...                                                                  1016.1       NaN 2000-01-05   False

[5 rows x 40 columns]
>>> 


both these give same result - So problem not with code but how we creating our df with ts flag in 1st place
as 

ValueError: cannot reindex from a duplicate axis

has more to do with index of our df


FIXED BOTH functions 

def get_ts_predictions_stations(stations,sonde2day=None,my_date=None):

AND


@app.route('/results_TS_station/<string:station>/', methods=['GET','POST'])
def results_TS_station(station):


with this same code now and all works okay


        aws_sonde_daily = pd.merge(
        #left = df.resample('D')[['AvID','Td','QNH','any_ts','AMP']].first(),
        left = df.between_time('00:00', '00:45')[['AvID','Td','QNH','any_ts']].resample('D').first(),
        right=sonde_data[['wdir500','wspd500','T500', 'tmp_rate850_500']],
        left_index=True, right_index=True,how='left')\
        .rename(columns={'QNH': 'P','any_ts':'TS'})

    # now do predictions for all stations
    for station in stations:
        print("\n\nProcessing FG prediction for station:{}"\
            .format(station))
        # get station aws archive data for station
        df  = pickle.load(
                open(
                os.path.join('app','data', station+'_aws.pkl'), 'rb'))

        fg_aut=get_fog_data_vins(station=station,auto_obs='Yes')
        fg_dates = fg_aut.index.date
        sonde_data = pd.merge(left=snd, right=fg_aut[['fogflag']],how='left',\
            left_index=True,right_index=True)

        df_temp = df.between_time('16:45','17:15').resample('D').first()
        df_temp = df_temp.loc[:df.index.date[-1]]  # resample introduces days for rest of 2020!!
        # note the above keeps 'fogflag' as bool but introduces some NaN which can make filtering paninful
        # df_temp['fogflag'] = df_temp['fogflag'].astype(bool)  # force to be boolean converts NaN to False
        # Now this would work to filter fog days only ==>  df_temp[df_temp['fogflag']]

        aws_sonde_daily = pd.merge(
            left=df_temp[['AvID', 'T', 'Td', 'WS', 'WDir', 'QNH']], \
            right=sonde_data[['900_wdir', '900_WS','lr_sfc_850','fogflag']], \
            # right=df_winds[['Station', 'level','900_wdir','900_WS']],\  #if using EC data file
            left_index=True, right_index=True, how='left')



==============================================================================================

NOW moving to fog stats fog stats for each station 
Just run the tcz file though fogger checker scripts



-------------------------------------------
Create fog Stats using tcz data file
USING ./aws_format_v6.5_2020.pl
-------------------------------------------

Only two liner to get all fog stats
AWK to extract only certain columns 

(py37) [bou@bous-fed31 data]$ pwd
/home/bou/shared/stats-R/flask_projects/avguide/app/data


[bou@bous-fed31 data]$ ls -ltr HM01X_Data_*
-rw-r--r--. 1 bou bou 515531150 Oct  2 12:55 HM01X_Data_YBRM.txt
-rw-r--r--. 1 bou bou 549146071 Oct  2 12:55 HM01X_Data_YPPD.txt
-rw-r--r--. 1 bou bou 523235749 Oct  2 12:55 HM01X_Data_YPDN.txt
-rw-r--r--. 1 bou bou 509768237 Oct  2 12:55 HM01X_Data_YPTN.txt
-rw-r--r--. 1 bou bou 516265842 Oct  2 12:55 HM01X_Data_YBAS.txt
-rw-r--r--. 1 bou bou 524245260 Oct  2 12:55 HM01X_Data_YBRK.txt
-rw-r--r--. 1 bou bou 458277652 Oct  2 12:55 HM01X_Data_YAMB.txt
-rw-r--r--. 1 bou bou 520117451 Oct  2 12:55 HM01X_Data_YBAF.txt
-rw-r--r--. 1 bou bou 530291278 Oct  2 12:55 HM01X_Data_YBCG.txt
-rw-r--r--. 1 bou bou 513836663 Oct  2 12:55 HM01X_Data_YBBN.txt
-rw-r--r--. 1 bou bou 538421225 Oct  2 12:55 HM01X_Data_YBSU.txt
-rw-r--r--. 1 bou bou 450096608 Oct  2 12:55 HM01X_Data_YBOK.txt
-rw-r--r--. 1 bou bou 448918615 Oct  2 12:55 HM01X_Data_YTWB.txt
-rw-r--r--. 1 bou bou 534907961 Oct  2 12:55 HM01X_Data_YWLM.txt
-rw-r--r--. 1 bou bou 519501525 Oct  2 12:55 HM01X_Data_YSSY.txt
-rw-r--r--. 1 bou bou 528745939 Oct  2 12:55 HM01X_Data_YSBK.txt
-rw-r--r--. 1 bou bou 545682523 Oct  2 12:55 HM01X_Data_YSRI.txt
-rw-r--r--. 1 bou bou 511857690 Oct  2 12:55 HM01X_Data_YSCN.txt
-rw-r--r--. 1 bou bou 456722646 Oct  2 12:55 HM01X_Data_YSHL.txt
-rw-r--r--. 1 bou bou 454200940 Oct  2 12:55 HM01X_Data_YPCC.txt
-rw-r--r--. 1 bou bou 406702826 Oct  2 12:55 HM01X_Data_YPXM.txt

for each data file we need to run these commands

 1013  cd app/data

 1016  awk -F , '{print $6 "," $8 "," $9 "," $65 "," $19  "," $18 "," $20 "," $40 "," $41 "," $42 "," $43 "," $45$46 "," $48$49 "," $51$52 ","$59 ","$60 ","$61 ","  $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $39 "," $12 "," $14 "," $15 "," $64 ","$10 "," $11 } ' HM01X_Data_YBBN.txt > YBBN1_newbie.txt

 1017  ./aws_format_v6.5_2020.pl YBBN1_newbie.txt YBBN2_newbie.csv > YBBN3_newbie.txt

 1018  head -n 1 YBBN2_newbie.csv > YBBN_fog_days_newbie.csv; awk -F , '$8 ~ /YES/ {print $0}' YBBN2_newbie.csv  >> YBBN_fog_days_newbie.csv;



for each data file run 

awk -F , '{print $6  ,..................} ' file > file1.txt


(py37) [bou@bous-fed31 data]$ awk -F , '{print $6 "," $8 "," $9 "," $65 "," $19  "," $18 "," $20 "," $40 "," $41 "," $42 "," $43 "," $45$46 "," $48$49 "," $51$52 ","$59 ","$60 ","$61 ","  $21 "," $23 ","  $24 "," $26 ","  $27 "," $29 ","  $33 "," $34 "," $35 "," $36 "," $37 "," $38 "," $39 "," $12 "," $14 "," $15 "," $64 ","$10 "," $11 } ' HM01X_Data_040842_newbie.txt > YBBN1_newbie.txt


Then for each of the _1.txt files
run
./aws_format_v6.5_2020.pl _1,txt _2.csv > _3.txt

[bou@bous-fed31 data]$ ./aws_format_v6.5_2020.pl YBBN1_newbie.txt YBBN2_newbie.csv > YBBN3_newbie.txt

finally

head -n 1 _2.csv > _3.csv; awk -F , '$8 ~ /YES/ {print $0}' _2.csv  >> _3.csv;


To get fog only days
[bou@bous-fed31 fog_climatology_2019]$ head -n 1 YBBN2_newbie.csv > YBBN_fog_days_newbie.csv; awk -F , '$8 ~ /YES/ {print $0}' YBBN2_newbie.csv  >> YBBN_fog_days_newbie.csv;


-rw-rw-r--. 1 bou bou       274 Sep 23 16:38 YBBN_fog_days_newbie.csv



cd /home/bou/shared/stats-R/flask_projects/avguide/app/data

(py37) [bou@bous-fed31 data]$ ls -lr *pl
-rwxrwxr-x. 1 bou bou 49048 Jun  2 09:59 aws_format_v6.5_2020.pl


++++++++++++++++++++++++++++
check_fog_manual_obs_first()
++++++++++++++++++++++++++++
./aws_format_v6.5_2020.pl YBBN1.txt YBBN2manual.csv > YBBN3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YAMB1.txt YAMB2manual.csv > YAMB3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBOK1.txt YBOK2manual.csv > YBOK3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBAF1.txt YBAF2manual.csv > YBAF3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YTWB1.txt YTWB2manual.csv > YTWB3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBCG1.txt YBCG2manual.csv > YBCG3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBSU1.txt YBSU2manual.csv > YBSU3manual.txt;sleep 10;./aws_format_v6.5_2020.pl YBRK1.txt YBRK2manual.csv > YBRK3manual.txt;


head -n 1  YBBN2manual.csv > YBBN2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBBN2manual.csv >> YBBN2manual_fog_days.csv;sleep 2;head -n 1  YAMB2manual.csv > YAMB2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YAMB2manual.csv >> YAMB2manual_fog_days.csv;sleep 2;head -n 1  YBOK2manual.csv > YBOK2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBOK2manual.csv >> YBOK2manual_fog_days.csv;sleep 2;head -n 1  YBAF2manual.csv > YBAF2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBAF2manual.csv >> YBAF2manual_fog_days.csv;sleep 2;head -n 1  YTWB2manual.csv > YTWB2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YTWB2manual.csv >> YTWB2manual_fog_days.csv;sleep 2;head -n 1  YBCG2manual.csv > YBCG2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBCG2manual.csv >> YBCG2manual_fog_days.csv;sleep 2;head -n 1  YBSU2manual.csv > YBSU2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBSU2manual.csv >> YBSU2manual_fog_days.csv;sleep 2;head -n 1  YBRK2manual.csv > YBRK2manual_fog_days.csv ; awk -F , '$8 ~ /YES/ {print $0}' YBRK2manual.csv >> YBRK2manual_fog_days.csv;



++++++++++++++++++++++++++++
check_fog_auto_obs_first()
++++++++++++++++++++++++++
./aws_format_v6.5_2020.pl YBBN1.txt YBBN2auto.csv > YBBN3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YAMB1.txt YAMB2auto.csv > YAMB3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YBOK1.txt YBOK2auto.csv > YBOK3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YBAF1.txt YBAF2auto.csv > YBAF3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YTWB1.txt YTWB2auto.csv > YTWB3auto.txt;sleep 10; ./aws_format_v6.5_2020.pl YBCG1.txt YBCG2auto.csv > YBCG3auto.txt;sleep 10;./aws_format_v6.5_2020.pl YBSU1.txt YBSU2auto.csv > YBSU3auto.txt;sleep 10;./aws_format_v6.5_2020.pl YBRK1.txt YBRK2auto.csv > YBRK3auto.txt;



head -n 1  YBBN2auto.csv > YBBN2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBBN2auto.csv >> YBBN2auto_fog_days.csv;sleep 2;head -n 1  YAMB2auto.csv > YAMB2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YAMB2auto.csv >> YAMB2auto_fog_days.csv;sleep 2;head -n 1  YBOK2auto.csv > YBOK2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBOK2auto.csv >> YBOK2auto_fog_days.csv;sleep 2;head -n 1  YBAF2auto.csv > YBAF2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBAF2auto.csv >> YBAF2auto_fog_days.csv;sleep 2;head -n 1  YTWB2auto.csv > YTWB2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YTWB2auto.csv >> YTWB2auto_fog_days.csv;sleep 2;head -n 1  YBCG2auto.csv > YBCG2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBCG2auto.csv >> YBCG2auto_fog_days.csv;sleep 2;head -n 1  YBSU2auto.csv > YBSU2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBSU2auto.csv >> YBSU2auto_fog_days.csv;sleep 2;head -n 1  YBRK2auto.csv > YBRK2auto_fog_days.csv ;awk -F , '$8 ~ /YES/ {print $0}' YBRK2auto.csv >> YBRK2auto_fog_days.csv;


Retrive data for given synoptic hour e.g 3pm/0500Z or 12am / 1400

Now how to get 18:00Z - 4am obs for all dates as we want to use these dates for synop matching


>>> df.index[0],df.index[-1]
(Timestamp('2000-01-01 00:00:00'), Timestamp('2020-03-29 18:00:00'))


>>> dates = pd.date_range(start=df.index[0], end=df.index[-1], freq='D')

>>> dates[0],dates[-1]
(Timestamp('2000-01-01 00:00:00', freq='D'), Timestamp('2020-03-29 00:00:00', freq='D'))

>>> len(dates)
7394
>>> # so 7394 days in dataset

so we wat to grab 1800 data for each day


>>> df.between_time('17:45','18:15').resample('D').first().loc[df.index[0]:df.index[-1]].shape
(7394, 38)

this is great - we get a row for each day 

>>> df.between_time('17:50','18:10').resample('D').first().loc[df.index[0]:df.index[-1]].shape
(7394, 38)
>>> df.between_time('17:55','18:05').resample('D').first().loc[df.index[0]:df.index[-1]].shape
(7394, 38)
>>> df.between_time('17:59','18:01').resample('D').first().loc[df.index[0]:df.index[-1]].shape
(7394, 38)

Getting same number of dates due resample('D') introducing day for each day regardless of any data for those days

We can find how many days with missing obs near 18:00 by counting NaNs
Count total NaN at each column in DataFrame df.isnull().sum()
Great article

https://thispointer.com/python-pandas-count-number-of-nan-or-missing-values-in-dataframe-also-row-column-wise/

>>> dat = df[['WDir','WS','T','Td','RH','pptnSince9','vis','vis_aws','PW','QNH']]

>>> dat.between_time('17:59','18:01').resample('D').first().loc[dat.index[0]:dat.index[-1]].isnull().sum()
WDir            66
WS              65
T               65
Td              65
RH              65
pptnSince9     139
vis            137
vis_aws        308
PW            6670
QNH            103

>>> dat.between_time('17:55','18:05').resample('D').first().loc[dat.index[0]:dat.index[-1]].isnull().sum()
WDir            59
WS              58
T               58
Td              58
RH              58
pptnSince9     132
vis            130
vis_aws        301
PW            6665
QNH             96

>>> dat.between_time('17:50','18:10').resample('D').first().loc[dat.index[0]:dat.index[-1]].isnull().sum()
WDir            57
WS              56
T               56
Td              56
RH              56
pptnSince9     130
vis            127
vis_aws        298
PW            6651
QNH             94

>>> dat.between_time('17:45','18:15').resample('D').first().loc[dat.index[0]:dat.index[-1]].isnull().sum()
WDir            57
WS              56
T               56
Td              56
RH              56
pptnSince9     130
vis            126
vis_aws        297
PW            6641
QNH             94

Note little gains by making time window larger beyond 10min either wise of 1800.



>>> dat_1800 = dat.between_time('17:50','18:10').resample('D').first().loc[df.index[0]:df.index[-1]]
>>> dat_1800.shape
(7394, 10)


>>> dat_1800.isnull().sum()
WDir            57
WS              56
T               56
Td              56
RH              56
pptnSince9     130
vis            127
vis_aws        298
PW            6651
QNH             94
dtype: int64


>>> dat_1800.head()
             WDir   WS     T    Td    RH  pptnSince9   vis  vis_aws    PW     QNH
date                                                                             
2000-01-01  200.0  5.1  18.9  16.4  85.0         0.0  10.0      NaN   NaN     NaN
2000-01-02  180.0  7.0  21.9  18.3  80.0         0.0  10.0      NaN   NaN     NaN
2000-01-03  150.0  2.9  20.6  18.5  88.0         1.0  10.0      NaN   NaN  1016.9
2000-01-04  140.0  4.1  21.7  19.4  87.0         0.0  10.0      NaN   NaN  1017.8
2000-01-05  110.0  9.9  22.1  20.7  92.0        10.4   8.0      NaN  80.0  1016.3

>>> dat_1800.tail()
             WDir   WS     T    Td    RH  pptnSince9   vis  vis_aws  PW     QNH
date                                                                           
2020-03-25  210.0  6.0  17.6  16.4  93.0         0.0  10.0     10.0 NaN  1018.8
2020-03-26  210.0  4.1  19.2  17.7  91.0         5.8  10.0     10.0 NaN  1019.3
2020-03-27  210.0  8.0  18.0  15.3  84.0         0.0  10.0     10.0 NaN  1019.0
2020-03-28  210.0  5.1  19.7  18.9  95.0         0.0  10.0     10.0 NaN  1017.2
2020-03-29  190.0  5.1  20.0  18.9  93.0         0.0  10.0     10.0 NaN  1015.9

get fog dates into 1800Z dataframe

>>> fg_dates = bous.get_fog_data_vins(station = 'YBBN',get_dates_only='Yes')
Getting fog data for YBBN derived using VINS auto obs matching

>>> dat_1800['any_fg'] = dat_1800.index.isin(fg_dates)

>>> dat_1800['any_fg'].sum()
173

>>> dat_1800.head()
             WDir   WS     T    Td    RH  pptnSince9   vis  vis_aws    PW     QNH  any_fg
date                                                                                     
2000-01-01  200.0  5.1  18.9  16.4  85.0         0.0  10.0      NaN   NaN     NaN   False
2000-01-02  180.0  7.0  21.9  18.3  80.0         0.0  10.0      NaN   NaN     NaN   False
2000-01-03  150.0  2.9  20.6  18.5  88.0         1.0  10.0      NaN   NaN  1016.9   False
2000-01-04  140.0  4.1  21.7  19.4  87.0         0.0  10.0      NaN   NaN  1017.8   False
2000-01-05  110.0  9.9  22.1  20.7  92.0        10.4   8.0      NaN  80.0  1016.3   False

>>> dat_1800.tail()
             WDir   WS     T    Td    RH  pptnSince9   vis  vis_aws  PW     QNH  any_fg
date                                                                                   
2020-03-25  210.0  6.0  17.6  16.4  93.0         0.0  10.0     10.0 NaN  1018.8   False
2020-03-26  210.0  4.1  19.2  17.7  91.0         5.8  10.0     10.0 NaN  1019.3   False
2020-03-27  210.0  8.0  18.0  15.3  84.0         0.0  10.0     10.0 NaN  1019.0   False
2020-03-28  210.0  5.1  19.7  18.9  95.0         0.0  10.0     10.0 NaN  1017.2   False
2020-03-29  190.0  5.1  20.0  18.9  93.0         0.0  10.0     10.0 NaN  1015.9   False


>>> df = bous.process_climate_zone_csv_2020('HM01X_Data_040842.txt')
>>> dat_1800 = dat.between_time('17:50','18:10').resample('D').first().loc[df.index[0]:df.index[-1]]

Now merge with sonde 900hpa data 
>>> sonde = bous.get_sounding_data(station='YBBN',level='900')


>>> sonde.tail()
            900_wdir  900_WS
2020-05-28     145.0    23.6
2020-05-29     125.0    24.6
2020-05-30      60.0     8.2
2020-05-31     335.0    20.6
2020-06-01     250.0    44.2



aws_sonde_daily = pd.merge(
            left=df_temp[['AvID', 'T', 'Td', 'WS', 'WDir', 'QNH', 'any_fg']], \
            right=sonde_data[['P900', '900_wdir', '900_WS', 'P850', '850_wdir', '850_WS']], \
            # right=df_winds[['Station', 'level','900_wdir','900_WS']],\  #if using EC data file
            left_index=True, right_index=True, how='left')

>>> aws_sonde_daily = pd.merge(left=dat_1800[['T', 'Td', 'WS', 'WDir', 'QNH', 'any_fg']],
... right=sonde[['900_wdir', '900_WS']],
... left_index=True, right_index=True, how='left')

>>> aws_sonde_daily.head()
               T    Td   WS   WDir     QNH  any_fg  900_wdir  900_WS
date                                                                
2000-01-01  18.9  16.4  5.1  200.0     NaN   False       NaN     NaN
2000-01-02  21.9  18.3  7.0  180.0     NaN   False       NaN     NaN
2000-01-03  20.6  18.5  2.9  150.0  1016.9   False       NaN     NaN
2000-01-04  21.7  19.4  4.1  140.0  1017.8   False       NaN     NaN
2000-01-05  22.1  20.7  9.9  110.0  1016.3   False       NaN     NaN


>>> aws_sonde_daily.tail()
               T    Td   WS   WDir     QNH  any_fg  900_wdir  900_WS
date                                                                
2020-03-25  17.6  16.4  6.0  210.0  1018.8   False     115.0     8.2
2020-03-26  19.2  17.7  4.1  210.0  1019.3   False     150.0    13.4
2020-03-27  18.0  15.3  8.0  210.0  1019.0   False      95.0    20.6
2020-03-28  19.7  18.9  5.1  210.0  1017.2   False      95.0     6.2
2020-03-29  20.0  18.9  5.1  190.0  1015.9   False       5.0     6.2


Now we step through the datafrae - a day at a time 
grab data 1 month wide window - so 14 day either side of date and find how many matching obs had fog


>>> importlib.reload(bous)
<module 'utility_functions_sep2018' from '/home/bou/Downloads/utility_functions_sep2018.py'>


>>> win=bous.grab_data_period_centered_day_sonde(aws_sonde_daily,14,'2019-06-03')

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-20 00:00:00, end = 2019-06-17 00:00:00


>>> win.loc[win.index.month.isin([4])].loc['2019']
Empty DataFrame
Columns: [T, Td, WS, WDir, QNH, any_fg, 900_wdir, 900_WS]
Index: []

>>> win.loc[win.index.month.isin([5])].loc['2019']
               T    Td   WS   WDir     QNH  any_fg  900_wdir  900_WS
date                                                                
2019-05-20  17.2  15.5  5.1  200.0  1024.9   False     100.0    15.4 <--- -14
2019-05-21  13.8  13.3  7.0  210.0  1025.8   False     115.0    11.4 <--- 13
2019-05-22  13.4  12.5  7.0  210.0  1024.7   False     110.0    20.6 <--- -12
2019-05-23  14.8  13.5  7.0  220.0  1023.0   False     130.0    10.2 <--- -11
2019-05-24  14.8  13.9  8.9  210.0  1021.1   False     100.0     2.0 <--- -10
2019-05-25  13.8  13.4  5.1  220.0  1018.1   False     305.0     8.2 <--- -9
2019-05-26  13.1  12.9  5.1  210.0  1015.0   False     265.0    17.4 <--- -8
2019-05-27  10.5   2.8  6.0  260.0  1016.2   False     215.0    13.4 <--- -7
2019-05-28  10.8   0.9  4.1  350.0  1015.0   False     240.0    22.6 <--- -6
2019-05-29  11.2   2.6  8.9  320.0  1015.3   False     240.0    25.8 <--- -5
2019-05-30   8.2  -3.4  9.9  240.0  1020.6   False     170.0     7.2 <--- -4
2019-05-31   8.6   0.6  7.0  200.0  1022.5   False     130.0    19.6 <--- -3

>>> win.loc[win.index.month.isin([6])].loc['2019']
               T    Td    WS   WDir     QNH  any_fg  900_wdir  900_WS
date                                                                 
2019-06-01  24.1  20.4   7.0  130.0  1014.4   False     120.0    24.6 <--- -2
2019-06-02  23.2  20.7   5.1  170.0  1011.1   False     270.0    10.2 <--- -1
2019-06-03  22.9  22.0   4.1  340.0  1011.7   False     240.0    35.0 <--- Center
2019-06-04  18.1  16.6   6.0  210.0  1020.4   False     150.0    25.8 <--- 1
2019-06-05  14.3  11.1   7.0  210.0  1017.1   False     140.0    23.6 <--- 2
2019-06-06  10.8   8.1   8.9  210.0  1029.6   False     135.0    27.8 <--- 3
2019-06-07  15.4  14.7   6.0  190.0  1028.3   False     120.0    15.4 <--- 4
2019-06-08  13.0  12.4   7.0  210.0  1017.7   False     225.0     2.0 <--- 5
2019-06-09  19.6  -6.1  23.9  270.0  1012.6   False      15.0     3.0 <--- 6
2019-06-10  17.2  16.2   5.1  310.0  1015.4   False     160.0     7.2 <--- 7
2019-06-11  14.4  11.4   0.0    0.0  1012.0   False      95.0    17.4 <--- 8
2019-06-12  21.3  20.5   1.9  200.0  1008.1   False     310.0     8.2 <--- 9
2019-06-13  13.0  12.6   0.0    0.0  1016.9   False     160.0     7.2 <--- 10
2019-06-14  14.8  14.3   5.1  260.0  1017.1   False     310.0     7.2 <--- 11
2019-06-15  14.3  13.9   0.0    0.0  1017.5    True     320.0    14.4 <--- 12
2019-06-16  16.0  15.5   4.1  200.0  1015.1   False     245.0    26.8 <--- 13
2019-06-17   8.0   5.1   1.0  230.0  1016.4   False     155.0     1.0 <--- 14

>>> win.loc[win.index.month.isin([7])].loc['2019']
Empty DataFrame
Columns: [T, Td, WS, WDir, QNH, any_fg, 900_wdir, 900_WS]
Index: []

>>> win.loc[win.index.month.isin([5])].loc['2000']
               T    Td    WS   WDir     QNH  any_fg  900_wdir  900_WS
date                                                                 
2000-05-19  14.4  10.6   6.0  220.0  1022.7   False     115.0     4.2  <--- -15
2000-05-20   9.5   9.0   7.0  220.0  1021.5   False     135.0    22.6 <--- -14
2000-05-21  15.4  12.5   7.0  210.0  1024.0   False     140.0    22.6 <--- 13
2000-05-22  15.2  12.9   1.9  220.0  1024.6   False     115.0    22.6 <--- -12
2000-05-23  14.2  13.3   6.0  250.0  1020.5   False      90.0    24.6 <--- -11
2000-05-24  15.7  15.5   4.1  230.0  1017.0    True       NaN     NaN <--- -10
2000-05-25  16.6  15.9   0.0    0.0  1012.7   False     335.0    20.6 <--- -9
2000-05-26  16.9  16.5   7.0  350.0  1010.2    True     255.0    24.6 <--- -8
2000-05-27  13.5   3.5  13.0  270.0  1012.4   False     235.0    26.8 <--- -7
2000-05-28  10.0  -2.0   4.1  310.0  1015.5   False     225.0    26.8 <--- -6
2000-05-29   6.0   0.3   0.0    0.0  1015.5   False     225.0    12.4 <--- -5
2000-05-30   6.8  -2.2  12.1  270.0  1012.8   False     225.0    28.8 <--- -4
2000-05-31   6.4   0.6   8.0  280.0  1013.3   False     210.0    14.4 <--- -3

>>> win.loc[win.index.month.isin([6])].loc['2000']
               T    Td    WS   WDir     QNH  any_fg  900_wdir  900_WS
date                                                                 
2000-06-01  19.3  17.9   4.1  180.0     NaN   False     220.0    22.6 <--- -2
2000-06-02  21.5  19.0   5.1  120.0     NaN   False     160.0    16.4 <--- -1
2000-06-03  21.1  20.5   2.9  220.0  1014.0   False     305.0     4.2 <--- Center
2000-06-04  19.3  16.8  13.0  200.0  1018.4   False     265.0    24.6 <--- 1
2000-06-05  14.1  11.5   8.0  220.0  1020.5   False     220.0    21.6 <--- 2
2000-06-06   3.4   0.8   4.1  260.0  1023.8   False      60.0     4.2 <--- 3
2000-06-07   7.4   6.5   5.1  230.0  1015.7   False     135.0    15.4 <--- 4
2000-06-08   9.8   9.0   6.0  220.0  1024.3   False      60.0     8.2 <--- 5
2000-06-09  15.8  14.5   8.0   20.0  1010.5   False     135.0    20.6 <--- 6
2000-06-10  13.9  13.2   2.9  220.0  1020.5   False     135.0    39.0 <--- 7
2000-06-11  19.6  15.6   7.0  170.0  1011.2   False     150.0    39.0 <--- 8
2000-06-12  20.5  17.6   0.0    0.0  1013.3   False     130.0    24.6 <--- 9
2000-06-13  11.6   9.9   7.0  230.0  1031.9   False     140.0    28.8 <--- 10
2000-06-14  13.0  10.1   7.0  230.0  1031.1   False       NaN     NaN <--- 11
2000-06-15  13.6  12.6   4.1  220.0  1028.8   False       NaN     NaN <--- 12
2000-06-16  11.9   9.8   5.1  210.0  1027.8   False     115.0    15.4 <--- 13
>>> 



Lets see we want fog forecast for these 3rd June 2019

def calculate_percent_chance_fg_sonde(obb_win, obs_4day):

Function accepts the data for seasonla match -- win
and obs_4day is the observation we want to match against


Now the 1800Z T/Td looks bit strange too HIGH 22.9 at 4am in the morning in JU
>>> aws_sonde_daily.loc['2019-06-03']
T             22.9
Td              22
WS             4.1
WDir           340
QNH         1011.7
any_fg       False
900_wdir       240
900_WS          35
Name: 2019-06-03 00:00:00, dtype: object


>>> dat.loc['2019-06-03 18:00']
                      WDir   WS     T    Td    RH  pptnSince9   vis  vis_aws  PW     QNH  any_fg
date                                                                                            
2019-06-03 18:00:00  340.0  4.1  22.9  22.0  95.0         0.0  10.0     10.0 NaN  1011.7   False




anyway pressing on - just relabel the sereis index as fn lookibg for different labels then what we have

>>> aws_sonde_daily.loc['2019-06-03'].rename({'900_wdir':'900Dir','900_WS':'900spd','QNH': 'P'})
T           22.9
Td            22
WS           4.1
WDir         340
P         1011.7
any_fg     False
900Dir       240
900spd        35
Name: 2019-06-03 00:00:00, dtype: object

>>> obs_4day = aws_sonde_daily.loc['2019-06-03']
.rename({'900_wdir':'900Dir','900_WS':'900spd','QNH': 'P'}) # mapping, changes series index/labels
.rename('obs_4day')											# scalar, changes Series.name to obs_4day
>>> obs_4day
T           22.9
Td            22
WS           4.1
WDir         340
P         1011.7
any_fg     False
900Dir       240
900spd        35
Name: obs_4day, dtype: object


>>> mask,matching_synops,num_of_days_match_synop_pattern,num_of_FG_days= \
	bous.calculate_percent_chance_fg_sonde(win,obs_4day)

Searching matched season records/days for fog like conditions...using these parameters
 T           22.9
Td            22
WS           4.1
WDir         340
P         1011.7
any_fg     False
900Dir       240
900spd        35
Name: 2019-06-03 00:00:00, dtype: object
900Hpa wind direction envelop: 240.0 210.0 270.0
35.0 25.0 45.0
QNH envelope:  1011.7 1006.7 1016.7
SFC T:  22.9 17.9 27.9
SFC TD:  22.0 17.0 27.0
Synop parameter thresholds
900hPa wind DIR:	210.0 and 270.0 degrees          
SFC Temp:		17.9 to 27.9 deg C          
sfc Td:		17.0 to 27.0 deg C, 
SLP:		1006.7 to 1016.7 hPa

>>> num_of_FG_days
0

>>> num_of_days_match_synop_pattern
3



Lets try lopp for a few dates



>>> for date in dates:
...     print(date.strftime("%Y-%m-%d"))
...     win=bous.grab_data_period_centered_day_sonde(aws_sonde_daily,14,date.strftime("%Y-%m-%d"))
...     obs_4day = aws_sonde_daily.loc[date]
...     obs_4day = obs_4day.rename({'900_wdir':'900Dir','900_WS':'900spd','QNH': 'P'})
...     mask,matching_synops,num_of_days_match_synop_pattern,num_of_FG_days=bous.calculate_percent_chance_fg_sonde(win,obs_4day)
...     print(f'num_of_FG_days={num_of_FG_days},num_of_days_match_synop_pattern={num_of_days_match_synop_pattern}, chance={num_of_FG_days/num_of_days_match_synop_pattern}')
 

wrap into function

def get_climatological_fog_probability(station:str='YBBN',dates:[pd.datetime]=None,aws_sonde_daily=None):
    for date in dates:
        print(date.strftime("%Y-%m-%d"))
        win=grab_data_period_centered_day_sonde(aws_sonde_daily,14,date.strftime("%Y-%m-%d"))
        obs_4day = aws_sonde_daily.loc[date]
        obs_4day = obs_4day.rename({'900_wdir':'900Dir','900_WS':'900spd','QNH': 'P'})
        mask,matching_synops,num_of_days_match_synop_pattern,num_of_FG_days=\
            calculate_percent_chance_fg_sonde(win,obs_4day)
        print(f'num_of_FG_days={num_of_FG_days},\
              num_of_days_match_synop_pattern={num_of_days_match_synop_pattern},\
              chance={100*num_of_FG_days/num_of_days_match_synop_pattern}')


# generate dates for which forecast sought
>>> dates = pd.date_range(start='2019-06-03' , end='2019-06-10', freq='D')

# generate climatological fog probs
>>> bous.get_climatological_fog_probability(station='YBBN',dates=dates,aws_sonde_daily=aws_sonde_daily)
2019-06-03

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-20 00:00:00, end = 2019-06-17 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           22.9
Td            22
WS           4.1
WDir         340
P         1011.7
any_fg     False
900Dir       240
900spd        35
Name: 2019-06-03 00:00:00, dtype: object
35.0 25.0 45.0
QNH envelope:  1011.7 1006.7 1016.7
Synop parameter thresholds
900hPa wind DIR:	210.0 and 270.0 degrees          
SFC Temp:		17.9 to 27.9 deg C          
sfc Td:		17.0 to 27.0 deg C, 
SLP:		1006.7 to 1016.7 hPa
num_of_FG_days=0,              num_of_days_match_synop_pattern=3,              chance=0.0
2019-06-04

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-21 00:00:00, end = 2019-06-18 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           18.1
Td          16.6
WS             6
WDir         210
P         1020.4
any_fg     False
900Dir       150
900spd      25.8
Name: 2019-06-04 00:00:00, dtype: object
25.8 15.8 35.8
QNH envelope:  1020.4 1015.4 1025.4
Synop parameter thresholds
900hPa wind DIR:	120.0 and 180.0 degrees          
SFC Temp:		13.1 to 23.1 deg C          
sfc Td:		11.6 to 21.6 deg C, 
SLP:		1015.4 to 1025.4 hPa
num_of_FG_days=1,              num_of_days_match_synop_pattern=22,              chance=4.545454545454546
2019-06-05

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-22 00:00:00, end = 2019-06-19 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           14.3
Td          11.1
WS             7
WDir         210
P         1017.1
any_fg     False
900Dir       140
900spd      23.6
Name: 2019-06-05 00:00:00, dtype: object
23.6 13.600000000000001 33.6
QNH envelope:  1017.1 1012.1 1022.1
Synop parameter thresholds
900hPa wind DIR:	110.0 and 170.0 degrees          
SFC Temp:		9.3 to 19.3 deg C          
sfc Td:		6.1 to 16.1 deg C, 
SLP:		1012.1 to 1022.1 hPa
num_of_FG_days=1,              num_of_days_match_synop_pattern=45,              chance=2.2222222222222223
2019-06-06

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-23 00:00:00, end = 2019-06-20 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           10.8
Td           8.1
WS           8.9
WDir         210
P         1029.6
any_fg     False
900Dir       135
900spd      27.8
Name: 2019-06-06 00:00:00, dtype: object
27.8 17.8 37.8
QNH envelope:  1029.6 1024.6 1034.6
Synop parameter thresholds
900hPa wind DIR:	105.0 and 165.0 degrees          
SFC Temp:		5.8 to 15.8 deg C          
sfc Td:		3.1 to 13.1 deg C, 
SLP:		1024.6 to 1034.6 hPa
num_of_FG_days=0,              num_of_days_match_synop_pattern=18,              chance=0.0
2019-06-07

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-24 00:00:00, end = 2019-06-21 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           15.4
Td          14.7
WS             6
WDir         190
P         1028.3
any_fg     False
900Dir       120
900spd      15.4
Name: 2019-06-07 00:00:00, dtype: object
15.4 5.4 25.4
QNH envelope:  1028.3 1023.3 1033.3
Synop parameter thresholds
900hPa wind DIR:	90.0 and 150.0 degrees          
SFC Temp:		10.4 to 20.4 deg C          
sfc Td:		9.7 to 19.7 deg C, 
SLP:		1023.3 to 1033.3 hPa
num_of_FG_days=0,              num_of_days_match_synop_pattern=30,              chance=0.0
2019-06-08

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-25 00:00:00, end = 2019-06-22 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T             13
Td          12.4
WS             7
WDir         210
P         1017.7
any_fg     False
900Dir       225
900spd         2
Name: 2019-06-08 00:00:00, dtype: object
2.0 -8.0 12.0
QNH envelope:  1017.7 1012.7 1022.7
Synop parameter thresholds
900hPa wind DIR:	195.0 and 255.0 degrees          
SFC Temp:		8.0 to 18.0 deg C          
sfc Td:		7.4 to 17.4 deg C, 
SLP:		1012.7 to 1022.7 hPa
num_of_FG_days=1,              num_of_days_match_synop_pattern=17,              chance=5.882352941176471
2019-06-09

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-26 00:00:00, end = 2019-06-23 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           19.6
Td          -6.1
WS          23.9
WDir         270
P         1012.6
any_fg     False
900Dir        15
900spd         3
Name: 2019-06-09 00:00:00, dtype: object
3.0 -7.0 13.0
QNH envelope:  1012.6 1007.6 1017.6
Synop parameter thresholds
900hPa wind DIR:	345.0 and 45.0 degrees          
SFC Temp:		14.6 to 24.6 deg C          
sfc Td:		-11.1 to -1.1 deg C, 
SLP:		1007.6 to 1017.6 hPa
num_of_FG_days=0,              num_of_days_match_synop_pattern=1,              chance=0.0
2019-06-10

Extracting aws/sonde data for same seasonality or        
calendar days window:14 days either side of 14
start = 2019-05-27 00:00:00, end = 2019-06-24 00:00:00

Searching matched season records/days for fog like conditions...using these parameters
 T           17.2
Td          16.2
WS           5.1
WDir         310
P         1015.4
any_fg     False
900Dir       160
900spd       7.2
Name: 2019-06-10 00:00:00, dtype: object
7.2 -2.8 17.2
QNH envelope:  1015.4 1010.4 1020.4
Synop parameter thresholds
900hPa wind DIR:	130.0 and 190.0 degrees          
SFC Temp:		12.2 to 22.2 deg C          
sfc Td:		11.2 to 21.2 deg C, 
SLP:		1010.4 to 1020.4 hPa
num_of_FG_days=0,              num_of_days_match_synop_pattern=8,              chance=0.0
>>> 




In Summary

# grab climate zone data file for station
>>> df = bous.process_climate_zone_csv_2020('HM01X_Data_040842.txt')

# filter 1800Z/4am data 
>>> dat_1800 = df.between_time('17:50','18:10').resample('D').first().loc[df.index[0]:df.index[-1]]

# Now merge with sonde 900hpa data 
>>> sonde = bous.get_sounding_data(station='YBBN',level='900')
>>> aws_sonde_daily = pd.merge(left=dat_1800[['T', 'Td', 'WS', 'WDir', 'QNH', 'any_fg']],
... right=sonde[['900_wdir', '900_WS']],
... left_index=True, right_index=True, how='left')

# generate dates for which forecast sought
>>> dates = pd.date_range(start='2019-06-03' , end='2019-06-10', freq='D')

# generate climatological fog probs
>>> bous.get_climatological_fog_probability(station='YBBN',dates=dates,aws_sonde_daily=aws_sonde_daily)



>>> cur_dir="/home/bou/shared/stats-R/flask_projects/avguide"
>>> stations = ['YBBN', 'YBAF', 'YAMB', 'YBSU', 'YBCG', 'YBOK', 'YTWB','YBRK']
>>> for station in stations:
...     cur_dir="/home/bou/shared/stats-R/fog-project/fog_climatology_2019/tcz_se_qld"
...     bous.process_climate_zone_csv_2020(station)  #process new tcz and write as pkl files
...     
        cur_dir="/home/bou/shared/stats-R/flask_projects/avguide"  # write to avguide folder
...     df = pickle.load(open(os.path.join(cur_dir, 'app','data', station+'_aws_new.pkl'), 'rb'))
...     df.tail(1)


merge storm data from previous old tcz files
>> stations = ['YBBN', 'YBAF', 'YAMB', 'YBSU', 'YBCG', 'YBOK', 'YTWB']

>>> for station in stations:
...     df_old = pickle.load(open(os.path.join(cur_dir, 'app','data', 'old_data_pkls',station+'_aws.pkl'), 'rb'))
...     df_new = pickle.load(open(os.path.join(cur_dir, 'app','data', station+'_aws_new.pkl'), 'rb'))
...     df = pd.merge(left=df_new,right=df_old[['any_ts','AMP']],left_index=True, right_index=True,how='left')
...     with open(os.path.join(cur_dir,'app','data',station+'_aws.pkl') , 'wb') as f:
...         pickle.dump(df, f)







https://stackoverflow.com/questions/42733194/subplot-of-windrose-in-matplotlib



from windrose import WindroseAxes
wd=fg_days['wdir17'].values
ws=fg_days['wspd17'].values
ax = WindroseAxes.from_ax()
ax.bar(wd, ws, bins=np.arange(0, 10, 2), normed=True, nsector=16)

table = ax._info['table']

dirs=('N','NNE','NE','ENE','E','ESE','SE','SSE','S','SSW','SW','WSW','W','WNW','NW','NNW')


table[0,:].sum(),table[1,:].sum(),table[2,:].sum(),table[3,:].sum(),table[4,:].sum()
table.sum(axis=1)


ax._info['bins']

table.sum(axis=0) # OR  np.sum(table, axis=0) sum along rows so column sums

list(zip(dirs,table.sum(axis=0)))



wd_freq = np.sum(table, axis=0)
direction = ax._info['dir']
plt.bar(np.arange(16), wd_freq, align='center')








cx_OracleTools 

cx-Oracle (7.3.0)                                        - Python interface to
                                                           Oracle

sqlpython (1.7.3)                                        - Command-line
                                                           interface to Oracle

cx-PyOracleLib (3.0)                                     - Set of Python
                                                           modules for
                                                           handling Oracle
                                                           databases

cx_OracleTools (8.0)                                     - Tools for managing
                                                           Oracle data and
                                                           source code.

pandas-oracle (2.1.4)                                    - Tools for working
                                                           with an Oracle
                                                           database from
                                                          Pandas



portfoli ideas
--------------
primary lead for each area , and then secondary/assistants
with num of PO2s helping out


customer person - customer lead
innovation perosn - innovation lead
visual weather - felix
tropical lead
fog/low cloud lead


SPOC can switch staff between desks on any given shift
say person not confy with sev wx - move to another desk 

no one will be completely comfortable with any shift or wx situation

major airports team



















  971  grep 'mydata' * -R
  972  grep 'resultsFOG' * -R
  973  grep 'results_seqld' * -R
  974  grep 'fog_predict' * -R
  975  grep 'results' * -R
  993  cd /home/bou/shared/stats-R/flask_projects/avguide

  995  grep url_for\(\'results\' * -R	# check for ('results')   in url_for route definitions
  996  grep \/results\/ * -R			# check for /results/     in path definitions
  997  grep 'results(' * -R				# check for 'results('    in function definitions

@app.route('/results', methods=['POST'])
def results():


we changed above to reflect that its doing storm results se qld
@app.route('/results_TS_seqld', methods=['POST'])
def results_TS_seqld():




